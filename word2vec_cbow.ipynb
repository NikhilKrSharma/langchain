
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Word2Vec from Scratch Using PyTorch\n",
    "This notebook builds a Word2Vec CBOW model using a custom text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use the provided text\n",
    "text = \"\"\"Getting numbers is easy; getting numbers you can trust is hard. This practical guide by experimentation leaders at Google, LinkedIn, and Microsoft will teach you how to accelerate innovation using trustworthy online controlled experiments, or A/B tests. Based on practical experiences at companies that each runs more than 20,000 controlled experiments a year, the authors share examples, pitfalls, and advice for students and industry professionals getting started with experiments, p...
    "Learn how to:\n",
    "\u25cf Use the scientific method to evaluate hypotheses using controlled experiments\n",
    "\u25cf Define key metrics and ideally an Overall Evaluation Criterion\n",
    "\u25cf Test for trustworthiness of the results and alert experimenters to violated assumptions\n",
    "\u25cf Interpret and iterate quickly based on the results\n",
    "\u25cf Implement guardrails to protect key business goals\n",
    "\u25cf Build a scalable platform that lowers the marginal cost of experiments close to zero \u25cf Avoid pitfalls such as carryover effects, Twyman’s law, Simpson’s paradox, and network interactions\n",
    "\u25cf Understand how statistical issues play out in practice, including common violations of assumptions\"\"\"\n",
    "\n",
    "# Preprocessing\n",
    "tokens = text.lower().replace(';', '').replace('.', '').replace(',', '').split()\n",
    "vocab = list(set(tokens))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Generate CBOW pairs\n",
    "def generate_cbow_pairs(words, window_size=2):\n",
    "    pairs = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
    "        target = words[i]\n",
    "        pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_cbow_pairs(tokens)\n",
    "training_data = [([word2idx[w] for w in ctx], word2idx[target]) for ctx, target in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: CBOW Model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_idxs):\n",
    "        embeds = self.embeddings(context_idxs)\n",
    "        avg_embed = embeds.mean(dim=0).view(1, -1)\n",
    "        out = self.linear(avg_embed)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Training\n",
    "embedding_dim = 50\n",
    "model = CBOWModel(vocab_size, embedding_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    random.shuffle(training_data)\n",
    "    for context, target in training_data:\n",
    "        context_var = torch.tensor(context, dtype=torch.long)\n",
    "        target_var = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        output = model(context_var)\n",
    "        loss = loss_fn(output, target_var)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Similarity Function\n",
    "def most_similar(word, topn=5):\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    vec = model.embeddings(torch.tensor(word2idx[word])).detach().numpy().reshape(1, -1)\n",
    "    all_vecs = model.embeddings.weight.detach().numpy()\n",
    "    sims = cosine_similarity(vec, all_vecs)[0]\n",
    "    top_idxs = np.argsort(-sims)[1:topn+1]\n",
    "    return [(idx2word[i], sims[i]) for i in top_idxs]\n",
    "\n",
    "# Example\n",
    "print(most_similar('experiments'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
