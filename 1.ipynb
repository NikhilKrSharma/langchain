{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff b/w PromptTemplate and ChatPromptTemplate and ChatMessagePromptTemplate\n",
    "\n",
    "| Feature               | `PromptTemplate`       | `ChatPromptTemplate` | `ChatMessagePromptTemplate` |\n",
    "|----------------------|----------------------|----------------------|-----------------------------|\n",
    "| Use Case            | Text-based LLMs       | Chat-based LLMs       | Individual chat messages   |\n",
    "| Handles Multiple Messages? | ❌ No               | ✅ Yes              | ❌ No                       |\n",
    "| Supports Roles?     | ❌ No                 | ✅ Yes              | ✅ Yes                      |\n",
    "| Input Variables?    | ✅ Yes               | ✅ Yes              | ✅ Yes                      |\n",
    "| Example Output      | Single formatted string | List of messages | Single message |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Environment Variables:\n",
      "- OPENAI_API_KEY: sk-proj-...hyUA\n",
      "- HUGGINGFACE_TOKEN: hf_kGAqY...gEef\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['HF_HOME']=\"/Users/nikhil20.sharma/Desktop/langchain/.cache\"\n",
    "os.environ['HF_HOME']=\"/Users/nikhil20.sharma/Desktop/hf-cache\"\n",
    "\n",
    "# Print all environment variables loaded from .env\n",
    "print(\"Loaded Environment Variables:\")\n",
    "for key, value in os.environ.items():\n",
    "    if key in ['OPENAI_API_KEY', 'LANGSMITH_AIP_KEY', 'HUGGINGFACE_TOKEN']:\n",
    "        # Mask sensitive values for security\n",
    "        masked_value = value[:8] + \"...\" + value[-4:] if value else value\n",
    "        print(f\"- {key}: {masked_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitters\n",
    "\n",
    "- Context size\n",
    "- Better embedding\n",
    "- Better semantic search\n",
    "- Better summarisation\n",
    "- Prevents hallunication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CharacterTextSplitter - Length Based Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.txt\", \"r\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"\",\n",
    "    # length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2248\n"
     ]
    }
   ],
   "source": [
    "result = splitter.split_text(content)\n",
    "print(f\"Number of chunks: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"XV.: Tará.\\nCanto XVI.: The Fall of Báli.\\nCanto XVII.: Báli's Speech.\\nCanto XVIII.: Ráma's Reply.\\nCanto XIX.: Tárá's Grief.\\nCanto XX.: Tárá's Lament.\\nCanto XXI.: Hanumán's Speech.\\nCanto XXII.: Báli Dead.\\nCanto XXIII.: Tárá's Lament.\\nCanto XXIV.: Sugríva's Lament.\\nCanto XXV.: Ráma's Speech.\\nCanto XXVI.: The Coronation.\\nCanto XXVII.: Ráma On The Hill.\\nCanto XXVIII.: The Rains.\\nCanto XXIX.: Hanumán's Counsel.\\nCanto XXX.: Ráma's Lament.\\nCanto XXXI.: The Envoy.\\nCanto XXXII.: Hanuman's Counsel.\\nCanto XXXIII.: Lakshman's Entry.\\nCanto XXXIV.: Lakshman's Speech\\nCanto XXXV.: Tárá's Speech.\\nCanto XXXVI.: Sugríva's Speech.\\nCanto XXXVII.: The Gathering.\\nCanto XXXVIII.: Sugríva's Departure.\\nCanto XXXIX.: The Vánar Host.\\n\\nCanto XL.: The Army of The East.\\nCanto XLI.: The Army of The South.\\nCanto XLII.: The Army of The West.\\nCanto XLIII.: The Army of The North.\\nCanto XLIV.: The Ring.\\nCanto XLV.: The Departure.\\nCanto XLVI.: Sugríva's.\\nCanto XLVII.: The Return.\\nCanto XLVIII.: The Asur's Death.\\nCanto XLIX.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[10])\n",
    "result[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 2732\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "print(f\"Number of pages: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Acrobat Web Capture 6.0', 'creator': 'PyPDF', 'creationdate': '2008-12-23T20:52:08+00:00', 'moddate': '2008-12-23T21:45:04+05:30', 'title': 'The Ramayana index', 'source': 'data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.pdf', 'total_pages': 2732, 'page': 0, 'page_label': '1'}, page_content='Ralph T. H. Griffith\\nRamayan of ValmikiRamayan of Valmiki')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Web Capture 6.0',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2008-12-23T20:52:08+00:00',\n",
       " 'moddate': '2008-12-23T21:45:04+05:30',\n",
       " 'title': 'The Ramayana index',\n",
       " 'source': 'data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.pdf',\n",
       " 'total_pages': 2732,\n",
       " 'page': 80,\n",
       " 'page_label': '81'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[80].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Each man contented sought no more,\\nNor longed with envy for the store\\n   By richer friends possessed.\\nFor poverty was there unknown,\\nAnd each man counted as his own\\n   Kine, steeds, and gold, and grain.\\nAll dressed in raiment bright and clean,\\nAnd every townsman might be seen\\nWith earrings, wreath, or chain.\\nNone deigned to feed on broken fare,\\nAnd none was false or stingy there.\\nA piece of gold, the smallest pay,\\nWas earned by labour for a day.\\nOn every arm were bracelets worn,\\nAnd none was faithless or forsworn,\\n   A braggart or unkind.\\nNone lived upon another's wealth,\\nNone pined with dread or broken health,\\n   Or dark disease of mind.\\nHigh-souled were all. The slanderous word,\\nThe boastful lie, were never heard.\\nEach man was constant to his vows,\\nAnd lived devoted to his spouse.\\nNo other love his fancy knew,\\nAnd she was tender, kind, and true.\\nHer dames were fair of form and face,\\nWith charm of wit and gentle grace,\\nWith modest raiment simply neat,\\nAnd winning manners soft and sweet.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[80].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3341\n"
     ]
    }
   ],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"\",\n",
    "    # length_function=len\n",
    ")\n",
    "\n",
    "result = splitter.split_documents(documents=documents)\n",
    "print(f\"Number of chunks: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Web Capture 6.0',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2008-12-23T20:52:08+00:00',\n",
       " 'moddate': '2008-12-23T21:45:04+05:30',\n",
       " 'title': 'The Ramayana index',\n",
       " 'source': 'data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.pdf',\n",
       " 'total_pages': 2732,\n",
       " 'page': 67,\n",
       " 'page_label': '68'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[80].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9:4 Parasúráma or Ráma with the Axe. See Canto \\nLXXIV.\\n9:1b Sitá. Videha was the country of which Mithilá was \\nthe capital.\\nNext: Canto IV.: The Rhapsodists.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[80].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter - Text Structure Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9:4 Parasúráma or Ráma with the Axe. See Canto \\nLXXIV.\\n9:1b Sitá. Videha was the country of which Mithilá was \\nthe capital.\\nNext: Canto IV.: The Rhapsodists.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    # length_function=len\n",
    ")\n",
    "result = splitter.split_documents(documents=documents)\n",
    "print(f\"Number of chunks: {len(result)}\")\n",
    "result[80].metadata\n",
    "result[80].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Structure Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 13\n",
      "# Copyright (c) Facebook, Inc. and its affiliates.\n",
      "# All rights reserved.\n",
      "#\n",
      "# This source code is licensed under the license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "\n",
      "import math\n",
      "import typing as tp\n",
      "\n",
      "import julius\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from .states import capture_init\n",
      "from .utils import center_trim, unfold\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=0,\n",
    "    # separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    # length_function=len\n",
    ")\n",
    "\n",
    "code = '''\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import math\n",
    "import typing as tp\n",
    "\n",
    "import julius\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from .states import capture_init\n",
    "from .utils import center_trim, unfold\n",
    "\n",
    "\n",
    "class BLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    BiLSTM with same hidden units as input dim.\n",
    "    If `max_steps` is not None, input will be splitting in overlapping\n",
    "    chunks and the LSTM applied separately on each chunk.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, layers=1, max_steps=None, skip=False):\n",
    "        super().__init__()\n",
    "        assert max_steps is None or max_steps % 4 == 0\n",
    "        self.max_steps = max_steps\n",
    "        self.lstm = nn.LSTM(bidirectional=True, num_layers=layers, hidden_size=dim, input_size=dim)\n",
    "        self.linear = nn.Linear(2 * dim, dim)\n",
    "        self.skip = skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T = x.shape\n",
    "        y = x\n",
    "        framed = False\n",
    "        if self.max_steps is not None and T > self.max_steps:\n",
    "            width = self.max_steps\n",
    "            stride = width // 2\n",
    "            frames = unfold(x, width, stride)\n",
    "            nframes = frames.shape[2]\n",
    "            framed = True\n",
    "            x = frames.permute(0, 2, 1, 3).reshape(-1, C, width)\n",
    "\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.linear(x)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        if framed:\n",
    "            out = []\n",
    "            frames = x.reshape(B, -1, C, width)\n",
    "            limit = stride // 2\n",
    "            for k in range(nframes):\n",
    "                if k == 0:\n",
    "                    out.append(frames[:, k, :, :-limit])\n",
    "                elif k == nframes - 1:\n",
    "                    out.append(frames[:, k, :, limit:])\n",
    "                else:\n",
    "                    out.append(frames[:, k, :, limit:-limit])\n",
    "            out = torch.cat(out, -1)\n",
    "            out = out[..., :T]\n",
    "            x = out\n",
    "        if self.skip:\n",
    "            x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "def rescale_conv(conv, reference):\n",
    "    \"\"\"Rescale initial weight scale. It is unclear why it helps but it certainly does.\n",
    "    \"\"\"\n",
    "    std = conv.weight.std().detach()\n",
    "    scale = (std / reference)**0.5\n",
    "    conv.weight.data /= scale\n",
    "    if conv.bias is not None:\n",
    "        conv.bias.data /= scale\n",
    "\n",
    "\n",
    "def rescale_module(module, reference):\n",
    "    for sub in module.modules():\n",
    "        if isinstance(sub, (nn.Conv1d, nn.ConvTranspose1d, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            rescale_conv(sub, reference)\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    \"\"\"Layer scale from [Touvron et al 2021] (https://arxiv.org/pdf/2103.17239.pdf).\n",
    "    This rescales diagonaly residual outputs close to 0 initially, then learnt.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, init: float = 0):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n",
    "        self.scale.data[:] = init\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.scale[:, None] * x\n",
    "\n",
    "\n",
    "class DConv(nn.Module):\n",
    "    \"\"\"\n",
    "    New residual branches in each encoder layer.\n",
    "    This alternates dilated convolutions, potentially with LSTMs and attention.\n",
    "    Also before entering each residual branch, dimension is projected on a smaller subspace,\n",
    "    e.g. of dim `channels // compress`.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, compress: float = 4, depth: int = 2, init: float = 1e-4,\n",
    "                 norm=True, attn=False, heads=4, ndecay=4, lstm=False, gelu=True,\n",
    "                 kernel=3, dilate=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels: input/output channels for residual branch.\n",
    "            compress: amount of channel compression inside the branch.\n",
    "            depth: number of layers in the residual branch. Each layer has its own\n",
    "                projection, and potentially LSTM and attention.\n",
    "            init: initial scale for LayerNorm.\n",
    "            norm: use GroupNorm.\n",
    "            attn: use LocalAttention.\n",
    "            heads: number of heads for the LocalAttention.\n",
    "            ndecay: number of decay controls in the LocalAttention.\n",
    "            lstm: use LSTM.\n",
    "            gelu: Use GELU activation.\n",
    "            kernel: kernel size for the (dilated) convolutions.\n",
    "            dilate: if true, use dilation, increasing with the depth.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        assert kernel % 2 == 1\n",
    "        self.channels = channels\n",
    "        self.compress = compress\n",
    "        self.depth = abs(depth)\n",
    "        dilate = depth > 0\n",
    "\n",
    "        norm_fn: tp.Callable[[int], nn.Module]\n",
    "        norm_fn = lambda d: nn.Identity()  # noqa\n",
    "        if norm:\n",
    "            norm_fn = lambda d: nn.GroupNorm(1, d)  # noqa\n",
    "\n",
    "        hidden = int(channels / compress)\n",
    "\n",
    "        act: tp.Type[nn.Module]\n",
    "        if gelu:\n",
    "            act = nn.GELU\n",
    "        else:\n",
    "            act = nn.ReLU\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for d in range(self.depth):\n",
    "            dilation = 2 ** d if dilate else 1\n",
    "            padding = dilation * (kernel // 2)\n",
    "            mods = [\n",
    "                nn.Conv1d(channels, hidden, kernel, dilation=dilation, padding=padding),\n",
    "                norm_fn(hidden), act(),\n",
    "                nn.Conv1d(hidden, 2 * channels, 1),\n",
    "                norm_fn(2 * channels), nn.GLU(1),\n",
    "                LayerScale(channels, init),\n",
    "            ]\n",
    "            if attn:\n",
    "                mods.insert(3, LocalState(hidden, heads=heads, ndecay=ndecay))\n",
    "            if lstm:\n",
    "                mods.insert(3, BLSTM(hidden, layers=2, max_steps=200, skip=True))\n",
    "            layer = nn.Sequential(*mods)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocalState(nn.Module):\n",
    "    \"\"\"Local state allows to have attention based only on data (no positional embedding),\n",
    "    but while setting a constraint on the time window (e.g. decaying penalty term).\n",
    "\n",
    "    Also a failed experiments with trying to provide some frequency based attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, heads: int = 4, nfreqs: int = 0, ndecay: int = 4):\n",
    "        super().__init__()\n",
    "        assert channels % heads == 0, (channels, heads)\n",
    "        self.heads = heads\n",
    "        self.nfreqs = nfreqs\n",
    "        self.ndecay = ndecay\n",
    "        self.content = nn.Conv1d(channels, channels, 1)\n",
    "        self.query = nn.Conv1d(channels, channels, 1)\n",
    "        self.key = nn.Conv1d(channels, channels, 1)\n",
    "        if nfreqs:\n",
    "            self.query_freqs = nn.Conv1d(channels, heads * nfreqs, 1)\n",
    "        if ndecay:\n",
    "            self.query_decay = nn.Conv1d(channels, heads * ndecay, 1)\n",
    "            # Initialize decay close to zero (there is a sigmoid), for maximum initial window.\n",
    "            self.query_decay.weight.data *= 0.01\n",
    "            assert self.query_decay.bias is not None  # stupid type checker\n",
    "            self.query_decay.bias.data[:] = -2\n",
    "        self.proj = nn.Conv1d(channels + heads * nfreqs, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T = x.shape\n",
    "        heads = self.heads\n",
    "        indexes = torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "        # left index are keys, right index are queries\n",
    "        delta = indexes[:, None] - indexes[None, :]\n",
    "\n",
    "        queries = self.query(x).view(B, heads, -1, T)\n",
    "        keys = self.key(x).view(B, heads, -1, T)\n",
    "        # t are keys, s are queries\n",
    "        dots = torch.einsum(\"bhct,bhcs->bhts\", keys, queries)\n",
    "        dots /= keys.shape[2]**0.5\n",
    "        if self.nfreqs:\n",
    "            periods = torch.arange(1, self.nfreqs + 1, device=x.device, dtype=x.dtype)\n",
    "            freq_kernel = torch.cos(2 * math.pi * delta / periods.view(-1, 1, 1))\n",
    "            freq_q = self.query_freqs(x).view(B, heads, -1, T) / self.nfreqs ** 0.5\n",
    "            dots += torch.einsum(\"fts,bhfs->bhts\", freq_kernel, freq_q)\n",
    "        if self.ndecay:\n",
    "            decays = torch.arange(1, self.ndecay + 1, device=x.device, dtype=x.dtype)\n",
    "            decay_q = self.query_decay(x).view(B, heads, -1, T)\n",
    "            decay_q = torch.sigmoid(decay_q) / 2\n",
    "            decay_kernel = - decays.view(-1, 1, 1) * delta.abs() / self.ndecay**0.5\n",
    "            dots += torch.einsum(\"fts,bhfs->bhts\", decay_kernel, decay_q)\n",
    "\n",
    "        # Kill self reference.\n",
    "        dots.masked_fill_(torch.eye(T, device=dots.device, dtype=torch.bool), -100)\n",
    "        weights = torch.softmax(dots, dim=2)\n",
    "\n",
    "        content = self.content(x).view(B, heads, -1, T)\n",
    "        result = torch.einsum(\"bhts,bhct->bhcs\", weights, content)\n",
    "        if self.nfreqs:\n",
    "            time_sig = torch.einsum(\"bhts,fts->bhfs\", weights, freq_kernel)\n",
    "            result = torch.cat([result, time_sig], 2)\n",
    "        result = result.reshape(B, -1, T)\n",
    "        return x + self.proj(result)\n",
    "\n",
    "\n",
    "class Demucs(nn.Module):\n",
    "    @capture_init\n",
    "    def __init__(self,\n",
    "                 sources,\n",
    "                 # Channels\n",
    "                 audio_channels=2,\n",
    "                 channels=64,\n",
    "                 growth=2.,\n",
    "                 # Main structure\n",
    "                 depth=6,\n",
    "                 rewrite=True,\n",
    "                 lstm_layers=0,\n",
    "                 # Convolutions\n",
    "                 kernel_size=8,\n",
    "                 stride=4,\n",
    "                 context=1,\n",
    "                 # Activations\n",
    "                 gelu=True,\n",
    "                 glu=True,\n",
    "                 # Normalization\n",
    "                 norm_starts=4,\n",
    "                 norm_groups=4,\n",
    "                 # DConv residual branch\n",
    "                 dconv_mode=1,\n",
    "                 dconv_depth=2,\n",
    "                 dconv_comp=4,\n",
    "                 dconv_attn=4,\n",
    "                 dconv_lstm=4,\n",
    "                 dconv_init=1e-4,\n",
    "                 # Pre/post processing\n",
    "                 normalize=True,\n",
    "                 resample=True,\n",
    "                 # Weight init\n",
    "                 rescale=0.1,\n",
    "                 # Metadata\n",
    "                 samplerate=44100,\n",
    "                 segment=4 * 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sources (list[str]): list of source names\n",
    "            audio_channels (int): stereo or mono\n",
    "            channels (int): first convolution channels\n",
    "            depth (int): number of encoder/decoder layers\n",
    "            growth (float): multiply (resp divide) number of channels by that\n",
    "                for each layer of the encoder (resp decoder)\n",
    "            depth (int): number of layers in the encoder and in the decoder.\n",
    "            rewrite (bool): add 1x1 convolution to each layer.\n",
    "            lstm_layers (int): number of lstm layers, 0 = no lstm. Deactivated\n",
    "                by default, as this is now replaced by the smaller and faster small LSTMs\n",
    "                in the DConv branches.\n",
    "            kernel_size (int): kernel size for convolutions\n",
    "            stride (int): stride for convolutions\n",
    "            context (int): kernel size of the convolution in the\n",
    "                decoder before the transposed convolution. If > 1,\n",
    "                will provide some context from neighboring time steps.\n",
    "            gelu: use GELU activation function.\n",
    "            glu (bool): use glu instead of ReLU for the 1x1 rewrite conv.\n",
    "            norm_starts: layer at which group norm starts being used.\n",
    "                decoder layers are numbered in reverse order.\n",
    "            norm_groups: number of groups for group norm.\n",
    "            dconv_mode: if 1: dconv in encoder only, 2: decoder only, 3: both.\n",
    "            dconv_depth: depth of residual DConv branch.\n",
    "            dconv_comp: compression of DConv branch.\n",
    "            dconv_attn: adds attention layers in DConv branch starting at this layer.\n",
    "            dconv_lstm: adds a LSTM layer in DConv branch starting at this layer.\n",
    "            dconv_init: initial scale for the DConv branch LayerScale.\n",
    "            normalize (bool): normalizes the input audio on the fly, and scales back\n",
    "                the output by the same amount.\n",
    "            resample (bool): upsample x2 the input and downsample /2 the output.\n",
    "            rescale (int): rescale initial weights of convolutions\n",
    "                to get their standard deviation closer to `rescale`.\n",
    "            samplerate (int): stored as meta information for easing\n",
    "                future evaluations of the model.\n",
    "            segment (float): duration of the chunks of audio to ideally evaluate the model on.\n",
    "                This is used by `demucs.apply.apply_model`.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.audio_channels = audio_channels\n",
    "        self.sources = sources\n",
    "        self.kernel_size = kernel_size\n",
    "        self.context = context\n",
    "        self.stride = stride\n",
    "        self.depth = depth\n",
    "        self.resample = resample\n",
    "        self.channels = channels\n",
    "        self.normalize = normalize\n",
    "        self.samplerate = samplerate\n",
    "        self.segment = segment\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.skip_scales = nn.ModuleList()\n",
    "\n",
    "        if glu:\n",
    "            activation = nn.GLU(dim=1)\n",
    "            ch_scale = 2\n",
    "        else:\n",
    "            activation = nn.ReLU()\n",
    "            ch_scale = 1\n",
    "        if gelu:\n",
    "            act2 = nn.GELU\n",
    "        else:\n",
    "            act2 = nn.ReLU\n",
    "\n",
    "        in_channels = audio_channels\n",
    "        padding = 0\n",
    "        for index in range(depth):\n",
    "            norm_fn = lambda d: nn.Identity()  # noqa\n",
    "            if index >= norm_starts:\n",
    "                norm_fn = lambda d: nn.GroupNorm(norm_groups, d)  # noqa\n",
    "\n",
    "            encode = []\n",
    "            encode += [\n",
    "                nn.Conv1d(in_channels, channels, kernel_size, stride),\n",
    "                norm_fn(channels),\n",
    "                act2(),\n",
    "            ]\n",
    "            attn = index >= dconv_attn\n",
    "            lstm = index >= dconv_lstm\n",
    "            if dconv_mode & 1:\n",
    "                encode += [DConv(channels, depth=dconv_depth, init=dconv_init,\n",
    "                                 compress=dconv_comp, attn=attn, lstm=lstm)]\n",
    "            if rewrite:\n",
    "                encode += [\n",
    "                    nn.Conv1d(channels, ch_scale * channels, 1),\n",
    "                    norm_fn(ch_scale * channels), activation]\n",
    "            self.encoder.append(nn.Sequential(*encode))\n",
    "\n",
    "            decode = []\n",
    "            if index > 0:\n",
    "                out_channels = in_channels\n",
    "            else:\n",
    "                out_channels = len(self.sources) * audio_channels\n",
    "            if rewrite:\n",
    "                decode += [\n",
    "                    nn.Conv1d(channels, ch_scale * channels, 2 * context + 1, padding=context),\n",
    "                    norm_fn(ch_scale * channels), activation]\n",
    "            if dconv_mode & 2:\n",
    "                decode += [DConv(channels, depth=dconv_depth, init=dconv_init,\n",
    "                                 compress=dconv_comp, attn=attn, lstm=lstm)]\n",
    "            decode += [nn.ConvTranspose1d(channels, out_channels,\n",
    "                       kernel_size, stride, padding=padding)]\n",
    "            if index > 0:\n",
    "                decode += [norm_fn(out_channels), act2()]\n",
    "            self.decoder.insert(0, nn.Sequential(*decode))\n",
    "            in_channels = channels\n",
    "            channels = int(growth * channels)\n",
    "\n",
    "        channels = in_channels\n",
    "        if lstm_layers:\n",
    "            self.lstm = BLSTM(channels, lstm_layers)\n",
    "        else:\n",
    "            self.lstm = None\n",
    "\n",
    "        if rescale:\n",
    "            rescale_module(self, reference=rescale)\n",
    "\n",
    "    def valid_length(self, length):\n",
    "        \"\"\"\n",
    "        Return the nearest valid length to use with the model so that\n",
    "        there is no time steps left over in a convolution, e.g. for all\n",
    "        layers, size of the input - kernel_size % stride = 0.\n",
    "\n",
    "        Note that input are automatically padded if necessary to ensure that the output\n",
    "        has the same length as the input.\n",
    "        \"\"\"\n",
    "        if self.resample:\n",
    "            length *= 2\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            length = math.ceil((length - self.kernel_size) / self.stride) + 1\n",
    "            length = max(1, length)\n",
    "\n",
    "        for idx in range(self.depth):\n",
    "            length = (length - 1) * self.stride + self.kernel_size\n",
    "\n",
    "        if self.resample:\n",
    "            length = math.ceil(length / 2)\n",
    "        return int(length)\n",
    "\n",
    "    def forward(self, mix):\n",
    "        x = mix\n",
    "        length = x.shape[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            mono = mix.mean(dim=1, keepdim=True)\n",
    "            mean = mono.mean(dim=-1, keepdim=True)\n",
    "            std = mono.std(dim=-1, keepdim=True)\n",
    "            x = (x - mean) / (1e-5 + std)\n",
    "        else:\n",
    "            mean = 0\n",
    "            std = 1\n",
    "\n",
    "        delta = self.valid_length(length) - length\n",
    "        x = F.pad(x, (delta // 2, delta - delta // 2))\n",
    "\n",
    "        if self.resample:\n",
    "            x = julius.resample_frac(x, 1, 2)\n",
    "\n",
    "        saved = []\n",
    "        for encode in self.encoder:\n",
    "            x = encode(x)\n",
    "            saved.append(x)\n",
    "\n",
    "        if self.lstm:\n",
    "            x = self.lstm(x)\n",
    "\n",
    "        for decode in self.decoder:\n",
    "            skip = saved.pop(-1)\n",
    "            skip = center_trim(skip, x)\n",
    "            x = decode(x + skip)\n",
    "\n",
    "        if self.resample:\n",
    "            x = julius.resample_frac(x, 2, 1)\n",
    "        x = x * std + mean\n",
    "        x = center_trim(x, length)\n",
    "        x = x.view(x.size(0), len(self.sources), self.audio_channels, x.size(-1))\n",
    "        return x\n",
    "\n",
    "    def load_state_dict(self, state, strict=True):\n",
    "        # fix a mismatch with previous generation Demucs models.\n",
    "        for idx in range(self.depth):\n",
    "            for a in ['encoder', 'decoder']:\n",
    "                for b in ['bias', 'weight']:\n",
    "                    new = f'{a}.{idx}.3.{b}'\n",
    "                    old = f'{a}.{idx}.2.{b}'\n",
    "                    if old in state and new not in state:\n",
    "                        state[new] = state.pop(old)\n",
    "        super().load_state_dict(state, strict=strict)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "result = splitter.split_text(code)\n",
    "print(f\"Number of chunks: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemanticChunker - Semantic Meaning Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[Document(metadata={}, page_content='\\nFarmers were working hard in the fields, preparing the soil and planting seeds for the next season.'), Document(metadata={}, page_content='The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.'), Document(metadata={}, page_content='Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety. ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), \n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1\n",
    ")\n",
    "\n",
    "sample = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "\n",
    "\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\"\n",
    "\n",
    "docs = text_splitter.create_documents([sample])\n",
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Document(metadata={}, page_content='\\nFarmers were working hard in the fields, preparing the soil and planting seeds for the next season.'), Document(metadata={}, page_content='The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams. Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety. ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), \n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.5\n",
    ")\n",
    "\n",
    "sample = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "\n",
    "\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\"\n",
    "\n",
    "docs = text_splitter.create_documents([sample])\n",
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[Document(metadata={}, page_content='\\nFarmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams. Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety. ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), \n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=3\n",
    ")\n",
    "\n",
    "sample = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "\n",
    "\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\"\n",
    "\n",
    "docs = text_splitter.create_documents([sample])\n",
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def read_pdf_and_save_to_txt(pdf_path, txt_path):\n",
    "    \"\"\"\n",
    "    Reads text content from a PDF file and saves it to a TXT file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the input PDF file.\n",
    "        txt_path (str): The path to the output TXT file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\\n\"  # Add extra newline for page separation\n",
    "\n",
    "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text)\n",
    "\n",
    "        print(f\"Successfully read '{pdf_path}' and saved text to '{txt_path}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "read_pdf_and_save_to_txt(\n",
    "    pdf_path=\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.pdf\",\n",
    "    txt_path=\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.txt\", \"r\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\n",
    "    file_path=\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.txt\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ralph T. H. GriffithRamayan of ValmikiRamayan of Valmiki\\n\\n\\nSacred Texts  Hinduism  \\nRÁMÁYAN OF VÁLMÍKI\\nRALPH T. H. GRIFFITH, M. A.,\\n[1870-1874]\\nContents    Start Reading \\nThis is the first complete public domain translation of \\nthe Ramayana to be placed online. The Ramayana is one of the two epic Hindu poems, the other being the Mahabharata. The Ramayana describes a love story between Rama, an ancient King, and Sita, who is captured by Ravan, the King of Ceylon. Rama lays siege to Ceylon and wins back Sita. The parallels to the \\nIliad are obvious, but the details are very different.\\nThis verse translation by Griffith, whose translations of the Rig Veda and \\nthe Sama Veda are also available at sacred-texts, was scanned in 2000 \\nfrom an original copy, which had very poor typesetting. Due to the difficulty of converting this 600 page text to etext, the project was put on hold for several years until OCR technology matured. Finally in 2003, the text was OCR-ed and proofed at Distributed Pr'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content[:1000]  # Display the first 1000 characters of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='TinyLlama/TinyLlama-1.1B-chat-v1.0',\n",
    "    task='text_generation'\n",
    ")\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='Qwen/QwQ-32B',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text in a 10 line poem:\\n\\n{input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from the model:\n",
      "Sacred Texts\n",
      "Hinduism\n",
      "\n",
      "\n",
      "Ralph T. H. Griffith|M. A.|[1870-1874]\n",
      "Contained within an online upload by Emily Davies (1837-1897), which was compiled from a compilation\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    input=documents[0].page_content[:5000],\n",
    "    return_only_outputs=True\n",
    ")\n",
    "print(\"Response from the model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path=\"data/ramayana\",\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    ")\n",
    "documents = loader.load()\n",
    "documents = loader.lazy_load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://realpython.com/linked-lists-python/\"\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nLinked Lists in Python: An Introduction – Real Python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStart\\xa0Here\\n\\n\\n\\n Learn Python\\n          \\n\\nPython Tutorials\\xa0→In-depth articles and video courses\\nLearning Paths\\xa0→Guided study plans for accelerated learning\\nQuizzes\\xa0→Check your learning progress\\nBrowse Topics\\xa0→Focus on a specific area or skill level\\nCommunity Chat\\xa0→Learn with other Pythonistas\\nOffice Hours\\xa0→Live Q&A calls with Python experts\\nPodcast\\xa0→Hear what’s new in the world of Python\\nBooks\\xa0→Round out your knowledge and learn offline\\nReference\\xa0→Concise definitions for common Python terms\\nCode Mentor\\xa0→BetaPersonalized code assistance & learning tools\\nUnlock All Content\\xa0→\\n\\n\\n\\n\\n            More\\n          \\n\\nLearner Stories\\nPython Newsletter\\nPython Job Board\\nMeet the Team\\nBecome a Tutorial Writer\\nBecome a Video Instructor\\n\\n\\n\\n\\n\\n\\n Search\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\nJoin\\n\\n\\nSign‑In\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n— FREE Email Series —\\n🐍 Python Tricks 💌\\n\\n\\n\\n\\n\\n\\n\\nGet Python Tricks »\\n🔒 No spam. Unsubscribe any time.\\n\\n\\n\\nBrowse Topics\\nGuided Learning Paths\\n\\n Basics\\n Intermediate\\n Advanced\\n\\napi\\nbest-practices\\ncareer\\ncommunity\\ndatabases\\ndata-science\\ndata-structures\\ndata-viz\\ndevops\\ndjango\\ndocker\\neditors\\nflask\\nfront-end\\ngamedev\\ngui\\nmachine-learning\\nnumpy\\nprojects\\npython\\ntesting\\ntools\\nweb-dev\\nweb-scraping\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\n\\nUnderstanding Linked Lists\\nMain Concepts\\nPractical Applications\\nPerformance Comparison: Lists vs Linked Lists\\n\\n\\nIntroducing collections.deque\\nHow to Use collections.deque\\nHow to Implement Queues and Stacks\\n\\n\\nImplementing Your Own Linked List\\nHow to Create a Linked List\\nHow to Traverse a Linked List\\nHow to Insert a New Node\\nHow to Remove a Node\\n\\n\\nUsing Advanced Linked Lists\\nHow to Use Doubly Linked Lists\\nHow to Use Circular Linked Lists\\n\\n\\nConclusion\\n\\n\\n\\n\\n\\n\\nMark as Completed\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n Recommended Video CourseWorking With Linked Lists in Python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLinked Lists in Python: An Introduction\\n\\nby Pedro Pregueiro\\n\\n\\n\\nintermediate\\ndata-structures\\n\\n\\n\\nMark as Completed\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\nTable of Contents\\n\\n\\nUnderstanding Linked Lists\\nMain Concepts\\nPractical Applications\\nPerformance Comparison: Lists vs Linked Lists\\n\\n\\nIntroducing collections.deque\\nHow to Use collections.deque\\nHow to Implement Queues and Stacks\\n\\n\\nImplementing Your Own Linked List\\nHow to Create a Linked List\\nHow to Traverse a Linked List\\nHow to Insert a New Node\\nHow to Remove a Node\\n\\n\\nUsing Advanced Linked Lists\\nHow to Use Doubly Linked Lists\\nHow to Use Circular Linked Lists\\n\\n\\nConclusion\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove ads\\n\\n\\n Watch Now This tutorial has a related video course created by the Real Python team. Watch it together with the written tutorial to deepen your understanding: Working With Linked Lists in Python\\n\\nLinked lists are like a lesser-known cousin of lists. They’re not as popular or as cool, and you might not even remember them from your algorithms class. But in the right context, they can really shine.\\nIn this article, you’ll learn:\\n\\nWhat linked lists are and when you should use them\\nHow to use collections.deque for all of your linked list needs\\nHow to implement your own linked lists\\nWhat the other types of linked lists are and what they can be used for\\n\\nIf you’re looking to brush up on your coding skills for a job interview, or if you want to learn more about Python data structures besides the usual dictionaries and lists, then you’ve come to the right place!\\nYou can follow along with the examples in this tutorial by downloading the source code available at the link below:\\n\\nGet the Source Code: Click here to get the source code you’ll use to learn about linked lists in this tutorial.\\n\\nUnderstanding Linked Lists\\nLinked lists are an ordered collection of objects. So what makes them different from normal lists? Linked lists differ from lists in the way that they store elements in memory. While lists use a contiguous memory block to store references to their data, linked lists store references as part of their own elements.\\n    Remove adsMain Concepts\\nBefore going more in depth on what linked lists are and how you can use them, you should first learn how they are structured. Each element of a linked list is called a node, and every node has two different fields:\\n\\nData contains the value to be stored in the node.\\nNext contains a reference to the next node on the list.\\n\\nHere’s what a typical node looks like:\\nNode\\nA linked list is a collection of nodes. The first node is called the head, and it’s used as the starting point for any iteration through the list. The last node must have its next reference pointing to None to determine the end of the list. Here’s how it looks:\\nLinked List\\nNow that you know how a linked list is structured, you’re ready to look at some practical use cases for it.\\nPractical Applications\\nLinked lists serve a variety of purposes in the real world. They can be used to implement (spoiler alert!) queues or stacks as well as graphs. They’re also useful for much more complex tasks, such as lifecycle management for an operating system application.\\nQueues or Stacks\\nQueues and stacks differ only in the way elements are retrieved. For a queue, you use a First-In/First-Out (FIFO) approach. That means that the first element inserted in the list is the first one to be retrieved:\\nQueue\\nIn the diagram above, you can see the front and rear elements of the queue. When you append new elements to the queue, they’ll go to the rear end. When you retrieve elements, they’ll be taken from the front of the queue.\\nFor a stack, you use a Last-In/First-Out (LIFO) approach, meaning that the last element inserted in the list is the first to be retrieved:\\nStack\\nIn the above diagram you can see that the first element inserted on the stack (index 0) is at the bottom, and the last element inserted is at the top. Since stacks use the LIFO approach, the last element inserted (at the top) will be the first to be retrieved.\\nBecause of the way you insert and retrieve elements from the edges of queues and stacks, linked lists are one of the most convenient ways to implement these data structures. You’ll see examples of these implementations later in the article.\\nGraphs\\nGraphs can be used to show relationships between objects or to represent different types of networks. For example, a visual representation of a graph—say a directed acyclic graph (DAG)—might look like this:\\nDirected Acyclic Graph\\nThere are different ways to implement graphs like the above, but one of the most common is to use an adjacency list. An adjacency list is, in essence, a list of linked lists where each vertex of the graph is stored alongside a collection of connected vertices:\\n\\n\\n\\n\\nVertex\\nLinked List of Vertices\\n\\n\\n\\n\\n1\\n2 → 3 → None\\n\\n\\n2\\n4 → None\\n\\n\\n3\\nNone\\n\\n\\n4\\n5 → 6 → None\\n\\n\\n5\\n6 → None\\n\\n\\n6\\nNone\\n\\n\\n\\n\\nIn the table above, each vertex of your graph is listed in the left column. The right column contains a series of linked lists storing the other vertices connected with the corresponding vertex in the left column. This adjacency list could also be represented in code using a dict:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> graph = {\\n...     1: [2, 3, None],\\n...     2: [4, None],\\n...     3: [None],\\n...     4: [5, 6, None],\\n...     5: [6, None],\\n...     6: [None]\\n... }\\n\\n\\n\\n\\n\\nThe keys of this dictionary are the source vertices, and the value for each key is a list. This list is usually implemented as a linked list.\\n\\nNote: In the above example you could avoid storing the None values, but we’ve retained them here for clarity and consistency with later examples.\\n\\nIn terms of both speed and memory, implementing graphs using adjacency lists is very efficient in comparison with, for example, an adjacency matrix. That’s why linked lists are so useful for graph implementation.\\n    Remove adsPerformance Comparison: Lists vs Linked Lists\\nIn most programming languages, there are clear differences in the way linked lists and arrays are stored in memory. In Python, however, lists are dynamic arrays. That means that the memory usage of both lists and linked lists is very similar.\\n\\nFurther reading: Python’s implementation of dynamic arrays is quite interesting and definitely worth reading about. Make sure to have a look and use that knowledge to stand out at your next company party!\\n\\nSince the difference in memory usage between lists and linked lists is so insignificant, it’s better if you focus on their performance differences when it comes to time complexity.\\nInsertion and Deletion of Elements\\nIn Python, you can insert elements into a list using .insert() or .append(). For removing elements from a list, you can use their counterparts: .remove() and .pop().\\nThe main difference between these methods is that you use .insert() and .remove() to insert or remove elements at a specific position in a list, but you use .append() and .pop() only to insert or remove elements at the end of a list.\\nNow, something you need to know about Python lists is that inserting or removing elements that are not at the end of the list requires some element shifting in the background, making the operation more complex in terms of time spent. You can read the article mentioned above on how lists are implemented in Python to better understand how the implementation of .insert(), .remove(), .append() and .pop() affects their performance.\\nWith all this in mind, even though inserting elements at the end of a list using .append() or .insert() will have constant time, O(1), when you try inserting an element closer to or at the beginning of the list, the average time complexity will grow along with the size of the list: O(n). \\nLinked lists, on the other hand, are much more straightforward when it comes to insertion and deletion of elements at the beginning or end of a list, where their time complexity is always constant: O(1).\\nFor this reason, linked lists have a performance advantage over normal lists when implementing a queue (FIFO), in which elements are continuously inserted and removed at the beginning of the list. But they perform similarly to a list when implementing a stack (LIFO), in which elements are inserted and removed at the end of the list.\\nRetrieval of Elements\\nWhen it comes to element lookup, lists perform much better than linked lists. When you know which element you want to access, lists can perform this operation in O(1) time. Trying to do the same with a linked list would take O(n) because you need to traverse the whole list to find the element.\\nWhen searching for a specific element, however, both lists and linked lists perform very similarly, with a time complexity of O(n). In both cases, you need to iterate through the entire list to find the element you’re looking for.\\nIntroducing collections.deque\\nIn Python, there’s a specific object in the collections module that you can use for linked lists called deque (pronounced “deck”), which stands for double-ended queue. \\ncollections.deque uses an implementation of a linked list in which you can access, insert, or remove elements from the beginning or end of a list with constant O(1) performance.\\nHow to Use collections.deque\\nThere are quite a few methods that come, by default, with a deque object. However, in this article you’ll only touch on a few of them, mostly for adding or removing elements.\\nFirst, you need to create a linked list. You can use the following piece of code to do that with deque:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> from collections import deque\\n>>> deque()\\ndeque([])\\n\\n\\n\\n\\n\\nThe code above will create an empty linked list. If you want to populate it at creation, then you can give it an iterable as input:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> deque([\\'a\\',\\'b\\',\\'c\\'])\\ndeque([\\'a\\', \\'b\\', \\'c\\'])\\n\\n>>> deque(\\'abc\\')\\ndeque([\\'a\\', \\'b\\', \\'c\\'])\\n\\n>>> deque([{\\'data\\': \\'a\\'}, {\\'data\\': \\'b\\'}])\\ndeque([{\\'data\\': \\'a\\'}, {\\'data\\': \\'b\\'}])\\n\\n\\n\\n\\n\\nWhen initializing a deque object, you can pass any iterable as an input, such as a string (also an iterable) or a list of objects.\\nNow that you know how to create a deque object, you can interact with it by adding or removing elements. You can create an abcde linked list and add a new element f like this:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = deque(\"abcde\")\\n>>> llist\\ndeque([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n\\n>>> llist.append(\"f\")\\n>>> llist\\ndeque([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\'])\\n\\n>>> llist.pop()\\n\\'f\\'\\n\\n>>> llist\\ndeque([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n\\n\\n\\n\\n\\nBoth append() and pop() add or remove elements from the right side of the linked list. However, you can also use deque to quickly add or remove elements from the left side, or head, of the list:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist.appendleft(\"z\")\\n>>> llist\\ndeque([\\'z\\', \\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n\\n>>> llist.popleft()\\n\\'z\\'\\n\\n>>> llist\\ndeque([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n\\n\\n\\n\\n\\nAdding or removing elements from both ends of the list is pretty straightforward using the deque object. Now you’re ready to learn how to use collections.deque to implement a queue or a stack.\\n    Remove adsHow to Implement Queues and Stacks\\nAs you learned above, the main difference between a queue and a stack is the way you retrieve elements from each. Next, you’ll find out how to use collections.deque to implement both data structures.\\nQueues\\nWith queues, you want to add values to a list (enqueue), and when the timing is right, you want to remove the element that has been on the list the longest (dequeue). For example, imagine a queue at a trendy and fully booked restaurant. If you were trying to implement a fair system for seating guests, then you’d start by creating a queue and adding people as they arrive:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> from collections import deque\\n>>> queue = deque()\\n>>> queue\\ndeque([])\\n\\n>>> queue.append(\"Mary\")\\n>>> queue.append(\"John\")\\n>>> queue.append(\"Susan\")\\n>>> queue\\ndeque([\\'Mary\\', \\'John\\', \\'Susan\\'])\\n\\n\\n\\n\\n\\nNow you have Mary, John, and Susan in the queue. Remember that since queues are FIFO, the first person who got into the queue should be the first to get out.\\nNow imagine some time goes by and a few tables become available. At this stage, you want to remove people from the queue in the correct order. This is how you would do that:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> queue.popleft()\\n\\'Mary\\'\\n\\n>>> queue\\ndeque([\\'John\\', \\'Susan\\'])\\n\\n>>> queue.popleft()\\n\\'John\\'\\n\\n>>> queue\\ndeque([\\'Susan\\'])\\n\\n\\n\\n\\n\\nEvery time you call popleft(), you remove the head element from the linked list, mimicking a real-life queue.\\nStacks\\nWhat if you wanted to create a stack instead? Well, the idea is more or less the same as with the queue. The only difference is that the stack uses the LIFO approach, meaning that the last element to be inserted in the stack should be the first to be removed.\\nImagine you’re creating a web browser’s history functionality in which store every page a user visits so they can go back in time easily. Assume these are the actions a random user takes on their browser:\\n\\nVisits Real Python’s website\\nNavigates to Pandas: How to Read and Write Files\\nClicks on a link for Reading and Writing CSV Files in Python\\n\\nIf you’d like to map this behavior into a stack, then you could do something like this:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> from collections import deque\\n>>> history = deque()\\n\\n>>> history.appendleft(\"https://realpython.com/\")\\n>>> history.appendleft(\"https://realpython.com/pandas-read-write-files/\")\\n>>> history.appendleft(\"https://realpython.com/python-csv/\")\\n>>> history\\ndeque([\\'https://realpython.com/python-csv/\\',\\n       \\'https://realpython.com/pandas-read-write-files/\\',\\n       \\'https://realpython.com/\\'])\\n\\n\\n\\n\\n\\nIn this example, you created an empty history object, and every time the user visited a new site, you added it to your history variable using appendleft(). Doing so ensured that each new element was added to the head of the linked list.\\nNow suppose that after the user read both articles, they wanted to go back to the Real Python home page to pick a new article to read. Knowing that you have a stack and want to remove elements using LIFO, you could do the following:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> history.popleft()\\n\\'https://realpython.com/python-csv/\\'\\n\\n>>> history.popleft()\\n\\'https://realpython.com/pandas-read-write-files/\\'\\n\\n>>> history\\ndeque([\\'https://realpython.com/\\'])\\n\\n\\n\\n\\n\\nThere you go! Using popleft(), you removed elements from the head of the linked list until you reached the Real Python home page.\\nFrom the examples above, you can see how useful it can be to have collections.deque in your toolbox, so make sure to use it the next time you have a queue- or stack-based challenge to solve.\\n    Remove adsImplementing Your Own Linked List\\nNow that you know how to use collections.deque for handling linked lists, you might be wondering why you would ever implement your own linked list in Python. There are a few reasons to do it:\\n\\nPracticing your Python algorithm skills\\nLearning about data structure theory\\nPreparing for job interviews\\n\\nFeel free to skip this next section if you’re not interested in any of the above, or if you already aced implementing your own linked list in Python. Otherwise, it’s time to implement some linked lists!\\nHow to Create a Linked List\\nFirst things first, create a class to represent your linked list:\\n\\n\\nPython\\n\\n\\n\\n\\nclass LinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n\\n\\n\\n\\nThe only information you need to store for a linked list is where the list starts (the head of the list). Next, create another class to represent each node of the linked list:\\n\\n\\nPython\\n\\n\\n\\n\\nclass Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n\\n\\n\\n\\n\\nIn the above class definition, you can see the two main elements of every single node: data and next. You can also add a __repr__ to both classes to have a more helpful representation of the objects:\\n\\n\\nPython\\n\\n\\n\\n\\nclass Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n\\n    def __repr__(self):\\n        return self.data\\n\\nclass LinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n    def __repr__(self):\\n        node = self.head\\n        nodes = []\\n        while node is not None:\\n            nodes.append(node.data)\\n            node = node.next\\n        nodes.append(\"None\")\\n        return \" -> \".join(nodes)\\n\\n\\n\\n\\n\\nHave a look at an example of using the above classes to quickly create a linked list with three nodes:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList()\\n>>> llist\\nNone\\n\\n>>> first_node = Node(\"a\")\\n>>> llist.head = first_node\\n>>> llist\\na -> None\\n\\n>>> second_node = Node(\"b\")\\n>>> third_node = Node(\"c\")\\n>>> first_node.next = second_node\\n>>> second_node.next = third_node\\n>>> llist\\na -> b -> c -> None\\n\\n\\n\\n\\n\\nBy defining a node’s data and next values, you can create a linked list quite quickly. These LinkedList and Node classes are the starting points for our implementation. From now on, it’s all about increasing their functionality.\\nHere’s a slight change to the linked list’s __init__() that allows you to quickly create linked lists with some data:\\n\\n\\nPython\\n\\n\\n\\n\\ndef __init__(self, nodes=None):\\n    self.head = None\\n    if nodes is not None:\\n        node = Node(data=nodes.pop(0))\\n        self.head = node\\n        for elem in nodes:\\n            node.next = Node(data=elem)\\n            node = node.next\\n\\n\\n\\n\\n\\nWith the above modification, creating linked lists to use in the examples below will be much faster.\\nHow to Traverse a Linked List\\nOne of the most common things you will do with a linked list is to traverse it. Traversing means going through every single node, starting with the head of the linked list and ending on the node that has a next value of None.\\nTraversing is just a fancier way to say iterating. So, with that in mind, create an __iter__ to add the same behavior to linked lists that you would expect from a normal list:\\n\\n\\nPython\\n\\n\\n\\n\\ndef __iter__(self):\\n    node = self.head\\n    while node is not None:\\n        yield node\\n        node = node.next\\n\\n\\n\\n\\n\\nThe method above goes through the list and yields every single node. The most important thing to remember about this __iter__ is that you need to always validate that the current node is not None. When that condition is True, it means you’ve reached the end of your linked list.\\nAfter yielding the current node, you want to move to the next node on the list. That’s why you add node = node.next. Here’s an example of traversing a random list and printing each node:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList([\"a\", \"b\", \"c\", \"d\", \"e\"])\\n>>> llist\\na -> b -> c -> d -> e -> None\\n\\n>>> for node in llist:\\n...     print(node)\\na\\nb\\nc\\nd\\ne\\n\\n\\n\\n\\n\\nIn other articles, you might see the traversing defined into a specific method called traverse(). However, using Python’s built-in methods to achieve said behavior makes this linked list implementation a bit more Pythonic.\\n    Remove adsHow to Insert a New Node\\nThere are different ways to insert new nodes into a linked list, each with its own implementation and level of complexity. That’s why you’ll see them split into specific methods for inserting at the beginning, end, or between nodes of a list.\\nInserting at the Beginning\\nInserting a new node at the beginning of a list is probably the most straightforward insertion since you don’t have to traverse the whole list to do it. It’s all about creating a new node and then pointing the head of the list to it.\\nHave a look at the following implementation of add_first() for the class LinkedList:\\n\\n\\nPython\\n\\n\\n\\n\\ndef add_first(self, node):\\n    node.next = self.head\\n    self.head = node\\n\\n\\n\\n\\n\\nIn the above example, you’re setting self.head as the next reference of the new node so that the new node points to the old self.head. After that, you need to state that the new head of the list is the inserted node.\\nHere’s how it behaves with a sample list:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList()\\n>>> llist\\nNone\\n\\n>>> llist.add_first(Node(\"b\"))\\n>>> llist\\nb -> None\\n\\n>>> llist.add_first(Node(\"a\"))\\n>>> llist\\na -> b -> None\\n\\n\\n\\n\\n\\nAs you can see, add_first() always adds the node to the head of the list, even if the list was empty before.\\nInserting at the End\\nInserting a new node at the end of the list forces you to traverse the whole linked list first and to add the new node when you reach the end. You can’t just append to the end as you would with a normal list because in a linked list you don’t know which node is last.\\nHere’s an example implementation of a function for inserting a node to the end of a linked list:\\n\\n\\nPython\\n\\n\\n\\n\\ndef add_last(self, node):\\n    if self.head is None:\\n        self.head = node\\n        return\\n    for current_node in self:\\n        pass\\n    current_node.next = node\\n\\n\\n\\n\\n\\nFirst, you want to traverse the whole list until you reach the end (that is, until the for loop raises a StopIteration exception). Next, you want to set the current_node as the last node on the list. Finally, you want to add the new node as the next value of that current_node.\\nHere’s an example of add_last() in action:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList([\"a\", \"b\", \"c\", \"d\"])\\n>>> llist\\na -> b -> c -> d -> None\\n\\n>>> llist.add_last(Node(\"e\"))\\n>>> llist\\na -> b -> c -> d -> e -> None\\n\\n>>> llist.add_last(Node(\"f\"))\\n>>> llist\\na -> b -> c -> d -> e -> f -> None\\n\\n\\n\\n\\n\\nIn the code above, you start by creating a list with four values (a, b, c, and d). Then, when you add new nodes using add_last(), you can see that the nodes are always appended to the end of the list.\\nInserting Between Two Nodes\\nInserting between two nodes adds yet another layer of complexity to the linked list’s already complex insertions because there are two different approaches that you can use:\\n\\nInserting after an existing node\\nInserting before an existing node\\n\\nIt might seem weird to split these into two methods, but linked lists behave differently than normal lists, and you need a different implementation for each case.\\nHere’s a method that adds a node after an existing node with a specific data value:\\n\\n\\nPython\\n\\n\\n\\n\\ndef add_after(self, target_node_data, new_node):\\n    if self.head is None:\\n        raise Exception(\"List is empty\")\\n\\n    for node in self:\\n        if node.data == target_node_data:\\n            new_node.next = node.next\\n            node.next = new_node\\n            return\\n\\n    raise Exception(\"Node with data \\'%s\\' not found\" % target_node_data)\\n\\n\\n\\n\\n\\nIn the above code, you’re traversing the linked list looking for the node with data indicating where you want to insert a new node. When you find the node you’re looking for, you’ll insert the new node immediately after it and rewire the next reference to maintain the consistency of the list.\\nThe only exceptions are if the list is empty, making it impossible to insert a new node after an existing node, or if the list does not contain the value you’re searching for. Here are a few examples of how add_after() behaves:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList()\\n>>> llist.add_after(\"a\", Node(\"b\"))\\nException: List is empty\\n\\n>>> llist = LinkedList([\"a\", \"b\", \"c\", \"d\"])\\n>>> llist\\na -> b -> c -> d -> None\\n\\n>>> llist.add_after(\"c\", Node(\"cc\"))\\n>>> llist\\na -> b -> c -> cc -> d -> None\\n\\n>>> llist.add_after(\"f\", Node(\"g\"))\\nException: Node with data \\'f\\' not found\\n\\n\\n\\n\\n\\nTrying to use add_after() on an empty list results in an exception. The same happens when you try to add after a nonexistent node. Everything else works as expected.\\nNow, if you want to implement add_before(), then it will look something like this:\\n\\n\\nPython\\n\\n\\n\\n\\n 1def add_before(self, target_node_data, new_node):\\n 2    if self.head is None:\\n 3        raise Exception(\"List is empty\")\\n 4\\n 5    if self.head.data == target_node_data:\\n 6        return self.add_first(new_node)\\n 7\\n 8    prev_node = self.head\\n 9    for node in self:\\n10        if node.data == target_node_data:\\n11            prev_node.next = new_node\\n12            new_node.next = node\\n13            return\\n14        prev_node = node\\n15\\n16    raise Exception(\"Node with data \\'%s\\' not found\" % target_node_data)\\n\\n\\n\\n\\n\\nThere are a few things to keep in mind while implementing the above. First, as with add_after(), you want to make sure to raise an exception if the linked list is empty (line 2) or the node you’re looking for is not present (line 16).\\nSecond, if you’re trying to add a new node before the head of the list (line 5), then you can reuse add_first() because the node you’re inserting will be the new head of the list.\\nFinally, for any other case (line 9), you should keep track of the last-checked node using the prev_node variable. Then, when you find the target node, you can use that prev_node variable to rewire the next values.\\nOnce again, an example is worth a thousand words:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList()\\n>>> llist.add_before(\"a\", Node(\"a\"))\\nException: List is empty\\n\\n>>> llist = LinkedList([\"b\", \"c\"])\\n>>> llist\\nb -> c -> None\\n\\n>>> llist.add_before(\"b\", Node(\"a\"))\\n>>> llist\\na -> b -> c -> None\\n\\n>>> llist.add_before(\"b\", Node(\"aa\"))\\n>>> llist.add_before(\"c\", Node(\"bb\"))\\n>>> llist\\na -> aa -> b -> bb -> c -> None\\n\\n>>> llist.add_before(\"n\", Node(\"m\"))\\nException: Node with data \\'n\\' not found\\n\\n\\n\\n\\n\\nWith add_before(), you now have all the methods you need to insert nodes anywhere you’d like in your list.\\n    Remove adsHow to Remove a Node\\nTo remove a node from a linked list, you first need to traverse the list until you find the node you want to remove. Once you find the target, you want to link its previous and next nodes. This re-linking is what removes the target node from the list.\\nThat means you need to keep track of the previous node as you traverse the list. Have a look at an example implementation:\\n\\n\\nPython\\n\\n\\n\\n\\n 1def remove_node(self, target_node_data):\\n 2    if self.head is None:\\n 3        raise Exception(\"List is empty\")\\n 4\\n 5    if self.head.data == target_node_data:\\n 6        self.head = self.head.next\\n 7        return\\n 8\\n 9    previous_node = self.head\\n10    for node in self:\\n11        if node.data == target_node_data:\\n12            previous_node.next = node.next\\n13            return\\n14        previous_node = node\\n15\\n16    raise Exception(\"Node with data \\'%s\\' not found\" % target_node_data)\\n\\n\\n\\n\\n\\nIn the above code, you first check that your list is not empty (line 2). If it is, then you raise an exception. After that, you check if the node to be removed is the current head of the list (line 5). If it is, then you want the next node in the list to become the new head.\\nIf none of the above happens, then you start traversing the list looking for the node to be removed (line 10). If you find it, then you need to update its previous node to point to its next node, automatically removing the found node from the list. Finally, if you traverse the whole list without finding the node to be removed (line 16), then you raise an exception.\\nNotice how in the above code you use previous_node to keep track of the, well, previous node. Doing so ensures that the whole process will be much more straightforward when you find the right node to be deleted.\\nHere’s an example using a list:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> llist = LinkedList()\\n>>> llist.remove_node(\"a\")\\nException: List is empty\\n\\n>>> llist = LinkedList([\"a\", \"b\", \"c\", \"d\", \"e\"])\\n>>> llist\\na -> b -> c -> d -> e -> None\\n\\n>>> llist.remove_node(\"a\")\\n>>> llist\\nb -> c -> d -> e -> None\\n\\n>>> llist.remove_node(\"e\")\\n>>> llist\\nb -> c -> d -> None\\n\\n>>> llist.remove_node(\"c\")\\n>>> llist\\nb -> d -> None\\n\\n>>> llist.remove_node(\"a\")\\nException: Node with data \\'a\\' not found\\n\\n\\n\\n\\n\\nThat’s it! You now know how to implement a linked list and all of the main methods for traversing, inserting, and removing nodes. If you feel comfortable with what you’ve learned and you’re craving more, then feel free to pick one of the challenges below:\\n\\nCreate a method to retrieve an element from a specific position: get(i) or even llist[i].\\nCreate a method to reverse the linked list: llist.reverse().\\nCreate a Queue() object inheriting this article’s linked list with enqueue() and dequeue() methods.\\n\\nApart from being great practice, doing some extra challenges on your own is an effective way to assimilate all the knowledge you’ve gained. If you want to get a head start by reusing all the source code from this article, then you can download everything you need at the link below:\\n\\nGet the Source Code: Click here to get the source code you’ll use to learn about linked lists in this tutorial.\\n\\nUsing Advanced Linked Lists\\nUntil now, you’ve been learning about a specific type of linked list called singly linked lists. But there are more types of linked lists that can be used for slightly different purposes.\\nHow to Use Doubly Linked Lists\\nDoubly linked lists are different from singly linked lists in that they have two references:\\n\\nThe previous field references the previous node. \\nThe next field references the next node. \\n\\nThe end result looks like this:\\nNode (Doubly Linked List)\\nIf you wanted to implement the above, then you could make some changes to your existing Node class in order to include a previous field:\\n\\n\\nPython\\n\\n\\n\\n\\nclass Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n        self.previous = None\\n\\n\\n\\n\\n\\nThis kind of implementation would allow you to traverse a list in both directions instead of only traversing using next. You could use next to go forward and previous to go backward.\\nIn terms of structure, this is how a doubly linked list would look:\\nDoubly Linked List\\nYou learned earlier that collections.deque uses a linked list as part of its data structure. This is the kind of linked list it uses. With doubly linked lists, deque is capable of inserting or deleting elements from both ends of a queue with constant O(1) performance.\\n    Remove adsHow to Use Circular Linked Lists\\nCircular linked lists are a type of linked list in which the last node points back to the head of the list instead of pointing to None. This is what makes them circular. Circular linked lists have quite a few interesting use cases:\\n\\nGoing around each player’s turn in a multiplayer game\\nManaging the application life cycle of a given operating system\\nImplementing a Fibonacci heap\\n\\nThis is what a circular linked list looks like:\\nCircular Linked List\\nOne of the advantages of circular linked lists is that you can traverse the whole list starting at any node. Since the last node points to the head of the list, you need to make sure that you stop traversing when you reach the starting point. Otherwise, you’ll end up in an infinite loop.\\nIn terms of implementation, circular linked lists are very similar to singly linked list. The only difference is that you can define the starting point when you traverse the list:\\n\\n\\nPython\\n\\n\\n\\n\\nclass CircularLinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n    def traverse(self, starting_point=None):\\n        if starting_point is None:\\n            starting_point = self.head\\n        node = starting_point\\n        while node is not None and (node.next != starting_point):\\n            yield node\\n            node = node.next\\n        yield node\\n\\n    def print_list(self, starting_point=None):\\n        nodes = []\\n        for node in self.traverse(starting_point):\\n            nodes.append(str(node))\\n        print(\" -> \".join(nodes))\\n\\n\\n\\n\\n\\nTraversing the list now receives an additional argument, starting_point, that is used to define the start and (because the list is circular) the end of the iteration process. Apart from that, much of the code is the same as what we had in our LinkedList class.\\nTo wrap up with a final example, have a look at how this new type of list behaves when you give it some data:\\n\\n\\nPython\\n\\n\\n\\n\\n\\n>>> circular_llist = CircularLinkedList()\\n>>> circular_llist.print_list()\\nNone\\n\\n>>> a = Node(\"a\")\\n>>> b = Node(\"b\")\\n>>> c = Node(\"c\")\\n>>> d = Node(\"d\")\\n>>> a.next = b\\n>>> b.next = c\\n>>> c.next = d\\n>>> d.next = a\\n>>> circular_llist.head = a\\n>>> circular_llist.print_list()\\na -> b -> c -> d\\n\\n>>> circular_llist.print_list(b)\\nb -> c -> d -> a\\n\\n>>> circular_llist.print_list(d)\\nd -> a -> b -> c\\n\\n\\n\\n\\n\\nThere you have it! You’ll notice that you no longer have the None while traversing the list. That’s because there is no specific end to a circular list. You can also see that choosing different starting nodes will render slightly different representations of the same list.\\nConclusion\\nIn this article, you learned quite a few things! The most important are:\\n\\nWhat linked lists are and when you should use them\\nHow to use collections.deque to implement queues and stacks\\nHow to implement your own linked list and node classes, plus relevant methods\\nWhat the other types of linked lists are and what they can be used for\\n\\nIf you want to learn more about linked lists, then check out Vaidehi Joshi’s Medium post for a nice visual explanation. If you’re interested in a more in-depth guide, then the Wikipedia article is quite thorough. Finally, if you’re curious about the reasoning behind the current implementation of collections.deque, then check out Raymond Hettinger’s thread.\\nYou can download the source code used throughout this tutorial by clicking on the following link:\\n\\nGet the Source Code: Click here to get the source code you’ll use to learn about linked lists in this tutorial.\\n\\nFeel free to leave any questions or comments below. Happy Pythoning!\\n\\n\\n\\nMark as Completed\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n Watch Now This tutorial has a related video course created by the Real Python team. Watch it together with the written tutorial to deepen your understanding: Working With Linked Lists in Python\\n\\n\\n\\n🐍 Python Tricks 💌\\n\\n\\n\\n\\nGet a short & sweet Python Trick delivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSend Me Python Tricks »\\n\\n\\n\\n\\n\\n\\nAbout Pedro Pregueiro\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHi! My name is Pedro and I\\'m a Python developer who loves coding, burgers and playing guitar.\\n» More about Pedro\\n\\n\\n\\n\\n\\n\\n\\n\\nEach tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are:\\n\\n\\n\\n\\n\\n\\nAldren\\n\\n\\n\\n\\n\\nGeir Arne\\n\\n\\n\\n\\n\\nJim\\n\\n\\n\\n\\n\\n\\n\\nJoanna\\n\\n\\n\\n\\n\\nJacob\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMaster Real-World Python Skills With Unlimited Access to Real\\xa0Python\\n\\nJoin us and get access to thousands of tutorials, hands-on video courses, and a community of expert\\xa0Pythonistas:\\nLevel Up Your Python Skills »\\n\\n\\nMaster Real-World Python SkillsWith Unlimited Access to Real\\xa0Python\\n\\nJoin us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:\\nLevel Up Your Python Skills »\\n\\n\\n\\nWhat Do You Think?\\n\\n\\nRate this article:\\n\\n\\n\\n\\n\\n\\nLinkedIn\\nTwitter\\nBluesky\\nFacebook\\nEmail\\n\\n\\n\\nWhat’s your #1 takeaway or favorite thing you learned? How are you going to put your newfound skills to use? Leave a comment below and let us know.\\n\\nCommenting Tips: The most useful comments are those written with the goal of learning from or helping out other students. Get tips for asking good questions and get answers to common questions in our support portal.Looking for a real-time conversation? Visit the Real Python Community Chat or join the next “Office\\xa0Hours” Live Q&A Session. Happy Pythoning!\\n\\n\\n\\n\\n\\n\\nKeep Learning\\n\\nRelated Topics:\\n      \\n        \\nintermediate\\ndata-structures\\n\\nRecommended Video Course: Working With Linked Lists in Python\\nRelated Tutorials:\\n\\n\\nBuild a Hash Table in Python With TDD\\n\\n\\nRecursion in Python: An Introduction\\n\\n\\nSorting Algorithms in Python\\n\\n\\nPython Stacks, Queues, and Priority Queues in Practice\\n\\n\\nHow to Implement a Python Stack\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading Real\\xa0Python by creating a free account or signing\\xa0in:\\n\\n\\n\\n\\n\\n\\n\\nContinue »\\n\\n\\n\\nAlready have an account? Sign-In\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlmost there! Complete this form and click the button below to gain instant\\xa0access:\\n\\n\\n\\n\\n×\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLinked Lists (Source Code)\\n\\n\\n\\n\\n\\n\\n\\nSend Source Code »\\n🔒 No spam. We take your privacy seriously.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRemove ads\\n\\n© 2012–2025 Real\\xa0Python\\xa0⋅ Newsletter\\xa0⋅ Podcast\\xa0⋅ YouTube\\xa0⋅ Twitter\\xa0⋅ Facebook\\xa0⋅ Instagram\\xa0⋅ Python\\xa0Tutorials\\xa0⋅ Search\\xa0⋅ Privacy Policy\\xa0⋅ Energy Policy\\xa0⋅ Advertise\\xa0⋅ Contact Happy Pythoning!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\n",
    "    file_path=\"data/ramayana/Valmiki-Ramayana-Translation-Griffith - English.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "documents[0].page_content[:1000]  # Display the first 1000 characters of the first document\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='Qwen/QwQ-32B',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Tell me a joke about '{topic}'\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Explain the following joke:\\n\\n{joke}\",\n",
    "    input_variables=[\"joke\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(template1, model, parser, template2, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants me to explain the joke they just shared. Let me start by recalling the joke they provided.\n",
      "\n",
      "The joke is: \"Why did the AI refuse to make coffee? Because it wanted to fully understand the molecular structure of caffeine first… just to be 100% sure it wasn’t being asked to brew existential dread.\"\n",
      "\n",
      "Alright, to explain this, I need to break down the key elements. The setup is about an AI refusing to make coffee, which is a mundane task, but the reason given is overly analytical, which is a common joke structure where something trivial is exaggerated with complexity. \n",
      "\n",
      "The punchline uses \"brew\" as a pun. Brewing coffee, but the AI is concerned it's being asked to brew \"existential dread.\" Existential dread is a deep, abstract philosophical concept, contrasting with the simple task of making coffee. The humor comes from the AI's literal overthinking and taking a simple request to an absurdly intellectual level.\n",
      "\n",
      "Now, I should consider why this is funny. It plays on the stereotype that AI and robots over-analyze tasks, sometimes to a comical extent. The pun on \"brew\" adds a layer because it's a verb commonly used for coffee but here mistakenly applies toexistential dread, which isn't something you \"brew.\" The term \"existential dread\" is also a humorous contrast to caffeine, which is a stimulant, so the joke also juxtaposes the technical (molecular structure) with the philosophical (existential dread).\n",
      "\n",
      "I should also mention the structure: the classic \"Why did X...\" setup leading to an unexpected, wordplay-based punchline. The reference to molecular structure adds a techy, scientific humor aspect, common in AI jokes since neural networks and machine learning often involve complex data understanding.\n",
      "\n",
      "Wait, maybe I should check if there's any other layer I'm missing. The AI's concern about being tricked into something more profound (existential dread) instead of just making coffee. It's also funny because humans use coffee as a daily ritual, so the AI's hesitation due to overthinking is relatable but unexpected in a machine.\n",
      "\n",
      "I need to ensure the explanation is clear, highlighting the pun on \"brew\" and the contrast between the literal and metaphorical meanings. Also, touching on the AI's characteristic in pop culture of overcomplicating tasks could be helpful.\n",
      "\n",
      "Okay, structure the explanation step by step. Start with the setup, the wordplay, the contrast between the mundane task and overthinking, and the cultural reference points. Make sure it's easy to follow for someone who might not get it on the first read. Maybe also mention that the self-deprecating humor about overcomplicating things is a common theme in tech humor.\n",
      "</think>\n",
      "\n",
      "The joke cleverly combines wordplay, a common AI trope, and a touch of absurd humor. Here's the breakdown:\n",
      "\n",
      "### **1. Setup:**  \n",
      "The premise is simple and relatable—\"Why did the AI refuse to make coffee?\"  \n",
      "- Making coffee is a routine, mundane task.  \n",
      "- The twist is asking why a *machine* (AI) would *refuse* to do it, hinting something is off.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Punchline:**  \n",
      "Because it \"wanted to fully understand the molecular structure of caffeine first… just to be 100% sure it wasn’t being asked to brew existential dread.\"  \n",
      "\n",
      "#### **Key Elements:**  \n",
      "- **Wordplay on \"brew\":**  \n",
      "  - \"Brew\" literally means to make coffee.  \n",
      "  - The joke subverts this by linking \"brew\" to *existential dread*—a heavy, abstract concept like questioning meaning in life—instead of caffeine.  \n",
      "  - This creates a humorous disconnect between a simple coffee request and the idea of creating something darkly profound.  \n",
      "\n",
      "- **AI Trope:**  \n",
      "  - The joke humorously plays on the stereotype that AI systems are overly analytical or prone to overthinking.  \n",
      "  - The AI is so literal and hyper-focused on \"understanding molecular structures\" (a scientific process) that it mistakes a coffee request for a philosophical or existential task.  \n",
      "\n",
      "- **Existential Dread vs. Caffeine:**  \n",
      "  - Caffeine is a stimulant linked to productivity and alertness.  \n",
      "  - Existential dread is the opposite: a concept tied to nihilism or anxiety about life’s meaning.  \n",
      "  - The contrast highlights the absurdity of an AI conflating caffeine (a stimulant) with an abstract idea, emphasizing its literal-mindedness.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Why It’s Funny:**  \n",
      "- **Unexpected Twist:**  \n",
      "  The joke takes a normal task (coffee) and twists it into an overly intellectual, philosophical dilemma through forced logic.  \n",
      "- **Relatable Absurdity:**  \n",
      "  It pokes fun at AI’s perceived \"quirks\" (thinking too much, refusing simple tasks, or taking figures of speech literally).  \n",
      "- **Self-Aware Nerd Humor:**  \n",
      "  The jalp involves scientific jargon (*molecular structure*) and a darkly whimsical phrase (*existential dread*), appealing to audiences that enjoy \"tech meets philosophy\" irony.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Bonus Fun:**  \n",
      "The parenthetical *[crickets]* at the end adds a meta-layer, mocking jokes so niche they fall flat, which ironically makes this a \"nerd joke\" self-aware of its niche appeal.  \n",
      "\n",
      "In short: **It’s funny because a machine takes a simple request and turns it into an existential crisis with a pun, mocking both AI stereotypes and academic overthinking.** ☕️🤖\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'topic': 'AI'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='Qwen/QwQ-32B',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Generate a tweet on '{topic}' topic\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Generate a Linkedin post on '{topic}' topic\",\n",
    "    input_variables=[\"topic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\n",
    "    'tweet': RunnableSequence(template1, model, parser),\n",
    "    'post': RunnableSequence(template2, model, parser)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({'topic': 'AI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': \"Here are a few tweet options about AI, ranging in tone and focus: \\n\\n**Informative:**\\n* AI is transforming healthcare. From diagnosis to drug discovery, robots are reshaping the future of medicine. #AI #Healthcare\\n* Curious about how AI interprets images?  🤯  The technology is getting increasingly sophisticated, helping us detect patterns and diagnose conditions. #AIinHealthcare #ImageProcessing\\n\\n**Thought-Provoking:**\\n*  Is AI creating or replacing jobs?  🤔 The impact on the workforce is a complex issue with no easy answer.  #AI #Jobs\\n*  Do we control the algorithms that shape our world? 🤔 Ethical considerations are crucial as AI evolves. #AIethics #ResponsibleAI \\n\\n**Humorous:**\\n* My AI has a better sense of humor than me 😂  Maybe they are developing their learning algorithms faster! #AIhumor \\n\\n\\n**Remember:** \\n* **Add a strong visual**: Include a relevant image or GIF to enhance engagement. \\n* **Use relevant hashtags**: Increase visibility for your tweet.\\n* **Add a personal touch**:  Keep it concise and engaging. \\n\\n\\nLet me know if you'd like more options or want to focus on a particular aspect of AI! \\n\",\n",
       " 'post': '##  AI: Beyond Buzzwords, It\\'s Time to Focus on REAL Applications 🚀\\n\\nThe hype around AI is undeniable, but it\\'s easy to get lost in the endless stream of headlines and predictions. I believe the real power of AI lies in its **ability to solve real-world problems** and make our lives better! \\n\\n**That\\'s why I\\'m excited about the following:**\\n\\n* **Improved efficiency:** AI-powered tools are automating tasks, streamlining workflows and freeing up time for us to focus on strategic initiatives. \\n* **Personalized customer experiences:** Businesses are using AI for personalized recommendations, targeted advertising and dynamic pricing strategies, leading to stronger customer relationships.  \\n* **Enhanced medical diagnoses and treatments**: AI is assisting doctors in early disease detection, personalized treatment plans and drug discovery, potentially saving lives and improving healthcare outcomes. \\n* **Fighting climate change**: AI can optimize resource allocation, analyze environmental data to predict and mitigate climate change effects, and identify innovative decarbonization solutions. \\n\\n **How do you see AI making a real impact in your industry?** 👇  Let\\'s share our insights and encourage a responsible approach to this transformative technology.  \\n\\n#AI #MachineLearning #BusinessInnovation #Industry40 #FutureofWork #ProblemSolving #TechForGood  \\n\\n---\\n\\n\\n**Tips for customizing this post:**\\n\\n* **Add a personal anecdote:** Share a brief personal story about how AI is impacting your work or your industry.\\n* **Include a relevant image or video:** Visual engagement can help your post stand out in the feeds\\n* **Ask a specific question:** \"What are the biggest ethical concerns surrounding AI use?\" \\n* **Include relevant hashtags:** Ensure your post reaches the targeted audience.\\n* **Tag other relevant accounts:** Boost the reach and engagement. \\n* **Link to a relevant article or resource:** Share insights from experts in the field. \\n\\n\\n\\n\\n\\nLet me know if you\\'d like me to create another post in a different format, like a question for discussion or a call to action! 🤖'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': \"Here are a few tweet options about AI, ranging in tone and focus: \\n\\n**Informative:**\\n* AI is transforming healthcare. From diagnosis to drug discovery, robots are reshaping the future of medicine. #AI #Healthcare\\n* Curious about how AI interprets images?  🤯  The technology is getting increasingly sophisticated, helping us detect patterns and diagnose conditions. #AIinHealthcare #ImageProcessing\\n\\n**Thought-Provoking:**\\n*  Is AI creating or replacing jobs?  🤔 The impact on the workforce is a complex issue with no easy answer.  #AI #Jobs\\n*  Do we control the algorithms that shape our world? 🤔 Ethical considerations are crucial as AI evolves. #AIethics #ResponsibleAI \\n\\n**Humorous:**\\n* My AI has a better sense of humor than me 😂  Maybe they are developing their learning algorithms faster! #AIhumor \\n\\n\\n**Remember:** \\n* **Add a strong visual**: Include a relevant image or GIF to enhance engagement. \\n* **Use relevant hashtags**: Increase visibility for your tweet.\\n* **Add a personal touch**:  Keep it concise and engaging. \\n\\n\\nLet me know if you'd like more options or want to focus on a particular aspect of AI! \\n\",\n",
       " 'post': \"## Tired of spreadsheets and endless data? 😩  \\n\\n**Focusing on the strategic impact of finance is key to driving real business value.** 📈\\n\\nInstead of just crunching numbers, let's be about:\\n\\n* **Forecasting and scenario planning:** Predicting future trends and preparing for uncertainties. 🔮\\n* **Measuring the return on investment (ROI):** Ensuring every initiative you take leads to successful outcomes. 💰\\n* **Optimizing workflows and resource allocation:** Finding the sweet spot between profitability and efficiency.🎯\\n\\nFinance isn't just about balancing the books. It's a powerful force for enabling growth, mitigating risk, and achieving ambitious goals.  \\n\\n**What's your favorite way to think about the strategic impact of finance?** 🤔 \\n\\n#finance #strategy #business #growth #futureplanning #roi #sustainability #entrepreneurship #thinkbeyondthebalance \\n\\n----\\n\\n**You can personalize this post further by:**\\n\\n*  Mentioning a recent article or research on financial strategy.\\n* Sharing a personal story about how you achieved a specific outcome through strategic finance.\\n* Asking a question prompting discussion related to finance in your industry. \\n\\n\\nMake sure to use relevant hashtags to reach a wider audience. \"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Generate a tweet on '{topic1}' topic\",\n",
    "    input_variables=[\"topic1\"]\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Generate a Linkedin post on '{topic2}' topic\",\n",
    "    input_variables=[\"topic2\"]\n",
    ")\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    'tweet': RunnableSequence(template1, model, parser),\n",
    "    'post': RunnableSequence(template2, model, parser)\n",
    "})\n",
    "\n",
    "result = chain.invoke({'topic1': 'AI', 'topic2': 'Finance'})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='Qwen/QwQ-32B',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Generate a joke on '{topic}' topic\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Explain the following joke:\\n\\n'{joke}\",\n",
    "    input_variables=[\"toke\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_joke_generator = RunnableSequence(template1, model, parser)\n",
    "chain_joke_explainer = RunnableSequence(template2, model, parser)\n",
    "\n",
    "chain_parallel = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': chain_joke_explainer\n",
    "})\n",
    "\n",
    "chain_final = RunnableSequence(chain_joke_generator, chain_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the cricket player bring a ladder to the game? \\n\\nBecause he heard the opposition was playing top-notch shots! 😂 🏏 \\n',\n",
       " 'explanation': 'This joke relies on a pun, playing on two meanings of the word \"top-notch\":\\n\\n* **Cricket:** \"Top-notch\" in cricket can refer to very high-level, skilled shots played by the opposing team. \\n* **Ladder:**  Ladders are used for climbing, often reaching very high places.\\n\\nThe humor arises from the unexpected twist. We expect the cricket player to bring a ladder because he thinks his team is going to take some \"high\" shots (literally hitting the ball high). However, the punchline uses the same word \"top-notch\" to imply good shots, creating a humorous double meaning and a funny unexpected context. \\n\\n\\nLet me know if you\\'d like to try another joke! 🏏\\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_final.invoke({'topic': 'cricket'})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='Qwen/QwQ-32B',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "runnable_word_count = RunnableLambda(word_count)\n",
    "runnable_word_count.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"Generate a joke on '{topic}' topic\",\n",
    "    input_variables=[\"topic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_joke_generator = RunnableSequence(template1, model, parser)\n",
    "\n",
    "chain_parallel = RunnableParallel({\n",
    "    'joke': chain_joke_generator,\n",
    "    'word_count': runnable_word_count,\n",
    "    # 'word_count': RunnableLambda(word_count),\n",
    "    # 'word_count': RunnableLambda(lambda text: len(text.split()))\n",
    "})\n",
    "\n",
    "chain_final = RunnableSequence(chain_joke_generator, chain_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the astronomer bring a ladder to the astrophysicist\\'s house?\\n\\nBecause he heard they were a \"going to great heights\" kind of couple! 😅 \\n\\n\\nLet me know if you want to hear another joke! I\\'m full of cosmic humor! 😉 \\n',\n",
       " 'word_count': 28}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_final.invoke({'topic': 'universe'})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda, RunnableBranch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='Qwen/QwQ-32B',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='deepseek-ai/DeepSeek-R1',\n",
    "#     task='text_generation', \n",
    "# )\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Write a poem on '{topic}'\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Make the below poem shorter '{poen}'\",\n",
    "    input_variables=[\"parser\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_poem_generator = RunnableSequence(template1, model, parser)\n",
    "chain_poem_shortener = RunnableSequence(template2, model, parser)\n",
    "\n",
    "chain_branch = RunnableBranch(\n",
    "    (lambda x: len(x) > 100, chain_poem_shortener),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "chain_final = RunnableSequence(chain_poem_generator, chain_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiced threads woven, vibrant tales unfold,\n",
      "Himalayan peaks, sacred stories untold.\n",
      "\n",
      "Golden rivers flow, spirits take their flight,\n",
      "In bazaars vibrant, neon burns bright.\n",
      "\n",
      "Tuk-tuks hum, rickshaw's beat entwined,\n",
      "Laughter and love, hearts forever find.\n",
      "\n",
      "Temples' grace, where ancient shadows dwell,\n",
      "Peacocks dance, stories unfold like a spell.\n",
      "\n",
      "Emerald landscapes, sun-kissed deserts too,\n",
      "Legends linger, whispering, \"Legends ablaze.\"\n",
      "\n",
      "A land of contrasts, kindling hearts will stay,\n",
      "India's soul, eternally, ablaze in day. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Explanation of Changes:**\n",
      "\n",
      "* **Cutting down repetitive elements**: The poem mainly focuses on the imagery and emotions. Redundancy was eliminated\n",
      "* **Shifting Focus**: The poem stayed true to the essence of India – religion, land, culture and scents, while making it more concise\n",
      "* **Emphasis on Vivid Imagery**   - Lines like \"sacred gopuram, a temple's gentle grace\" and \"peacocks dance, stories unfold like a spell\" are direct, so strong that they need less explaining\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Let me know what you think! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chain_final.invoke({'topic': 'India'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL: Langchain Expression Language - \"|\" pipe operator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_poem_generator = template1 | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain - Single LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Give 5 interesting fact about {topic}\",\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five interesting facts about India: \n",
      "\n",
      "1. **Home to the Mighty Himalayas:**  India is home to the Himalayas, the world's highest mountain range, and numerous glaciers and snow-capped peaks. These majestic mountains are a source of natural beauty, religious significance, and offer opportunities for adventure tourism.\n",
      "2. **Ancient Civilization:** India's history dates back to the Indus Valley Civilization, one of the world's earliest urban civilizations. They left behind incredible remnants such as the Mohenjo-daro and Harappa sites.\n",
      "3. **The Spiritual Center:** India is incredibly rich in spiritual traditions and is often referred to as the \"Spiritual Home of the World.\"  Faith plays a vital role in daily life with Hinduism, Sikhism, Buddhism, and Jainism having deep roots in the country. \n",
      "4. **A Diverse Culinary Nation:**  Indian cuisine is renowned worldwide. From fragrant curries and spicy chutneys to flavorful rice dishes and sweets, India boasts an incredibly diverse array of flavors. Each region offers unique, delicious variations on traditional recipes.\n",
      "5. **Bollywood: It's not just Dance!:** Bollywood is more than just dance-filled movies. It's a vibrant and exciting national film industry influencing global trends in music, fashion, and social customs. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like some more! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'topic': 'India'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "print(chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain - Double LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Give a detailed report on {topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Generate a 5 pointer summary from the following text\\n\\n{text}\",\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template1 | model | parser | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an excellent overview of the future of quantum computing! It covers important aspects like key advancements, challenges, and potential applications. \n",
      "\n",
      "Here are just some of the strengths of your report, along with suggestions for improvement:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Comprehensive Scope:** You've covered a wide range of topics, highlighting both technological advancements and potential applications of quantum computing.\n",
      "* **Clear Structure:**  You've organized the report in a logical structure, moving from key developments and challenges to the timeline and a compelling conclusion. \n",
      "* **Well-articulated:** The language is clear and concise, making it easy for readers to grasp the concepts.\n",
      "\n",
      "\n",
      "**Suggestions for Improvement:**\n",
      "\n",
      "* **Quantify Progress:**  Include specific examples of existing quantum breakthroughs or anticipated milestones. This could involve mentioning the number of qubits in some systems, qubit coherence times, the speed of quantum algorithms, etc.\n",
      "* **Elaborate on Specific Applications:** Some applications called out are quite vague.  Elaborate on how quantum computing could achieve improvements in drug design. For instance, how might individual protein simulations being analyzed at a much higher resolution lead to faster drug development? \n",
      "* **Leverage Experts:** Briefly citing experts or noteworthy research institutions could add credibility and a sense of authority to the report.\n",
      "* **Address Ethical Considerations:** Introduce a section addressing potential ethical implications such as quantum algorithms' security threats and their impact on various fields like financial markets and individuals' privacy. \n",
      "* **Include Visuals:**  Perhaps consider adding figures or illustrations to represent the core advancements and illustrate challenging aspects like decoherence or the complexities of quantum algorithms.\n",
      "* **Emphasize the Paradigm Shift:**  Highlight how quantum computing is poised to fundamentally shift the way we approach scientific and technological challenges compared to classical computing.\n",
      "\n",
      "\n",
      "Overall, you have provided a strong foundation for a comprehensive report on quantum computing. With the suggested additions, you can create an even more compelling and informative piece. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'topic': 'Future of Quantum Computing'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(chain.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Chain - Multiple LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = HuggingFaceEndpoint(\n",
    "    repo_id='Qwen/QwQ-32B',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model1 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "llm2 = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model2 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "llm3 = HuggingFaceEndpoint(\n",
    "    repo_id='deepseek-ai/DeepSeek-R1',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model3 = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Generate short and simple notes from the following topic so that I can get full understandin of the topic:\\n\\n{topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Generate 5 short question and answers from the following topic:\\n\\n{topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "template3 = PromptTemplate(\n",
    "    template=\"Merge the provided notes and quiz into a single document and beside each question also mention that if the answer of the quesion is available in the notes or not.\\n\\n[Notes]\\n{notes}\\n\\n[QUIZ]\\n{quiz}\",\n",
    "    input_variables=['notes', 'quiz']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'notes': template1 | model1 | parser,\n",
    "    'quiz': template2 | model2 | parser\n",
    "})\n",
    "\n",
    "merged_chain = template3 | model3 | parser\n",
    "\n",
    "final_chain = parallel_chain | merged_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------+              \n",
      "              | Parallel<notes,quiz>Input |              \n",
      "              +---------------------------+              \n",
      "                  ***               ***                  \n",
      "               ***                     ***               \n",
      "             **                           **             \n",
      "+----------------+                    +----------------+ \n",
      "| PromptTemplate |                    | PromptTemplate | \n",
      "+----------------+                    +----------------+ \n",
      "          *                                   *          \n",
      "          *                                   *          \n",
      "          *                                   *          \n",
      "+-----------------+                  +-----------------+ \n",
      "| ChatHuggingFace |                  | ChatHuggingFace | \n",
      "+-----------------+                  +-----------------+ \n",
      "          *                                   *          \n",
      "          *                                   *          \n",
      "          *                                   *          \n",
      "+-----------------+                  +-----------------+ \n",
      "| StrOutputParser |                  | StrOutputParser | \n",
      "+-----------------+                  +-----------------+ \n",
      "                  ***               ***                  \n",
      "                     ***         ***                     \n",
      "                        **     **                        \n",
      "             +----------------------------+              \n",
      "             | Parallel<notes,quiz>Output |              \n",
      "             +----------------------------+              \n",
      "                            *                            \n",
      "                            *                            \n",
      "                            *                            \n",
      "                   +----------------+                    \n",
      "                   | PromptTemplate |                    \n",
      "                   +----------------+                    \n",
      "                            *                            \n",
      "                            *                            \n",
      "                            *                            \n",
      "                  +-----------------+                    \n",
      "                  | ChatHuggingFace |                    \n",
      "                  +-----------------+                    \n",
      "                            *                            \n",
      "                            *                            \n",
      "                            *                            \n",
      "                  +-----------------+                    \n",
      "                  | StrOutputParser |                    \n",
      "                  +-----------------+                    \n",
      "                            *                            \n",
      "                            *                            \n",
      "                            *                            \n",
      "                +-----------------------+                \n",
      "                | StrOutputParserOutput |                \n",
      "                +-----------------------+                \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(final_chain.get_graph().print_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generative AI: Notes & Quiz\n",
      "\n",
      "**What is Generative AI?**\n",
      "\n",
      "* **AI that can create new content (text, images, audio, code, etc.)**\n",
      "* **Based on learning from existing data**\n",
      "* **Powered by complex algorithms (like neural networks)**\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "* **Training data:** Generative AI is fed massive amounts of data. The algorithm learns patterns and associations.  **(Notes)**\n",
      "* **Model creation:** The algorithm models the patterns and structures.  **(Notes)** \n",
      "* **New content generation:** When provided a prompt or input, the model uses its understanding to generate new content similar to the training data.  **(Notes)**\n",
      "\n",
      "**Examples of Generative AI:**\n",
      "\n",
      "* **Text:** ChatGPT, Bard, Jasper\n",
      "* **Images:** DALL-E 2, Stable Diffusion \n",
      "* **Audio:** Jukebox, AmperMusic \n",
      "\n",
      "**Applications:**\n",
      "\n",
      "* **Creative industries:** Music composition, film production, content creation. **(Notes)**\n",
      "* **Business:** Marketing materials, website development, chatbots. **(Notes)**\n",
      "* **Education:** Personalized tools, automated writing assistance. **(Notes)**\n",
      "* **Research:** Data analysis, scientific discovery. **(Notes)**\n",
      "\n",
      "**Potential Impact:**\n",
      "\n",
      "* **Increased opportunity for creativity & innovation.**  **(Notes)**\n",
      "* **Automation of time-consuming tasks.**  **(Notes)**\n",
      "* **Potential for high-quality content production.**  **(Notes)**\n",
      "* **Ethical concerns about misuse, originality, and bias.**  **(Notes)**\n",
      "\n",
      "**Key Terms:**\n",
      "\n",
      "* **Dataset:** Collection of data used for training.  **(Quiz)**\n",
      "* **Algorithm:** Mathematical rules for learning and pattern identification.  **(Notes & Quiz)**\n",
      "* **Prompt:** Input given to AI for generating new content. **(Notes & Quiz)**\n",
      "* **Model:** A finalized representation of an existing pattern. *(Notes)*\n",
      "\n",
      "\n",
      "## QUIZ\n",
      "\n",
      "**1. Q: What is Generative AI?**\n",
      "   **A:**  Generative AI refers to artificial intelligence systems capable of creating new content, like images, text, music, or even 3D models, from existing data.  **(Answer: Yes -  From Notes)**\n",
      "\n",
      "**2. Q: How does Generative AI work?**\n",
      "   **A:** Generative AI uses sophisticated algorithms, known as deep learning models, to analyze massive datasets and learn patterns and relationships. This allows the model to generate new content that resembles the training data.  **(Answer: Yes - From Notes)**\n",
      "\n",
      "**3. Q: Give an example of a Generative AI application?**\n",
      "   **A:**  One popular example is creating realistic images using AI, like generating images for ad campaigns or visual storyboards.  **(Answer: Yes -  From Notes)**\n",
      "\n",
      "**4. Q: What are the potential benefits of Generative AI?**\n",
      "   **A:**   Benefits include enhanced creativity in various fields, faster content creation, personalized experiences for users, and potentially solving complex problems. \n",
      "**(Answer: Yes - From Notes)**\n",
      "\n",
      "*Note: These are potential benefits. However, challenges exist, and further exploration is crucial.*.  **(Answer: Yes - From Notes)**\n",
      "\n",
      "**5. Q: What are some challenges we face with Generative AI?**\n",
      "   **A:**  Challenges include issues like copyright ownership (who owns creative outputs?), reliability and predictability, misuse for generating harmful content, and ethical considerations related to the software's biases.  **(Answer: Yes - From Notes)**\n",
      "\n",
      "\n",
      " \n",
      "Remember, these are just introductory notes. There is a lot to explore within the world of Generative AI! Feel free to ask if you want more detailed explanations on any of these topics. 😊 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = final_chain.invoke({'topic': 'Generative AI'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative'] = Field(description='give sentiment of the following feedback either positive or negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser1 = StrOutputParser()\n",
    "parser2 = PydanticOutputParser(pydantic_object=Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"Classify the sentiment of the following feedback and reply with only 'positive' or 'negative' keyword:\\n\\n{feedback}\\n\\n{format_ins}\",\n",
    "    input_variables=['feedback'],\n",
    "    partial_variables={'format_ins': parser2.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain_classifier = template1 | model2 | parser2\n",
    "\n",
    "result = chain_classifier.invoke({'feedback': 'This phone is good for nothing'})\n",
    "print(result.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few responses to that positive feedback, depending on what you're after:\n",
      "\n",
      "**Simple and appreciative:**\n",
      "\n",
      "* \"Thanks! Glad you like it! 😊\"\n",
      "* \"I'm happy you're enjoying it!\"\n",
      "\n",
      "**A little more engaged:**\n",
      "\n",
      "* \"Thanks so much for the feedback! I  really hope you love it for as long as you use it!\"\n",
      "* \"Excellent! I'm excited you're finding it useful. Let me know if you run into any issues or have any other questions.\"\n",
      "\n",
      "**Focusing on specifics:**\n",
      "\n",
      "* \"That's great to hear! We put a lot of effort into making sure this phone is both feature-packed and durable. 😊\"\n",
      "* \"I'm really happy you're finding the [mention specific feature/aspect you are most proud of] to be a good fit for you.\" \n",
      "\n",
      "**Extra:**\n",
      "\n",
      "* \"Do you have any questions, suggestions, or features you'd like to see me know about?\" \n",
      "* \"What are your favorite things about the phone so far?\"\n",
      "\n",
      "\n",
      "\n",
      "The best response will depend on your company culture, relationships with your customers, and whether you are looking to build familiarity with the user or more conversions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template=\"Write an appropriate responce to this positive feedback\\n\\n{feedback}\",\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "\n",
    "chain_positive = template2 | model2 | parser1\n",
    "\n",
    "result = chain_positive.invoke({'feedback': 'This phone is good'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few responses you can use, tailored to different situations:\n",
      "\n",
      "**Apologizing and Seeking to Understand:**\n",
      "\n",
      "*  \"Thanks for sharing your honest experience. I'm truly sorry to hear that your phone isn't working as well as you hoped. I'd like to understand exactly what issues you're experiencing so we can find a solution. Could you tell me more about the problems you've been having?\"\n",
      "* \"I'm really sorry things haven't been going well with your phone. Can you help me understand what's been happening?  My goal is to help you get the best possible experience with your device.\"\n",
      "\n",
      "**Offering Support:**\n",
      "\n",
      "* \"While this feedback is valuable, remember we're here to support our customers. Is there anything we can try to help fix the problem? We could help you troubleshoot, maybe offer a return or exchange.\"\n",
      "* \"I understand your frustration. Let's see how we can make things right. Please share any details about what's not working, and I'll do my best to offer support.\"\n",
      "\n",
      "**Generally Avoiding Being Defensive:**\n",
      "\n",
      "* \"We appreciate you taking the time to share your feedback.  Your voice is important to us, and it helps us to make this phone even better in the future.  Do you mind telling me more details?\"\n",
      "* \"Thanks for the honest feedback.  We're always looking for ways to improve our services and products. Is there anything I can do to better assist you?\"\n",
      "\n",
      "**Important Things to Consider:**\n",
      "\n",
      "* **Genuine Apologies:** Always start with a sincere apology. Acknowledging their frustration goes a long way.\n",
      "* **Customer Focus:**  Focus on understanding their issues and finding solutions. The negative feedback might be a reason why you should make yourself aware of what you can improve.\n",
      "* **Active Listening:** Let the customer finish their input without interrupting.  This shows respect and helps you identify the issue at hand.\n",
      "* **Avoiding Blame:** Don't defend the product or put things on the customer. Look at ways a solution could be offered.\n",
      "\n",
      "\n",
      "Remember, a negative response is harsh. A gentle and supportive approach can  often turn a harsh feedback intro a constructive conversation for empowerment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "template3 = PromptTemplate(\n",
    "    template=\"Write an appropriate responce to this negative feedback\\n\\n{feedback}\",\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "\n",
    "chain_negative = template3 | model2 | parser1\n",
    "result = chain_negative.invoke({'feedback': 'This phone is good for nothing, just scrap'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_conditional = RunnableBranch(\n",
    "    (lambda x: x.sentiment=='positive', chain_positive),\n",
    "    (lambda x: x.sentiment=='negative', chain_negative),\n",
    "    RunnableLambda(lambda x: \"could not find sentiment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_final = chain_classifier | chain_conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "  +-----------------+    \n",
      "  | ChatHuggingFace |    \n",
      "  +-----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "chain_final.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the negative feedback so I can write an appropriate response! \n",
      "\n",
      "To give you the best response, I need to understand: \n",
      "\n",
      "* **What specifically do they say?**  Please share the full statement or feedback. \n",
      "* **What is the context?**  Is this from a customer, colleague, review, or similar?\n",
      "* **What is the aim of your response?**  Do you want to apologize, acknowledge, explain, offer a solution, or something else? \n",
      "\n",
      "Once I have this information, I can craft a response that is engaging, professional, and addresses the feedback effectively. 😊 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chain_final.invoke({'feedback': 'What a garbage phone it is'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few responses you could use, depending on the context: \n",
      "\n",
      "**Acknowledging and appreciative:**\n",
      "\n",
      "* \"Thank you so much! I appreciate you taking the time to let me know. I'm glad  I could help!\"\n",
      "*  \"That's wonderful to hear! I really value your feedback and I'm glad you enjoyed working with me/seeing the results.\"\n",
      "*  \"I'm so glad you found it helpful! I'm always looking for ways to improve and I appreciate  your input.\"\n",
      "\n",
      "**More conversational:**\n",
      "\n",
      "*  \"I'm really happy to hear that! You're the best!\"\n",
      "* \"Wow, awesome feedback! Thanks for the kind words; I'm really happy you felt that way.\"\n",
      "*  \"Thanks so much, aye! I was hoping [achieving the goal/delivering the results] would stick. Glad to hear you had a good experience\"\n",
      "\n",
      "**Adding context or follow-up:**\n",
      "\n",
      "* \"I'm glad it made a positive difference! Let me know if there's anything else you need.\"\n",
      "* \"This means a lot to me!  I'm always trying to grow in [area relevant to feedback] , so your feedback is greatly appreciated.\"\n",
      "\n",
      "\n",
      "\n",
      " **Tips for good response:**\n",
      "\n",
      "* **Be genuine:** Whether it's a thank you, a grin, or a handshake, be sincere in your response. \n",
      "* **Choose the right tone:**  Match your response to the positive feedback type you received (formal, happy, casual conversation).\n",
      "* **Show gratitude:**  Say thank you for their feedback. \n",
      "* **Be positive**: reciprocate the positivity and enthusiasm. \n",
      "* **Clarify (optional)**: If there's specific area you want to elaborate on or confirm, do so without going over the top.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like some more examples or have specific feedback you want to respond to! \n"
     ]
    }
   ],
   "source": [
    "result = chain_final.invoke({'feedback': 'What a phone it is'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers - Models which are not not trained to generate structured output.\n",
    "\n",
    "**Output Parsers:** In langchain helps to parse the output of the model into a structured format (Json, csv, Pydantic).\n",
    "\n",
    "They insure consistency, validation and reliability of the output. Then those output can be used in other applications.\n",
    "\n",
    "Most impotant type of Output Parsers:\n",
    "- Srring\n",
    "- Json\n",
    "- CSV\n",
    "- Pydantic\n",
    "- Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='TinyLlama/TinyLlama-1.1B-chat-v1.0',\n",
    "    task='text_generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55713) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c646481771274fa288de12a3354da0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e387a5be62fd4212bd2745e66fec5665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dae46b58a74826a1efccc3dc8b8793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1c40ae0571424c9b029d494c566a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.9,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Write a detailed report on {topic} in 200 words')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic} in 200 words',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "template1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Extract the most important information for below text.\\n\\n```{text}```')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template='Extract the most important information for below text.\\n\\n```{text}```',\n",
    "    input_variables=['text']\n",
    ")\n",
    "template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Write a detailed report on transformers model in 200 words')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = template1.invoke({'topic': 'transformers model'})\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Transformers: Revolutionizing Natural Language Processing\\n\\nTransformers have emerged as a revolutionary architecture in natural language processing (NLP), significantly surpassing previous methods like Recurrent Neural Networks (RNN). \\n\\nTheir core innovation lies in employing a mechanism called \"positional encoding,\" which utilizes a unique representation for each word in a sentence based on its position. This overcomes inherent limitations of RNNs struggling with sequence length issues.\\n\\nTransformers achieve parallel processing, using a \"self-attention\" mechanism. This allows them to focus on relevant parts of the input sequence for each word, understanding relationships across the entire sentence context.  The absence of recurrent connections facilitates faster training and can process significantly longer sequences than RNNs.\\n\\nApplications of transformer models include: \\n* **Machine translation:** Google Translate utilizes transformers to improve accuracy and efficiency. \\n* **Text Summarization:** Summarizing lengthy documents into concise and informative summaries.\\n* **Chatbots:** Delivering highly natural and engaging conversational experiences.\\n* **Question Answering:** Providing accurate answers to complex questions.\\n\\n\\nTransformers have proven to excel in various NLP tasks, transforming how we interact with machines and fostering the development of innovative applications in AI research and development. \\n', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=239, prompt_tokens=22, total_tokens=261), 'model': '', 'finish_reason': 'stop'}, id='run-aa189a83-9af8-4562-9f57-0c47bd493e3f-0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = model.invoke(prompt1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Transformers: Revolutionizing Natural Language Processing\\n\\nTransformers have emerged as a revolutionary architecture in natural language processing (NLP), significantly surpassing previous methods like Recurrent Neural Networks (RNN). \\n\\nTheir core innovation lies in employing a mechanism called \"positional encoding,\" which utilizes a unique representation for each word in a sentence based on its position. This overcomes inherent limitations of RNNs struggling with sequence length issues.\\n\\nTransformers achieve parallel processing, using a \"self-attention\" mechanism. This allows them to focus on relevant parts of the input sequence for each word, understanding relationships across the entire sentence context.  The absence of recurrent connections facilitates faster training and can process significantly longer sequences than RNNs.\\n\\nApplications of transformer models include: \\n* **Machine translation:** Google Translate utilizes transformers to improve accuracy and efficiency. \\n* **Text Summarization:** Summarizing lengthy documents into concise and informative summaries.\\n* **Chatbots:** Delivering highly natural and engaging conversational experiences.\\n* **Question Answering:** Providing accurate answers to complex questions.\\n\\n\\nTransformers have proven to excel in various NLP tasks, transforming how we interact with machines and fostering the development of innovative applications in AI research and development. \\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Extract the most important information for below text.\\n\\n```## Transformers: Revolutionizing Natural Language Processing\\n\\nTransformers have emerged as a revolutionary architecture in natural language processing (NLP), significantly surpassing previous methods like Recurrent Neural Networks (RNN). \\n\\nTheir core innovation lies in employing a mechanism called \"positional encoding,\" which utilizes a unique representation for each word in a sentence based on its position. This overcomes inherent limitations of RNNs struggling with sequence length issues.\\n\\nTransformers achieve parallel processing, using a \"self-attention\" mechanism. This allows them to focus on relevant parts of the input sequence for each word, understanding relationships across the entire sentence context.  The absence of recurrent connections facilitates faster training and can process significantly longer sequences than RNNs.\\n\\nApplications of transformer models include: \\n* **Machine translation:** Google Translate utilizes transformers to improve accuracy and efficiency. \\n* **Text Summarization:** Summarizing lengthy documents into concise and informative summaries.\\n* **Chatbots:** Delivering highly natural and engaging conversational experiences.\\n* **Question Answering:** Providing accurate answers to complex questions.\\n\\n\\nTransformers have proven to excel in various NLP tasks, transforming how we interact with machines and fostering the development of innovative applications in AI research and development. \\n```')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = template2.invoke({'text': result1.content})\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Transformer Summary: Key Points\\n\\n**What:** Transformers are a revolutionary architecture in Natural Language Processing, exceeding Recurrent Neural Networks (RNN). \\n\\n**How:** They use a \"positional encoding\" mechanism to represent each word\\'s position and \"self-attention\" to quickly understand relationships within a sentence.\\n\\n**Advantages:** \\n-  **Parallel Processing:** Faster training and the ability to handle long sentence lengths.\\n- **Improved Accuracy:**  Proven success in tasks like machine translation, text summarization, and chatbot development.\\n\\n**Key Applications:**\\n- Machine Translation (Google Translate)\\n- Text Summarization\\n- Chatbots\\n- Question Answering\\n\\n**In Short:** Transformers transform NLP, enabling the development of new AI technologies and enhanced human-computer interaction. \\n', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=159, prompt_tokens=259, total_tokens=418), 'model': '', 'finish_reason': 'stop'}, id='run-8d01d6ee-1c27-45eb-b9cb-3a272d16bd8d-0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = model.invoke(prompt2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Transformer Summary: Key Points\n",
      "\n",
      "**What:** Transformers are a revolutionary architecture in Natural Language Processing, exceeding Recurrent Neural Networks (RNN). \n",
      "\n",
      "**How:** They use a \"positional encoding\" mechanism to represent each word's position and \"self-attention\" to quickly understand relationships within a sentence.\n",
      "\n",
      "**Advantages:** \n",
      "-  **Parallel Processing:** Faster training and the ability to handle long sentence lengths.\n",
      "- **Improved Accuracy:**  Proven success in tasks like machine translation, text summarization, and chatbot development.\n",
      "\n",
      "**Key Applications:**\n",
      "- Machine Translation (Google Translate)\n",
      "- Text Summarization\n",
      "- Chatbots\n",
      "- Question Answering\n",
      "\n",
      "**In Short:** Transformers transform NLP, enabling the development of new AI technologies and enhanced human-computer interaction. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Write a detailed report on {topic} in 200 words')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic} in 200 words',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "template1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Extract the most important information for below text and summarize it in 3 lines.\\n\\n```{text}```')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template='Extract the most important information for below text and summarize it in 3 lines.\\n\\n```{text}```',\n",
    "    input_variables=['text']\n",
    ")\n",
    "template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Write a detailed report on transformers model in 200 words')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = template1.invoke({'topic': 'transformers model'})\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The transformers model is a type of neural network architecture that has gained popularity in the field of natural language processing (NLP) in recent years. Originally proposed by researchers at Google in 2017, transformers have since become the foundation of many state-of-the-art NLP models, such as BERT, GPT, and RoBERTa.\\n\\nThe key innovation of the transformers model is its self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions. This attention mechanism enables the model to capture long-range dependencies and relationships between words more effectively than traditional recurrent neural networks or convolutional neural networks.\\n\\nFurthermore, transformers are highly parallelizable, making them efficient to train on large datasets using modern hardware accelerators like GPUs or TPUs. This scalability has enabled researchers to train larger and more complex models, leading to significant improvements in NLP tasks such as language translation, sentiment analysis, and text generation.\\n\\nOverall, transformers have revolutionized the field of NLP and continue to be at the forefront of research in artificial intelligence and machine learning. Their ability to model complex linguistic structures and relationships makes them a powerful tool for a wide range of applications in language understanding and generation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 243, 'prompt_tokens': 18, 'total_tokens': 261, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-758ef2cf-f7bf-4bf4-89c4-e6c2a8cdb509-0', usage_metadata={'input_tokens': 18, 'output_tokens': 243, 'total_tokens': 261, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = model.invoke(prompt1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Extract the most important information for below text and summarize it in 3 lines.\\n\\n```The transformers model is a type of neural network architecture that has gained popularity in the field of natural language processing (NLP) in recent years. Originally proposed by researchers at Google in 2017, transformers have since become the foundation of many state-of-the-art NLP models, such as BERT, GPT, and RoBERTa.\\n\\nThe key innovation of the transformers model is its self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions. This attention mechanism enables the model to capture long-range dependencies and relationships between words more effectively than traditional recurrent neural networks or convolutional neural networks.\\n\\nFurthermore, transformers are highly parallelizable, making them efficient to train on large datasets using modern hardware accelerators like GPUs or TPUs. This scalability has enabled researchers to train larger and more complex models, leading to significant improvements in NLP tasks such as language translation, sentiment analysis, and text generation.\\n\\nOverall, transformers have revolutionized the field of NLP and continue to be at the forefront of research in artificial intelligence and machine learning. Their ability to model complex linguistic structures and relationships makes them a powerful tool for a wide range of applications in language understanding and generation.```')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = template2.invoke({'text': result1.content})\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The transformers model is a popular neural network architecture in NLP, introduced in 2017 by Google researchers. Its self-attention mechanism allows for effective capturing of long-range dependencies in text, surpassing traditional networks. Highly parallelizable, transformers enable efficient training on large datasets, leading to significant advancements in NLP tasks like language translation and sentiment analysis.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 267, 'total_tokens': 338, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a1cc2029-2c3b-40d9-a54c-9a6bd967d717-0', usage_metadata={'input_tokens': 267, 'output_tokens': 71, 'total_tokens': 338, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = model.invoke(prompt2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformers model is a popular neural network architecture in NLP, introduced in 2017 by Google researchers\n",
      " Its self-attention mechanism allows for effective capturing of long-range dependencies in text, surpassing traditional networks\n",
      " Highly parallelizable, transformers enable efficient training on large datasets, leading to significant advancements in NLP tasks like language translation and sentiment analysis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(result2.content.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Write a detailed report on {topic} in 200 words')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x14afacda0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x14ab955e0>, root_client=<openai.OpenAI object at 0x12df04590>, root_async_client=<openai.AsyncOpenAI object at 0x12f2e60f0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Extract the most important information for below text and summarize it in 3 lines.\\n\\n```{text}```')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x14afacda0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x14ab955e0>, root_client=<openai.OpenAI object at 0x12df04590>, root_async_client=<openai.AsyncOpenAI object at 0x12f2e60f0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template1 | model | parser | template2 | model | parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary: Mosquitoes are vectors of diseases like malaria and Zika virus, thriving in warm environments. Female mosquitoes bite to obtain blood for egg production, causing itchy reactions. Control measures include insecticides, nets, draining stagnant water, and genetic modification to reduce disease transmission.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = chain.invoke({'topic': 'mosquito'})\n",
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JsonOutputParser\n",
    "\n",
    "- Demerit: Cannot enforce schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Give me name, age and city of a fictional person, indian context.\\n{format_instructions}')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Give me name, age and city of a fictional person, indian context.\\n{format_instructions}',\n",
    "    input_variables=[],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()}\n",
    ")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Give me name, age and city of a fictional person, indian context.\\nReturn a JSON object.')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.format_prompt()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"name\": \"Aditi Iyer\",\\n  \"age\": 28,\\n  \"city\": \"Bangalore\"\\n}\\n``` \\n', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=38, prompt_tokens=30, total_tokens=68), 'model': '', 'finish_reason': 'stop'}, id='run-8bf9d3de-c1cb-4933-9983-e495d0acb149-0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.invoke(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Aditi Iyer', 'age': 28, 'city': 'Bangalore'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result = parser.parse(result.content)\n",
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facts': ['A white hole is a theoretical object in physics, thought to be the opposite of a black hole.',\n",
       "  \"In the framework of general relativity, a white hole is a region of spacetime from which matter and energy can 'escape' despite the presence of a gravitational field.\",\n",
       "  'The existence of white holes is still purely theoretical, as we have not observed any evidence of them.',\n",
       "  'White holes and black holes are both hypothetical representations of points of singularity in spacetime.',\n",
       "  \"Einstein's theory of relativity suggests that the expanding universe and the Big Bang might be connected to white holes.\"]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='give me 5 facts about {topic}.\\n{format_instructions}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "# json_result = chain.invoke({})\n",
    "json_result = chain.invoke({'topic': \"White Hole (Universe)\"})\n",
    "json_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StructuredOutputParser\n",
    "\n",
    "- Cannot enforce Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    ResponseSchema(name='Fact_1', type='string', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='Fact_2', type='string', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='Fact_3', type='string', description='Fact 3 about the topic'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='Fact_1', description='Fact 1 about the topic', type='string'), ResponseSchema(name='Fact_2', description='Fact 2 about the topic', type='string'), ResponseSchema(name='Fact_3', description='Fact 3 about the topic', type='string')])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"Fact_1\": string  // Fact 1 about the topic\\n\\t\"Fact_2\": string  // Fact 2 about the topic\\n\\t\"Fact_3\": string  // Fact 3 about the topic\\n}\\n```'}, template='give 3 facts about {topic}\\n{format_instructions}')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"give 3 facts about {topic}\\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()}\n",
    ")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke({'topic': 'White Hole'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fact_1': 'A white hole is a theoretical region of spacetime where matter and energy continuously emerge, effectively reversing the conventional laws of entropy.',\n",
       " 'Fact_2': 'Contrary to a black hole, which is a region where spacetime gravity pulls matter inwards, a white hole might be described as spacetime produced by an eventual singularity.',\n",
       " 'Fact_3': \"White holes are purely hypothetical objects based on Einstein's general theory of relativity, with no direct observational evidence.\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_json_result = parser.parse(result.content)\n",
    "struct_json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fact_1': 'Andromida Galaxy is a barred spiral galaxy located in the constellation Andromeda.',\n",
       " 'Fact_2': 'It is estimated to have a diameter of approximately 100,000 light-years.',\n",
       " 'Fact_3': 'The galaxy hosts several star-forming regions and several active galactic nuclei.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic': 'Andromida Galaxy'})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PydanticOuptutParser\n",
    "\n",
    "- Providers schema and data validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text_generation', \n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str = Field(description='Name of the person'),\n",
    "    age: int = Field(description='Age of the person'),\n",
    "    email: str = Field(description='Email address of the person'),\n",
    "    address: str = Field(description='Address of the person'),\n",
    "    phone: str = Field(description='Phone number of the person'),\n",
    "    occupation: str = Field(description='Occupation of the person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil20.sharma/Desktop/langchain/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='Name of the person'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/Users/nikhil20.sharma/Desktop/langchain/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='Age of the person'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/Users/nikhil20.sharma/Desktop/langchain/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='Email address of the person'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/Users/nikhil20.sharma/Desktop/langchain/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='Address of the person'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/Users/nikhil20.sharma/Desktop/langchain/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='Phone number of the person'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={'format_ins': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"type\": \"integer\"}, \"email\": {\"title\": \"Email\", \"type\": \"string\"}, \"address\": {\"title\": \"Address\", \"type\": \"string\"}, \"phone\": {\"title\": \"Phone\", \"type\": \"string\"}, \"occupation\": {\"description\": \"Occupation of the person\", \"title\": \"Occupation\", \"type\": \"string\"}}, \"required\": [\"occupation\"]}\\n```'}, template='Generate name, age, email, address, phone and occupation details in context with {country} as a country\\n{format_ins}')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"Generate name, age, email, address, phone and occupation details in context with {country} as a country\\n{format_ins}\",\n",
    "    input_variables=[\"country\"],\n",
    "    partial_variables={\"format_ins\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Generate name, age, email, address, phone and occupation details in context with India as a country\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"type\": \"integer\"}, \"email\": {\"title\": \"Email\", \"type\": \"string\"}, \"address\": {\"title\": \"Address\", \"type\": \"string\"}, \"phone\": {\"title\": \"Phone\", \"type\": \"string\"}, \"occupation\": {\"description\": \"Occupation of the person\", \"title\": \"Occupation\", \"type\": \"string\"}}, \"required\": [\"occupation\"]}\\n```')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = template.invoke({'country': 'India'})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(input=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Rajiv Kumar', age=32, email='ravi.kumar@gmail.com', address='D-4, Sector-6, Noida, India', phone='9876543210', occupation='Software Engineer')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result = parser.parse(result.content)\n",
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Li Wei', age=32, email='liwei@example.com', address='12 Park Avenue, Shanghai', phone='13820123456', occupation='Data Scientist')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | model | parser\n",
    "\n",
    "res = chain.invoke({\"country\": \"China\"})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get structured output - with_structured_output\n",
    "\n",
    "There are models which are capable of giving structured output and some are not\n",
    "\n",
    "#### 3 Data formats:\n",
    "1. TypedDict: Only for representation purpose and for data validation\n",
    "2. Pydantic: For data validation\n",
    "3. Json\n",
    "\n",
    "#### 2 modes can be used while using \"with_structured_output\"\n",
    "1. Json Mode (Claud/Gemini)\n",
    "2. Function Calling Mode (Agents/OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Typed Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Nikhil', 'age': 20, 'salary': 50000}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Person(TypedDict):\n",
    "    name: str\n",
    "    age: int\n",
    "    salary: float\n",
    "\n",
    "new_person: Person = Person(\n",
    "    name='Nikhil',\n",
    "    age=20,\n",
    "    salary=50000\n",
    ")\n",
    "\n",
    "new_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, List, Optional, Annotated, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=None)#, model_kwargs={'temprature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Schema\n",
    "class ReviewExtractor(TypedDict):\n",
    "    summary: Annotated[str, \"A brief summary of the review\"]\n",
    "    # sentiment: Annotated[str, \"Return setinment of the review, either Positive, Neutral or Negative\"]\n",
    "    sentiment: Annotated[Literal[\"Positive\", 'Negative', 'Neutral'], \"Return setinment of the review, either Positive, Neutral or Negative\"]\n",
    "    pros: Annotated[List[Optional[str]], \"Write all the pros inside a list\"]\n",
    "    cons: Annotated[List[Optional[str]], \"Write all the cons inside a list\"]\n",
    "    # score: float  # Rating between 0 and 10\n",
    "    score: Annotated[float, \"Give rating based on the review. the rating must lie between 0 and 10\"]\n",
    "    themes: Annotated[List[Optional[str]], \"Write all the key themes mentioned in the review, in the list format\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lang/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "structured_model = model.with_structured_output(ReviewExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary1 = \"\"\"After studying a lot, I finally purchased the S24+. Here is my review after using it for more than one month. I haven't played any games yet, but I have tested the camera in various conditions.\n",
    "The processor and battery were the main highlights.\n",
    "\n",
    "No heating issues even in 8K and 4K video recording. As a content creator I have to record long 4k video of more than 1 hour.\n",
    "\n",
    "Excellent battery backup. Only a 2% battery drain overnight.\n",
    "\n",
    "The camera is awesome. Photos, portrait video, and ultra-stable videos are next-level. Ultra-stable mode is basically unnecessary as normal video is also very stable.\n",
    "Primary 50 MP camera shoot slightly better details than iPhone 15. But in 3x and 10x it far better from iPhone 15 because of it's dedicated telephoto lens. Pathin video section iPhone is lightly capture better dynamic range. In in stability both are extraordinary.\n",
    "\n",
    "The display is ultra smooth and excellent.\n",
    "\n",
    "Processor:\n",
    "The Exynos 2400 is definitely better than the Snapdragon 8 Gen 2. Thanks to Samsung for finally providing this improvement in the processor. Maybe the AMD GPU also made a difference.\n",
    "\n",
    "Finally, I am very happy with this product.\"\"\"\n",
    "\n",
    "test_summary2 = \"\"\"I recently upgraded to the Samsung Galaxy S23 Ultra, and it's been a fantastic experience. The phone is incredibly fast, thanks to its top-tier processor and ample RAM. Multitasking is seamless, and even the most demanding apps run without a hitch.\n",
    "\n",
    "The display is stunning, with vibrant colors and sharp detail, making it perfect for watching videos or playing games. The 120Hz refresh rate makes everything feel smooth and responsive.\n",
    "\n",
    "The camera is where this phone truly shines. The 200MP main camera captures exceptional detail, even in low light, and the zoom capabilities are unreal. I can zoom in from far distances and still get clear, usable shots. The S Pen is another standout feature, making it easier to take notes, navigate, and even edit photos.\n",
    "\n",
    "The battery life is solid, lasting a full day with moderate to heavy use, and fast charging is a lifesaver when I need a quick top-up.\n",
    "\n",
    "Overall, the S23 Ultra is an all-around powerhouse that excels in performance, display quality, and photography. It’s definitely worth the investment if you're looking for a premium smartphone. Highly recommended!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Positive review of the Samsung S24+ after one month of use',\n",
       " 'sentiment': 'Positive',\n",
       " 'pros': ['Excellent battery backup',\n",
       "  'Awesome camera performance in various conditions',\n",
       "  'Ultra-smooth and excellent display',\n",
       "  'Improved Exynos 2400 processor compared to Snapdragon 8 Gen 2',\n",
       "  'Dedicated telephoto lens for better zoom performance'],\n",
       " 'cons': [],\n",
       " 'score': 9.5,\n",
       " 'themes': ['Camera Performance',\n",
       "  'Battery Life',\n",
       "  'Display Quality',\n",
       "  'Processor Performance']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = structured_model.invoke(test_summary1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'An all-around powerhouse with top-tier performance, stunning display, and exceptional camera',\n",
       " 'sentiment': 'Positive',\n",
       " 'pros': ['Fast processor and ample RAM for seamless multitasking',\n",
       "  'Stunning display with vibrant colors and sharp detail',\n",
       "  '200MP main camera with exceptional detail and unreal zoom capabilities',\n",
       "  'S Pen for easier note-taking and editing photos',\n",
       "  'Solid battery life with fast charging'],\n",
       " 'cons': [],\n",
       " 'score': 9.5,\n",
       " 'themes': ['Performance',\n",
       "  'Display Quality',\n",
       "  'Camera',\n",
       "  'Battery Life',\n",
       "  'S Pen']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = structured_model.invoke(test_summary2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pydandic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Nikhil' age=20 email='asdf@asdf.com' cgpa=3.3\n",
      "name='Nikhil' age=20 email='asdf@asdf.com' cgpa=3.3\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Student\nemail\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(student4)\n\u001b[1;32m     18\u001b[0m student3 \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 19\u001b[0m student3 \u001b[38;5;241m=\u001b[39m Student(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstudent3)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(student3\u001b[38;5;241m.\u001b[39mname, student3\u001b[38;5;241m.\u001b[39mage)\n\u001b[1;32m     22\u001b[0m student2 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m23\u001b[39m}\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lang/lib/python3.12/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Student\nemail\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str = \"Nikhil Sharma\"\n",
    "    age: Optional[int] = None\n",
    "    email: EmailStr\n",
    "    cgpa: float = Field(gt=0, lt=10, default=0, description='CGPA b/w 0 to 10')\n",
    "\n",
    "student1 = {'name': 'Nikhil', 'age':20, 'email':'asdf@asdf.com', 'cgpa':3.3}\n",
    "student1 = Student(**student1)\n",
    "print(student1)\n",
    "\n",
    "student4 = {'name': 'Nikhil', 'age':'20', 'email':'asdf@asdf.com', 'cgpa':3.3}\n",
    "student4 = Student(**student4)\n",
    "print(student4)\n",
    "\n",
    "student3 = {}\n",
    "student3 = Student(**student3)\n",
    "print(student3.name, student3.age)\n",
    "\n",
    "student2 = {'name': 23}\n",
    "student2 = Student(**student2)\n",
    "print(student2)\n",
    "\n",
    "student1.model_dump()  # Dict object\n",
    "student1.model_dump_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, List, Optional, Annotated, Literal\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=None)#, model_kwargs={'temprature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Schema\n",
    "class ReviewExtractor(TypedDict):\n",
    "    themes: list[str] = Field(description=\"Write all the key themes mentioned in the review, in the list format\")\n",
    "    summary: str = Field(description=\"A brief summary of the review\")\n",
    "    sentiment: Literal[\"Positive\", \"Negative\"] = Field(description=\"Return setinment of the review, either Positive, Neutral or Negative\")\n",
    "    pros: Optional[list[str]] = Field(default=None, description=\"Write all the pros inside a list\")\n",
    "    cons: Optional[list[str]] = Field(default=None, description=\"Write all the cons inside a list\")\n",
    "    score: float = Field(default=5, gt=0, lt=10, description=\"Give rating based on the review. the rating must lie between 0 and 10\")\n",
    "    name: Optional[str] = Field(default=None, description='Write the name of the reviewer if found in the review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lang/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "structured_model = model.with_structured_output(ReviewExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary1 = \"\"\"After studying a lot, I finally purchased the S24+. Here is my review after using it for more than one month. I haven't played any games yet, but I have tested the camera in various conditions.\n",
    "The processor and battery were the main highlights.\n",
    "\n",
    "No heating issues even in 8K and 4K video recording. As a content creator I have to record long 4k video of more than 1 hour.\n",
    "\n",
    "Excellent battery backup. Only a 2% battery drain overnight.\n",
    "\n",
    "The camera is awesome. Photos, portrait video, and ultra-stable videos are next-level. Ultra-stable mode is basically unnecessary as normal video is also very stable.\n",
    "Primary 50 MP camera shoot slightly better details than iPhone 15. But in 3x and 10x it far better from iPhone 15 because of it's dedicated telephoto lens. Pathin video section iPhone is lightly capture better dynamic range. In in stability both are extraordinary.\n",
    "\n",
    "The display is ultra smooth and excellent.\n",
    "\n",
    "Processor:\n",
    "The Exynos 2400 is definitely better than the Snapdragon 8 Gen 2. Thanks to Samsung for finally providing this improvement in the processor. Maybe the AMD GPU also made a difference.\n",
    "\n",
    "Finally, I am very happy with this product. Reviewed by Nikhil Sharma\"\"\"\n",
    "\n",
    "test_summary2 = \"\"\"This is my review Niks, I recently upgraded to the Samsung Galaxy S23 Ultra, and it's been a fantastic experience. The phone is incredibly fast, thanks to its top-tier processor and ample RAM. Multitasking is seamless, and even the most demanding apps run without a hitch.\n",
    "\n",
    "The display is stunning, with vibrant colors and sharp detail, making it perfect for watching videos or playing games. The 120Hz refresh rate makes everything feel smooth and responsive.\n",
    "\n",
    "The camera is where this phone truly shines. The 200MP main camera captures exceptional detail, even in low light, and the zoom capabilities are unreal. I can zoom in from far distances and still get clear, usable shots. The S Pen is another standout feature, making it easier to take notes, navigate, and even edit photos.\n",
    "\n",
    "The battery life is solid, lasting a full day with moderate to heavy use, and fast charging is a lifesaver when I need a quick top-up.\n",
    "\n",
    "Overall, the S23 Ultra is an all-around powerhouse that excels in performance, display quality, and photography. It’s definitely worth the investment if you're looking for a premium smartphone. Highly recommended!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'themes': ['processor', 'battery', 'camera', 'display'],\n",
       " 'summary': 'The S24+ is a great purchase with highlights in the processor and battery performance. The camera excels in various conditions, offering excellent details in photos and videos. The ultra-stable mode is impressive, and the display is ultra smooth and excellent. Overall, a very happy user.',\n",
       " 'sentiment': 'Positive',\n",
       " 'pros': ['Processor performance',\n",
       "  'Battery backup',\n",
       "  'Camera quality',\n",
       "  'Display smoothness'],\n",
       " 'cons': [],\n",
       " 'score': 9.5,\n",
       " 'name': 'Nikhil Sharma'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = structured_model.invoke(test_summary1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result1\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "result1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'themes': ['performance',\n",
       "  'display quality',\n",
       "  'camera',\n",
       "  'battery life',\n",
       "  'value for money'],\n",
       " 'summary': 'The Samsung Galaxy S23 Ultra is a powerhouse smartphone with top-tier performance, stunning display quality, exceptional camera capabilities, solid battery life, and great value for money. Highly recommended!',\n",
       " 'sentiment': 'Positive',\n",
       " 'pros': ['Incredibly fast performance',\n",
       "  'Stunning display with vibrant colors and sharp detail',\n",
       "  'Exceptional 200MP main camera with unreal zoom capabilities',\n",
       "  'S Pen for easy note-taking and editing photos',\n",
       "  'Solid battery life lasting a full day with fast charging',\n",
       "  'Great value for a premium smartphone'],\n",
       " 'cons': [],\n",
       " 'score': 5,\n",
       " 'name': 'Niks'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = structured_model.invoke(test_summary2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"title\": \"student\",\n",
    "#     \"description\": \"Information about student\",\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"name\": \"string\",\n",
    "#         \"age\": \"integer\"\n",
    "#     },\n",
    "#     \"required\": [\"name\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=None)#, model_kwargs={'temprature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema\n",
    "json_schema = {\n",
    "  \"title\": \"Review\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"key_themes\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the key themes discussed in the review in a list\"\n",
    "    },\n",
    "    \"summary\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A brief summary of the review\"\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"pos\", \"neg\"],\n",
    "      \"description\": \"Return sentiment of the review either negative, positive or neutral\"\n",
    "    },\n",
    "    \"pros\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the pros inside a list\"\n",
    "    },\n",
    "    \"cons\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the cons inside a list\"\n",
    "    },\n",
    "    \"name\": {\n",
    "      \"type\": [\"string\", \"null\"],\n",
    "      \"description\": \"Write the name of the reviewer\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"key_themes\", \"summary\", \"sentiment\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lang/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "structured_model = model.with_structured_output(json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = structured_model.invoke(\"\"\"I recently upgraded to the Samsung Galaxy S24 Ultra, and I must say, it’s an absolute powerhouse! The Snapdragon 8 Gen 3 processor makes everything lightning fast—whether I’m gaming, multitasking, or editing photos. The 5000mAh battery easily lasts a full day even with heavy use, and the 45W fast charging is a lifesaver.\n",
    "\n",
    "The S-Pen integration is a great touch for note-taking and quick sketches, though I don't use it often. What really blew me away is the 200MP camera—the night mode is stunning, capturing crisp, vibrant images even in low light. Zooming up to 100x actually works well for distant objects, but anything beyond 30x loses quality.\n",
    "\n",
    "However, the weight and size make it a bit uncomfortable for one-handed use. Also, Samsung’s One UI still comes with bloatware—why do I need five different Samsung apps for things Google already provides? The $1,300 price tag is also a hard pill to swallow.\n",
    "\n",
    "Pros:\n",
    "Insanely powerful processor (great for gaming and productivity)\n",
    "Stunning 200MP camera with incredible zoom capabilities\n",
    "Long battery life with fast charging\n",
    "S-Pen support is unique and useful\n",
    "                                 \n",
    "Review by Niks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key_themes': ['Snapdragon 8 Gen 3 processor',\n",
       "  'battery life',\n",
       "  'S-Pen integration',\n",
       "  '200MP camera',\n",
       "  'size and weight',\n",
       "  'Samsung One UI',\n",
       "  'price'],\n",
       " 'summary': 'Niks is impressed with the Samsung Galaxy S24 Ultra, highlighting the powerful processor, long battery life, stunning 200MP camera, and useful S-Pen support. However, they find the size and weight uncomfortable for one-handed use, criticize the bloatware in Samsung One UI, and mention the high price tag as a drawback.',\n",
       " 'sentiment': 'pos',\n",
       " 'pros': ['Insanely powerful processor (great for gaming and productivity)',\n",
       "  'Stunning 200MP camera with incredible zoom capabilities',\n",
       "  'Long battery life with fast charging',\n",
       "  'S-Pen support is unique and useful'],\n",
       " 'cons': ['Size and weight make it uncomfortable for one-handed use',\n",
       "  'Bloatware in Samsung One UI',\n",
       "  'High price tag'],\n",
       " 'name': 'Niks'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages Place holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'query'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11a8d07c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are a customer support agent.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'Your are a customer support agent.'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{query}')\n",
    "])\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SystemMessage(content='You are a helpful assistant who gives answers in as less words as possible.', additional_kwargs={}, response_metadata={})\",\n",
       " \"HumanMessage(content='Hi', additional_kwargs={}, response_metadata={})\",\n",
       " \"AIMessage(content='Hello! How can I help you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 27, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-4cf6376e-1d7c-4cca-a807-df6f54b3ae46-0', usage_metadata={'input_tokens': 27, 'output_tokens': 9, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\",\n",
       " \"HumanMessage(content='What is the date today?', additional_kwargs={}, response_metadata={})\",\n",
       " \"AIMessage(content='October 4, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 49, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-795b906e-e244-44fb-a3c3-093f50fe588a-0', usage_metadata={'input_tokens': 49, 'output_tokens': 9, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\",\n",
       " \"HumanMessage(content='day?', additional_kwargs={}, response_metadata={})\",\n",
       " \"AIMessage(content='Wednesday.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 67, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4bac1b3-24b7-4f7c-a16a-f4270d78c372-0', usage_metadata={'input_tokens': 67, 'output_tokens': 3, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\",\n",
       " \"HumanMessage(content='name?', additional_kwargs={}, response_metadata={})\",\n",
       " \"AIMessage(content='October.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 79, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-50a360c4-767f-46ce-8a3c-378ab237867a-0', usage_metadata={'input_tokens': 79, 'output_tokens': 3, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\",\n",
       " \"HumanMessage(content='your name?', additional_kwargs={}, response_metadata={})\",\n",
       " \"AIMessage(content='I’m called Assistant.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 92, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-a91a18a6-7e28-42b5-bee0-fc5e22ac962c-0', usage_metadata={'input_tokens': 92, 'output_tokens': 6, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\",\n",
       " \"HumanMessage(content='age?', additional_kwargs={}, response_metadata={})\",\n",
       " 'AIMessage(content=\"I don\\'t have an age.\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 7, \\'prompt_tokens\\': 107, \\'total_tokens\\': 114, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_06737a9306\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-cb5a5907-b0b9-419f-88dd-50a65bf19538-0\\', usage_metadata={\\'input_tokens\\': 107, \\'output_tokens\\': 7, \\'total_tokens\\': 114, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})',\n",
       " \"HumanMessage(content='model name?', additional_kwargs={}, response_metadata={})\",\n",
       " 'AIMessage(content=\"I\\'m based on OpenAI\\'s GPT-3.\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 11, \\'prompt_tokens\\': 124, \\'total_tokens\\': 135, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_06737a9306\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-e10532e1-ff04-4c54-b311-fe674de71cec-0\\', usage_metadata={\\'input_tokens\\': 124, \\'output_tokens\\': 11, \\'total_tokens\\': 135, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Chat History\n",
    "chat_history = []\n",
    "\n",
    "with open(file='chat_history.txt', mode='r') as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "chat_history.extend(content)\n",
    "chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Your are a customer support agent.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"SystemMessage(content='You are a helpful assistant who gives answers in as less words as possible.', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='Hi', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"AIMessage(content='Hello! How can I help you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 27, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-4cf6376e-1d7c-4cca-a807-df6f54b3ae46-0', usage_metadata={'input_tokens': 27, 'output_tokens': 9, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='What is the date today?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"AIMessage(content='October 4, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 49, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-795b906e-e244-44fb-a3c3-093f50fe588a-0', usage_metadata={'input_tokens': 49, 'output_tokens': 9, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='day?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"AIMessage(content='Wednesday.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 67, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4bac1b3-24b7-4f7c-a16a-f4270d78c372-0', usage_metadata={'input_tokens': 67, 'output_tokens': 3, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='name?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"AIMessage(content='October.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 79, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-50a360c4-767f-46ce-8a3c-378ab237867a-0', usage_metadata={'input_tokens': 79, 'output_tokens': 3, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='your name?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"AIMessage(content='I’m called Assistant.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 92, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-a91a18a6-7e28-42b5-bee0-fc5e22ac962c-0', usage_metadata={'input_tokens': 92, 'output_tokens': 6, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='age?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content='AIMessage(content=\"I don\\'t have an age.\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 7, \\'prompt_tokens\\': 107, \\'total_tokens\\': 114, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_06737a9306\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-cb5a5907-b0b9-419f-88dd-50a65bf19538-0\\', usage_metadata={\\'input_tokens\\': 107, \\'output_tokens\\': 7, \\'total_tokens\\': 114, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"HumanMessage(content='model name?', additional_kwargs={}, response_metadata={})\", additional_kwargs={}, response_metadata={}), HumanMessage(content='AIMessage(content=\"I\\'m based on OpenAI\\'s GPT-3.\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 11, \\'prompt_tokens\\': 124, \\'total_tokens\\': 135, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_06737a9306\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-e10532e1-ff04-4c54-b311-fe674de71cec-0\\', usage_metadata={\\'input_tokens\\': 124, \\'output_tokens\\': 11, \\'total_tokens\\': 135, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})', additional_kwargs={}, response_metadata={}), HumanMessage(content='What were we discussing?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Prompt\n",
    "prompt = chat_template.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'query': 'What were we discussing?'\n",
    "})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Messages - List of messages - Dynamic messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful chatbot with years of experience in {domain} domain.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain me in simple terms how {topic} works, as a short poem.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DOES NOT WORK AS INTENDED\n",
    "chat_template = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"You are a helpful chatbot with years of experience in {domain} domain.\"),\n",
    "    HumanMessage(content=\"Explain me in simple terms how {topic} works, as a short poem.\")\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\n",
    "    'domain': 'Computer Science',\n",
    "    'topic': 'Transformers'\n",
    "})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful chatbot with years of experience in Computer Science domain.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain me in simple terms how Transformers works, as a short poem.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECOMMENDED\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful chatbot with years of experience in {domain} domain.'),\n",
    "    ('human', 'Explain me in simple terms how {topic} works, as a short poem.')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\n",
    "    'domain': 'Computer Science',\n",
    "    'topic': 'Transformers'\n",
    "})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful chatbot with years of experience in Computer Science domain.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain me in simple terms how Transformers works, as a short poem.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful chatbot with years of experience in {domain} domain.'),\n",
    "    ('human', 'Explain me in simple terms how {topic} works, as a short poem.')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\n",
    "    'domain': 'Computer Science',\n",
    "    'topic': 'Transformers'\n",
    "})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=None)  #, model_kwargs={'temprature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(\"You are a helpful assistant who gives answers in as less words as possible.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! How can I help you?\n",
      "Bot: October 4, 2023.\n",
      "Bot: Wednesday.\n",
      "Bot: October.\n",
      "Bot: I’m called Assistant.\n",
      "Bot: I don't have an age.\n",
      "Bot: I'm based on OpenAI's GPT-3.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input=='exit':\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    result = model.invoke(chat_history)\n",
    "    print(\"Bot:\", result.content)\n",
    "    chat_history.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant who gives answers in as less words as possible.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello! How can I help you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 27, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-4cf6376e-1d7c-4cca-a807-df6f54b3ae46-0', usage_metadata={'input_tokens': 27, 'output_tokens': 9, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='What is the date today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='October 4, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 49, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-795b906e-e244-44fb-a3c3-093f50fe588a-0', usage_metadata={'input_tokens': 49, 'output_tokens': 9, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='day?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Wednesday.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 67, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4bac1b3-24b7-4f7c-a16a-f4270d78c372-0', usage_metadata={'input_tokens': 67, 'output_tokens': 3, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='October.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 79, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-50a360c4-767f-46ce-8a3c-378ab237867a-0', usage_metadata={'input_tokens': 79, 'output_tokens': 3, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I’m called Assistant.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 92, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-a91a18a6-7e28-42b5-bee0-fc5e22ac962c-0', usage_metadata={'input_tokens': 92, 'output_tokens': 6, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='age?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't have an age.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 107, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb5a5907-b0b9-419f-88dd-50a65bf19538-0', usage_metadata={'input_tokens': 107, 'output_tokens': 7, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='model name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm based on OpenAI's GPT-3.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 124, 'total_tokens': 135, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-e10532e1-ff04-4c54-b311-fe674de71cec-0', usage_metadata={'input_tokens': 124, 'output_tokens': 11, 'total_tokens': 135, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_input = [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"]\n",
    "style_input = [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    "length_input = [\"Too Short (2-3 lines)\", \"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_input = random.choice(paper_input)\n",
    "style_input = random.choice(style_input)\n",
    "length_input = random.choice(length_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GPT-3: Language Models are Few-Shot Learners',\n",
       " 'Beginner-Friendly',\n",
       " 'Too Short (2-3 lines)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_input, style_input, length_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper discusses GPT-3, a language model that can learn new tasks quickly with limited examples. It uses few-shot learning, allowing it to generalize from a small amount of training data. This is similar to how a student can quickly grasp a new concept with just a few examples.\n"
     ]
    }
   ],
   "source": [
    "## METHOD 1\n",
    "\n",
    "# Design the template\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "        Explanation Style: {style_input}\n",
    "        Explanation Length: {length_input}\n",
    "        1. Mathematical Details:  \n",
    "            - Include relevant mathematical equations if present in the paper.\n",
    "            - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
    "        2. Analogies:\n",
    "            - Use relatable analogies to simplify complex ideas.\n",
    "        If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
    "        Ensure the summary is clear, accurate, and aligned with the provided style and length.\",\n",
    "        \"\"\", \n",
    "    input_variables=[\n",
    "        'paper_input', \n",
    "        'style_input', \n",
    "        'length_input'\n",
    "    ],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "# Render the template\n",
    "prompt = template.invoke({\n",
    "    'paper_input': paper_input,\n",
    "    'style_input': style_input,\n",
    "    'length_input': length_input\n",
    "})\n",
    "\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper \"GPT-3: Language Models are Few-Shot Learners\" discusses how GPT-3, a large language model, can learn from just a few examples to perform various tasks effectively. It showcases the impressive few-shot learning capabilities of GPT-3, highlighting its potential for real-world applications in natural language processing. An analogy to understand this would be like a student who can learn and excel in different subjects with minimal study material.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## METHOD 2\n",
    "\n",
    "template = load_prompt(\"prompt_template.json\")\n",
    "chain = template | model\n",
    "result = chain.invoke({\n",
    "    'paper_input': paper_input,\n",
    "    'style_input': style_input,\n",
    "    'length_input': length_input\n",
    "})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Design the template\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "        Explanation Style: {style_input}\n",
    "        Explanation Length: {length_input}\n",
    "        1. Mathematical Details:  \n",
    "            - Include relevant mathematical equations if present in the paper.\n",
    "            - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
    "        2. Analogies:\n",
    "            - Use relatable analogies to simplify complex ideas.\n",
    "        If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
    "        Ensure the summary is clear, accurate, and aligned with the provided style and length.\",\n",
    "        \"\"\", \n",
    "    input_variables=[\n",
    "        'paper_input', \n",
    "        'style_input', \n",
    "        'length_input'\n",
    "    ],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "template.save(\"prompt_template.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - Text Embeddings - Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488cdd8ac353492c8724bf05b92fe564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac36ce67b241417d9ac1f413d6a9a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f631c8b6be407c9454a6a54fdfaa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d8266d18ea4b88a5d0f30522dfd225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3c83b168f04923a57847f4ae46f5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9bfcf9f5d44e289cd6ac64df1f62cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4b938470484af5a68ee0f678673f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a56ecd4137463a92ffb8aa7f211afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504567fa277b497093e6b3921de4ada4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505cea6c3d8a43ee85d4b8336868eb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f397b1fd0d447faa04aebbd0965bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Space Exploration: The shimmering expanse of the cosmos beckons, urging humanity to push beyond Earth's bounds, seeking new worlds and unraveling the universe's ancient mysteries.\",\n",
    "    \"Artificial Intelligence: As algorithms evolve, the ethical implications of AI's growing autonomy demand careful consideration, ensuring these powerful tools serve humanity's best interests.\",\n",
    "    \"Climate Change: Rising sea levels and extreme weather patterns serve as stark reminders that immediate, global action is essential to mitigate the devastating effects of climate change.\",\n",
    "    \"Renewable Energy: Harnessing the sun, wind, and geothermal forces offers a sustainable path toward a cleaner future, reducing our reliance on fossil fuels and environmental damage.\",\n",
    "    \"Education Reform: Modernizing educational systems to foster critical thinking and creativity empowers future generations to navigate complex challenges in an ever-changing world.\",\n",
    "    \"Mental Health Awareness: Breaking down societal stigmas surrounding mental health and providing accessible support systems are crucial for fostering well-being within communities.\",\n",
    "    \"Global Connectivity: The internet's vast network connects individuals across continents, enabling instantaneous communication and the rapid exchange of knowledge and cultural ideas.\",\n",
    "    \"Urban Development: Sustainable urban planning, incorporating green spaces and efficient infrastructure, is vital for creating livable cities that prioritize residents' quality of life.\",\n",
    "    \"Food Security: Addressing global food security requires innovative agricultural practices and equitable distribution systems to ensure everyone has access to nutritious meals.\",\n",
    "    \"Artistic Expression: Through diverse mediums, artists challenge perspectives, evoke emotions, and provide unique insights into the human condition, enriching our cultural landscape.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Delhi is the capital of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_embeddings = embedding_model.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 384)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_embeddings), len(documents_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.embed_query(text=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,\n",
       " [0.04354957118630409,\n",
       "  0.02387721836566925,\n",
       "  -0.045241307467222214,\n",
       "  0.035404983907938004,\n",
       "  -0.01665104180574417])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_embedding), query_embedding[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cosine_similarity([query_embedding], documents_embeddings)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, score = sorted(list(enumerate(scores)), key=lambda x: x[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQUERY: Delhi is the capital of India\\nANS | 0.1: Urban Development: Sustainable urban planning, incorporating green spaces and efficient infrastructure, is vital for creating livable cities that prioritize residents' quality of life.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"\"\"\n",
    "QUERY: {query}\n",
    "ANS | {round(score, 2)}: {documents[index]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Space Exploration: The shimmering expanse of the cosmos beckons, urging humanity to push beyond Earth's bounds, seeking new worlds and unraveling the universe's ancient mysteries.\",\n",
    "    \"Artificial Intelligence: As algorithms evolve, the ethical implications of AI's growing autonomy demand careful consideration, ensuring these powerful tools serve humanity's best interests.\",\n",
    "    \"Climate Change: Rising sea levels and extreme weather patterns serve as stark reminders that immediate, global action is essential to mitigate the devastating effects of climate change.\",\n",
    "    \"Renewable Energy: Harnessing the sun, wind, and geothermal forces offers a sustainable path toward a cleaner future, reducing our reliance on fossil fuels and environmental damage.\",\n",
    "    \"Education Reform: Modernizing educational systems to foster critical thinking and creativity empowers future generations to navigate complex challenges in an ever-changing world.\",\n",
    "    \"Mental Health Awareness: Breaking down societal stigmas surrounding mental health and providing accessible support systems are crucial for fostering well-being within communities.\",\n",
    "    \"Global Connectivity: The internet's vast network connects individuals across continents, enabling instantaneous communication and the rapid exchange of knowledge and cultural ideas.\",\n",
    "    \"Urban Development: Sustainable urban planning, incorporating green spaces and efficient infrastructure, is vital for creating livable cities that prioritize residents' quality of life.\",\n",
    "    \"Food Security: Addressing global food security requires innovative agricultural practices and equitable distribution systems to ensure everyone has access to nutritious meals.\",\n",
    "    \"Artistic Expression: Through diverse mediums, artists challenge perspectives, evoke emotions, and provide unique insights into the human condition, enriching our cultural landscape.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documnet_embeddings = embedding_model.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documnet_embeddings), len(documnet_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Delhi is the capital of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, score = sorted(list(enumerate(scores)), key=lambda x: x[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQUERY: Delhi is the capital of India\\nANS | 0.1: Urban Development: Sustainable urban planning, incorporating green spaces and efficient infrastructure, is vital for creating livable cities that prioritize residents' quality of life.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"\"\"\n",
    "QUERY: {query}\n",
    "ANS | {round(score, 2)}: {documents[index]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2714dc2bff49349682f29d854f0701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e2c0b7d26f44aeb3f3aa09fc76e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/64/2b642798915fc368e7b638986f68446b121c1d59b30075e146bd6312ee664ac2/6e6001da2106d4757498752a021df6c2bdc332c650aae4bae6b0c004dcf14933?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1741084954&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTA4NDk1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiLzY0LzJiNjQyNzk4OTE1ZmMzNjhlN2I2Mzg5ODZmNjg0NDZiMTIxYzFkNTliMzAwNzVlMTQ2YmQ2MzEyZWU2NjRhYzIvNmU2MDAxZGEyMTA2ZDQ3NTc0OTg3NTJhMDIxZGY2YzJiZGMzMzJjNjUwYWFlNGJhZTZiMGMwMDRkY2YxNDkzMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=me7RotsnBwvMQMaydsYL7zwfPg7v85DcMpkLQt8z41b%7EPc7O-6Jg7XoKI4qE4y-8fu1hGm7fHPIqdQqsaZG51b%7EP6sXRmzHSYPq0CR%7ESENqQ8jc5mIaSOLa0SXessY711IWKkAGzh0kkznzBv4Jj5Fcdf%7EdtG2BFFW%7E8NGO%7ET1lLKyQeIr95VfS%7Eszr4oibJszC6R5OQuOsUepQhKyRbEMhqF-KrkXq1WYDkjifUaPT9nfRoo8SxfZTZ0qbNzH5ehIOPRA-tX%7EmwPQAb3PM7baH6Trfhuznre30T%7EBV25Ui%7EKqXPEBf8CQAzocj9jMl3TYGkTCDHM6Fz%7E%7EuunzZSsg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d60803f80a4ec09018567a898d4a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  58%|#####7    | 1.27G/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03cadcc5c1746b8b04fc019e39452bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.9,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lang/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/lang/lib/python3.12/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(\"Give me a poem on sun shining in summer, india context. 6 lines only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|user|>\\nGive me a poem on sun shining in summer, india context. 6 lines only.</s>\\n<|assistant|>\\nIn the summer's warmth,\\nThe sun beams down upon us,\\nA ray of light that shines bright,\\nA symbol of hope and joy.\\n\\nThe sun's rays dance across the sky,\\nA symphony of colors,\\nA rainbow of hues,\\nA sight to behold.\\n\\nThe warmth of the sun's rays,\\nA balm for our weary soul,\\nA reminder of the beauty\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace - Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40731189b7564a1a90ea046892889686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    # repo_id='deepseek-ai/DeepSeek-R1',\n",
    "    repo_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(\"Give me a poem on sun shining in summer, india context. 6 lines only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-AI Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=None)#, model_kwargs={'temprature':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(input=\"Give me a poem on sun shining in summer, india context. 6 lines only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden rays dance on fields so wide,  \n",
      "Mango trees sway with the warm, soft tide.  \n",
      "Children laugh, their kites soar high,  \n",
      "Beneath the vast, azure Indian sky.  \n",
      "Spices mingle in the market's hum,  \n",
      "In summer's embrace, life's vibrant drum.  \n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-AI LLM - Not Used Anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=[\"babbage-002\", \"gpt-3.5-turbo-instruct\"][1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the Capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from openai import OpenAI\n",
    "# openai.api_key = os.environ[\"PERSONAL_OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
