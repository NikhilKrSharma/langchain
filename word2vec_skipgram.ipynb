
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-Gram from Scratch Using PyTorch\n",
    "This notebook builds a Word2Vec Skip-Gram model using negative sampling on a custom text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use the provided text\n",
    "text = \"\"\"Getting numbers is easy; getting numbers you can trust is hard. This practical guide by experimentation leaders at Google, LinkedIn, and Microsoft will teach you how to accelerate innovation using trustworthy online controlled experiments, or A/B tests. Based on practical experiences at companies that each runs more than 20,000 controlled experiments a year, the authors share examples, pitfalls, and advice for students and industry professionals getting started with experiments, plus deeper dives into advanced topics for experienced practitioners who want to improve the way they and their organizations make data-driven decisions.\n",
    "Learn how to:\n",
    "\u25cf Use the scientific method to evaluate hypotheses using controlled experiments\n",
    "\u25cf Define key metrics and ideally an Overall Evaluation Criterion\n",
    "\u25cf Test for trustworthiness of the results and alert experimenters to violated assumptions\n",
    "\u25cf Interpret and iterate quickly based on the results\n",
    "\u25cf Implement guardrails to protect key business goals\n",
    "\u25cf Build a scalable platform that lowers the marginal cost of experiments close to zero \u25cf Avoid pitfalls such as carryover effects, Twyman’s law, Simpson’s paradox, and network interactions\n",
    "\u25cf Understand how statistical issues play out in practice, including common violations of assumptions\"\"\"\n",
    "\n",
    "# Preprocessing\n",
    "tokens = text.lower().replace(';', '').replace('.', '').replace(',', '').split()\n",
    "vocab = list(set(tokens))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Generate skip-gram pairs\n",
    "def generate_pairs(words, window_size=2):\n",
    "    pairs = []\n",
    "    for i, center in enumerate(words):\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0 and 0 <= i + j < len(words):\n",
    "                pairs.append((center, words[i + j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_pairs(tokens)\n",
    "training_data = [(word2idx[w1], word2idx[w2]) for w1, w2 in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Definition\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_word, context_word):\n",
    "        center_embed = self.in_embed(center_word)\n",
    "        context_embed = self.out_embed(context_word)\n",
    "        score = torch.sum(center_embed * context_embed, dim=1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Training\n",
    "embedding_dim = 50\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def get_negative_samples(pos_word_idx, num_neg=5):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_neg:\n",
    "        neg = random.randint(0, vocab_size - 1)\n",
    "        if neg != pos_word_idx:\n",
    "            neg_samples.append(neg)\n",
    "    return neg_samples\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    random.shuffle(training_data)\n",
    "    for center, context in training_data:\n",
    "        center_tensor = torch.tensor([center], dtype=torch.long)\n",
    "        context_tensor = torch.tensor([context], dtype=torch.long)\n",
    "        pos_label = torch.tensor([1.0])\n",
    "\n",
    "        neg_samples = get_negative_samples(context)\n",
    "        neg_tensor = torch.tensor(neg_samples, dtype=torch.long)\n",
    "        neg_labels = torch.zeros(len(neg_samples))\n",
    "\n",
    "        pos_score = model(center_tensor, context_tensor)\n",
    "        pos_loss = loss_fn(pos_score, pos_label)\n",
    "        neg_score = model(center_tensor.repeat(len(neg_samples)), neg_tensor)\n",
    "        neg_loss = loss_fn(neg_score, neg_labels)\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Similarity Function\n",
    "def most_similar(word, topn=5):\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    vec = model.in_embed(torch.tensor(word2idx[word])).detach().numpy().reshape(1, -1)\n",
    "    all_vecs = model.in_embed.weight.detach().numpy()\n",
    "    sims = cosine_similarity(vec, all_vecs)[0]\n",
    "    top_idxs = np.argsort(-sims)[1:topn+1]\n",
    "    return [(idx2word[i], sims[i]) for i in top_idxs]\n",
    "\n",
    "# Example\n",
    "print(most_similar('experiments'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
