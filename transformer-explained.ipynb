{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Show up to 10,000 elements, and widen the output lines\n",
    "# torch.set_printoptions(threshold=10000, linewidth=200)\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Module -------------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-np.log(10000.0) / d_model\n",
    "        ))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"\\nPositional Encoding Input: {x.shape}\")\n",
    "        print(x.detach().numpy())\n",
    "        output = x + self.pe[:, :x.size(1)]\n",
    "        print(\"\\nPositional Encoding Output:\")\n",
    "        print(output.detach().numpy())\n",
    "        return output\n",
    "\n",
    "# Embedding Layer ------------------------------------------------------------------\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"\\nEmbedding Input (token IDs): {x.cpu().numpy()}\")\n",
    "        output = self.embed(x) * np.sqrt(self.d_model)\n",
    "        print(f\"\\nEmbedding Output: {output.shape}\")\n",
    "        print(output.detach().numpy())\n",
    "        return output\n",
    "\n",
    "# Multi-Head Attention Module ------------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.is_decoder = is_decoder\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.WO = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        print(f\"\\nMultiHeadAttention (Decoder={self.is_decoder})\")\n",
    "        print(f\"Input Q: {Q.shape}, K: {K.shape}, V: {V.shape}\")\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.WQ(Q).view(batch_size, -1, num_heads, self.d_k).transpose(1,2)\n",
    "        K = self.WK(K).view(batch_size, -1, num_heads, self.d_k).transpose(1,2)\n",
    "        V = self.WV(V).view(batch_size, -1, num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # Attention calculation\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # Fix: Create proper 2D mask for decoder\n",
    "        if self.is_decoder:\n",
    "            seq_len = scores.size(-1)\n",
    "            # Create square mask of size (seq_len, seq_len)\n",
    "            mask = torch.triu(\n",
    "                torch.ones(seq_len, seq_len, device=scores.device),\n",
    "                diagonal=1\n",
    "            ).bool()\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, d_model)\n",
    "        output = self.WO(context)\n",
    "        \n",
    "        print(\"\\nAttention Weights Sample:\")\n",
    "        print(attn_weights[0,0,0].detach().numpy())\n",
    "        print(\"\\nAttention Output:\")\n",
    "        print(output.detach().numpy())\n",
    "        return output\n",
    "\n",
    "# Feed-Forward Network -------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"\\nFeedForward Input: {x.shape}\")\n",
    "        print(x.detach().numpy())\n",
    "        output = self.ffn(x)\n",
    "        print(\"\\nFeedForward Output:\")\n",
    "        print(output.detach().numpy())\n",
    "        return output\n",
    "\n",
    "# Encoder Layer --------------------------------------------------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForward()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\n\" + \"=\"*30 + \" Encoder Layer \" + \"=\"*30)\n",
    "        # Self-attention\n",
    "        print(\"\\nEncoder Self-Attention:\")\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        print(\"\\nPost Attention (Norm):\")\n",
    "        print(x.detach().numpy())\n",
    "        \n",
    "        # FFN\n",
    "        print(\"\\nEncoder Feed-Forward:\")\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        print(\"\\nPost FFN (Norm):\")\n",
    "        print(x.detach().numpy())\n",
    "        return x\n",
    "\n",
    "# Decoder Layer --------------------------------------------------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(is_decoder=True)\n",
    "        self.cross_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForward()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        print(\"\\n\" + \"=\"*30 + \" Decoder Layer \" + \"=\"*30)\n",
    "        # Masked self-attention\n",
    "        print(\"\\nDecoder Self-Attention:\")\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        print(\"\\nPost Self-Attention (Norm):\")\n",
    "        print(x.detach().numpy())\n",
    "        \n",
    "        # Cross-attention\n",
    "        print(\"\\nDecoder Cross-Attention:\")\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output)\n",
    "        x = self.norm2(x + attn_output)\n",
    "        print(\"\\nPost Cross-Attention (Norm):\")\n",
    "        print(x.detach().numpy())\n",
    "        \n",
    "        # FFN\n",
    "        print(\"\\nDecoder Feed-Forward:\")\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_output)\n",
    "        print(\"\\nPost FFN (Norm):\")\n",
    "        print(x.detach().numpy())\n",
    "        return x\n",
    "\n",
    "# Encoder Stack --------------------------------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = Embeddings(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\n\" + \"=\"*40 + \" Encoder Start \" + \"=\"*40)\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Decoder Stack --------------------------------------------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = Embeddings(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        print(\"\\n\" + \"=\"*40 + \" Decoder Start \" + \"=\"*40)\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output)\n",
    "        return x\n",
    "\n",
    "# Complete Transformer -------------------------------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        print(\"\\n\" + \"=\"*40 + \" Forward Pass Start \" + \"=\"*40)\n",
    "        enc_output = self.encoder(src)\n",
    "        dec_output = self.decoder(tgt[:, :-1], enc_output)\n",
    "        return self.final_layer(dec_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "d_model = 3        # Embedding dimension (divisible by num_heads)\n",
    "num_heads = 3      # Number of attention heads\n",
    "d_ff = 6           # Feed-forward hidden dimension\n",
    "num_layers = 2     # Number of encoder/decoder layers\n",
    "max_seq_len = 5    # Maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "d_model = 6        # Embedding dimension (divisible by num_heads)\n",
    "num_heads = 3      # Number of attention heads\n",
    "d_ff = 12          # Feed-forward hidden dimension\n",
    "num_layers = 2     # Number of encoder/decoder layers\n",
    "max_seq_len = 5    # Maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "src_sentence = [\"Nikhil\", \"likes\", \"to\", \"play\", \"football\"]\n",
    "tgt_sentence = [\"<start>\", \"Nikhil\", \"likes\", \"to\", \"play\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building vocabulary...\n",
      "Vocabulary (size 7): {'<pad>': 0, '<start>': 1, 'football': 2, 'Nikhil': 3, 'play': 4, 'likes': 5, 'to': 6}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary -----------------------------------------------------------------\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "special_tokens = [\"<pad>\", \"<start>\"]\n",
    "all_words = list(set(src_sentence + tgt_sentence))\n",
    "\n",
    "vocab = {}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<start>\"] = 1\n",
    "current_idx = 2\n",
    "for word in all_words:\n",
    "    if word not in special_tokens and word not in vocab:\n",
    "        vocab[word] = current_idx\n",
    "        current_idx += 1\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary (size {vocab_size}): {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert sentences to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source tokens: ['Nikhil', 'likes', 'to', 'play', 'football'] => IDs: [3, 5, 6, 4, 2]\n",
      "Target tokens: ['<start>', 'Nikhil', 'likes', 'to', 'play'] => IDs: [1, 3, 5, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert sentences to token IDs ---------------------------------------------------\n",
    "src_ids = [vocab[w] for w in src_sentence]\n",
    "tgt_ids = [vocab[w] for w in tgt_sentence]\n",
    "print(f\"\\nSource tokens: {src_sentence} => IDs: {src_ids}\")\n",
    "print(f\"Target tokens: {tgt_sentence} => IDs: {tgt_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "src = torch.LongTensor(src_ids).unsqueeze(0)  # (1,5)\n",
    "tgt = torch.LongTensor(tgt_ids).unsqueeze(0)  # (1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Training Components -----------------------------------------\n",
    "model = Transformer()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Starting Forward Pass ========================================\n",
      "\n",
      "======================================== Forward Pass Start ========================================\n",
      "\n",
      "======================================== Encoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[3 5 6 4 2]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 5, 6])\n",
      "[[[-3.5316937e+00  1.6512412e+00  2.7454066e+00 -2.3487000e+00 -8.1406766e-01  3.5352362e-03]\n",
      "  [ 2.9384294e+00  5.3358903e+00 -1.2465447e+00  2.3554676e+00 -2.9935477e+00 -1.7453690e+00]\n",
      "  [-1.4768381e+00  2.8245058e+00 -1.8362260e+00  3.9558768e+00  4.9465564e-01 -1.7560655e+00]\n",
      "  [-4.9802560e-01 -1.5842095e-01  1.4116590e+00 -2.3076582e+00 -2.0125613e+00 -5.9117329e-01]\n",
      "  [-1.2358226e+00 -8.2297730e-01 -4.2127466e-01 -7.5625145e-01 -1.6337032e+00  2.1894784e+00]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 5, 6])\n",
      "[[[-3.5316937e+00  1.6512412e+00  2.7454066e+00 -2.3487000e+00 -8.1406766e-01  3.5352362e-03]\n",
      "  [ 2.9384294e+00  5.3358903e+00 -1.2465447e+00  2.3554676e+00 -2.9935477e+00 -1.7453690e+00]\n",
      "  [-1.4768381e+00  2.8245058e+00 -1.8362260e+00  3.9558768e+00  4.9465564e-01 -1.7560655e+00]\n",
      "  [-4.9802560e-01 -1.5842095e-01  1.4116590e+00 -2.3076582e+00 -2.0125613e+00 -5.9117329e-01]\n",
      "  [-1.2358226e+00 -8.2297730e-01 -4.2127466e-01 -7.5625145e-01 -1.6337032e+00  2.1894784e+00]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[-3.5316937   2.6512413   2.7454066  -1.3487     -0.81406766  1.0035353 ]\n",
      "  [ 3.7799003   5.8761926  -1.2001455   3.3543906  -2.9913933  -0.7453712 ]\n",
      "  [-0.5675407   2.408359   -1.7435275   4.951571    0.4989645  -0.7560747 ]\n",
      "  [-0.35690558 -1.1484134   1.5504571  -1.3173375  -2.006098    0.4088058 ]\n",
      "  [-1.992625   -1.4766209  -0.23667593  0.2265625  -1.6250856   3.1894412 ]]]\n",
      "\n",
      "============================== Encoder Layer ==============================\n",
      "\n",
      "Encoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 5, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.1577217  0.1535524  0.3144288  0.15010248 0.2241946 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.05036052  0.41356683  0.14896062 -1.3904159   0.5025073  -0.48000228]\n",
      "  [-0.21323597  0.03128868  0.52338433 -1.546832    0.6308199  -0.7806337 ]\n",
      "  [-0.02009055  0.08141217  0.3470712  -1.2901261   0.5748379  -0.5601667 ]\n",
      "  [ 0.05929413  0.16330142  0.20698962 -1.3483413   0.58822    -0.459157  ]\n",
      "  [-0.09057808  0.34014374  0.31300536 -1.2460146   0.4770139  -0.42704076]]]\n",
      "\n",
      "Post Attention (Norm):\n",
      "[[[-1.3843883   1.2249113   1.1569734  -1.0885396  -0.12091307  0.21195611]\n",
      "  [ 0.83340555  1.6306677  -0.6118686   0.23426934 -1.1853606  -0.9011137 ]\n",
      "  [-0.6452778   0.9539053  -1.0655868   1.5627697   0.21809211 -1.0239028 ]\n",
      "  [ 0.23024943 -0.27670097  1.745612   -1.5159204  -0.59581476  0.41257465]\n",
      "  [-1.065255   -0.45715714  0.32184896 -0.38198993 -0.4646045   2.0471575 ]]]\n",
      "\n",
      "Encoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 5, 6])\n",
      "[[[-1.3843883   1.2249113   1.1569734  -1.0885396  -0.12091307  0.21195611]\n",
      "  [ 0.83340555  1.6306677  -0.6118686   0.23426934 -1.1853606  -0.9011137 ]\n",
      "  [-0.6452778   0.9539053  -1.0655868   1.5627697   0.21809211 -1.0239028 ]\n",
      "  [ 0.23024943 -0.27670097  1.745612   -1.5159204  -0.59581476  0.41257465]\n",
      "  [-1.065255   -0.45715714  0.32184896 -0.38198993 -0.4646045   2.0471575 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.21953236  0.05105793  0.4565798  -0.312665    0.48060864 -0.6216748 ]\n",
      "  [-0.08811759  0.09154925  0.66449654 -0.22153443  0.37664342 -0.30278203]\n",
      "  [ 0.00874774  0.06151623  0.70962846  0.09326078  0.1976096  -0.31924537]\n",
      "  [-0.24199294  0.249995    0.32791835 -0.23159163  0.5316242  -0.27279723]\n",
      "  [-0.08086288  0.58470494  0.34721735  0.03565602  0.2713505  -0.31516367]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-1.282765    1.0608141   1.3355311  -1.1178      0.31517452 -0.31095466]\n",
      "  [ 0.682746    1.6955241  -0.03533183 -0.07668875 -0.9282847  -1.3379647 ]\n",
      "  [-0.7501108   0.8765294  -0.47383803  1.5073229   0.28599858 -1.445902  ]\n",
      "  [-0.06526975 -0.07878304  1.818034   -1.6329195  -0.11263704  0.07157554]\n",
      "  [-1.436036   -0.01443835  0.5899757  -0.5433598  -0.37250015  1.7763586 ]]]\n",
      "\n",
      "============================== Encoder Layer ==============================\n",
      "\n",
      "Encoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 5, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.18154223 0.13954452 0.07820506 0.3520332  0.24867506]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.34058204 -0.23094237 -0.3476916  -0.00909077 -0.09537263  0.50830775]\n",
      "  [ 0.27050337 -0.39448062 -0.18182965 -0.2384155  -0.2506759   0.54774845]\n",
      "  [ 0.25551957 -0.3473255  -0.28692046 -0.03008293 -0.12157771  0.5116564 ]\n",
      "  [ 0.35188735 -0.18914673 -0.3746079  -0.00500718 -0.08078752  0.5022042 ]\n",
      "  [ 0.34224632 -0.17889315 -0.33142793 -0.17418596 -0.15626158  0.54402494]]]\n",
      "\n",
      "Post Attention (Norm):\n",
      "[[[-1.202161    0.99443835  1.1902516  -1.4311206   0.23820938  0.21038237]\n",
      "  [ 1.117406    1.5082058  -0.19772902 -0.30778265 -1.2784569  -0.8416434 ]\n",
      "  [-0.5884336   0.63735044 -0.90711427  1.7724288   0.20059785 -1.1148293 ]\n",
      "  [ 0.2691963  -0.32195637  1.5023651  -1.782386   -0.24253312  0.57531387]\n",
      "  [-0.98648334 -0.17995673  0.22478463 -0.6494874  -0.48039645  2.0715394 ]]]\n",
      "\n",
      "Encoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 5, 6])\n",
      "[[[-1.202161    0.99443835  1.1902516  -1.4311206   0.23820938  0.21038237]\n",
      "  [ 1.117406    1.5082058  -0.19772902 -0.30778265 -1.2784569  -0.8416434 ]\n",
      "  [-0.5884336   0.63735044 -0.90711427  1.7724288   0.20059785 -1.1148293 ]\n",
      "  [ 0.2691963  -0.32195637  1.5023651  -1.782386   -0.24253312  0.57531387]\n",
      "  [-0.98648334 -0.17995673  0.22478463 -0.6494874  -0.48039645  2.0715394 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.31386253  0.10006846 -0.24563113 -0.18273315  0.16127576 -0.07760863]\n",
      "  [ 0.16571489 -0.07943158 -0.03224346 -0.00315954  0.2955658   0.10683022]\n",
      "  [ 0.08923146  0.30980918  0.11766657 -0.30418217  0.04409704 -0.27077997]\n",
      "  [ 0.62145585 -0.13177806  0.09280512 -0.1153632   0.25087494  0.10268885]\n",
      "  [-0.15204182  0.16991948  0.07563189  0.04124837  0.02323717 -0.05237693]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.9264432   1.1149883   0.9606703  -1.6734514   0.3994167   0.12481932]\n",
      "  [ 1.283733    1.4385724  -0.32478666 -0.4108629  -1.1251897  -0.86146617]\n",
      "  [-0.5008187   0.95711833 -0.79338706  1.4823756   0.24903175 -1.3943197 ]\n",
      "  [ 0.67461073 -0.5284292   1.3050569  -1.8206224  -0.11493489  0.4843189 ]\n",
      "  [-1.1522213  -0.02754687  0.28185782 -0.62372714 -0.47315797  1.9947958 ]]]\n",
      "\n",
      "======================================== Decoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[1 3 5 6]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 4, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.8287523   0.2831893   3.9240348   3.7534795   4.327421   -0.7995676 ]\n",
      "  [ 0.69877243  2.2029848  -0.25926727 -0.67575365 -4.4152427  -1.8571308 ]\n",
      "  [-4.78185    -1.1971927   0.40134028 -1.2475296  -0.290999    2.1355565 ]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 4, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.8287523   0.2831893   3.9240348   3.7534795   4.327421   -0.7995676 ]\n",
      "  [ 0.69877243  2.2029848  -0.25926727 -0.67575365 -4.4152427  -1.8571308 ]\n",
      "  [-4.78185    -1.1971927   0.40134028 -1.2475296  -0.290999    2.1355565 ]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[ 0.67611086 -0.8870462  -5.0502243   3.6508052  -0.7652741   2.5243046 ]\n",
      "  [ 2.6702232   0.8234916   3.970434    4.7524023   4.3295755   0.20043015]\n",
      "  [ 1.6080699   1.7868379  -0.16656877  0.31994057 -4.410934   -0.85714   ]\n",
      "  [-4.64073    -2.1871853   0.54013836 -0.25720894 -0.28453574  3.1355355 ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 4, 6]), V: torch.Size([1, 4, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.7943455   0.6793327  -0.34162438  0.4902337   1.1164801  -0.5476399 ]\n",
      "  [-1.0797566   1.0152009  -0.27235162  0.4350009   0.92560196 -0.7878978 ]\n",
      "  [-2.3549738  -0.52950686  2.6540654  -0.06441337  1.367532    1.4448286 ]\n",
      "  [-0.87679815 -0.9325884   0.32541755  0.2375657   0.48482102 -0.53591   ]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[-0.08414338 -0.11507369 -1.9070811   1.3881658   0.07812884  0.6400035 ]\n",
      "  [-0.5912158  -0.47285786  0.41372806  1.1238592   1.156175   -1.6296885 ]\n",
      "  [-0.50949913  0.65108985  1.3634397   0.07097693 -1.8393284   0.26332107]\n",
      "  [-1.7387669  -0.8489934   0.62990713  0.30142203  0.38303447  1.2733965 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.23680963 0.15480056 0.14082599 0.20976941 0.2577944 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.13987225  0.00973104 -0.31805223 -0.45877331 -0.26560825  0.03092791]\n",
      "  [-0.18622765  0.08770409 -0.3854147  -0.43213606 -0.2577111   0.06369889]\n",
      "  [-0.28389195  0.18009281 -0.487211   -0.23711032 -0.15904321  0.1315152 ]\n",
      "  [-0.1872333   0.07420965 -0.38051742 -0.3751537  -0.21618238  0.05219884]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.7009249  -0.23679237  0.25239608  1.037301    1.2819037  -1.6338835 ]\n",
      "  [-0.6431568   0.9623793   1.0068974  -0.02324966 -1.8340167   0.5311463 ]\n",
      "  [-1.7561184  -0.60343736  0.42203903  0.09850659  0.33939642  1.4996139 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 4, 6])\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.7009249  -0.23679237  0.25239608  1.037301    1.2819037  -1.6338835 ]\n",
      "  [-0.6431568   0.9623793   1.0068974  -0.02324966 -1.8340167   0.5311463 ]\n",
      "  [-1.7561184  -0.60343736  0.42203903  0.09850659  0.33939642  1.4996139 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.00740568  0.07201971  0.29183885  0.06082219  0.28262395 -0.11354794]\n",
      "  [ 0.30829027 -0.06450079 -0.36408654  0.36071935  0.40899977 -0.48709214]\n",
      "  [ 0.3538295   0.22565077  0.64372325 -0.17271914 -0.15195689 -0.6428646 ]\n",
      "  [-0.17472008  0.08404067  0.19895141  0.11166283 -0.01944885 -0.10974151]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.13946532  0.0616485  -2.0114903   1.1798747   0.20477162  0.70466095]\n",
      "  [-0.33239618 -0.26005334 -0.10988709  1.085811    1.317776   -1.7012504 ]\n",
      "  [-0.2838732   0.97956264  1.3751702  -0.20403299 -1.7348449  -0.13198197]\n",
      "  [-1.8753257  -0.51511806  0.58387387  0.18796533  0.29375863  1.324846  ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 4, 6]), V: torch.Size([1, 4, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.5743567   0.8840643   0.23544767  0.15842699  0.1647327  -0.15561277]\n",
      "  [ 0.34580147  0.7363907   0.16260532  0.02550864  0.06600307 -0.15034615]\n",
      "  [ 0.25721753  0.5072118   0.05058438 -0.09558544 -0.14529566 -0.17826915]\n",
      "  [ 0.35954005  0.43750635  0.12438244  0.02503309 -0.07065135 -0.22134943]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[ 0.12581992  0.64141333 -2.1057682   1.0376695   0.05982208  0.24104318]\n",
      "  [-0.1760304   0.26623726 -0.13847233  0.8728751   1.1331725  -1.957782  ]\n",
      "  [-0.08053061  1.2351737   1.1821259  -0.3178315  -1.6918625  -0.32707503]\n",
      "  [-1.9785064  -0.22732052  0.72958803  0.12653957  0.13884859  1.2108506 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.26710007 0.173288   0.16750504 0.21446617 0.1776406 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.87049305 -0.26932955 -0.07900444  0.1696752  -0.1942007  -0.0319175 ]\n",
      "  [ 0.7140084  -0.13868251 -0.00593758  0.14271499 -0.16505073  0.04368192]\n",
      "  [ 0.617641    0.01464386  0.03517827  0.20320287 -0.09326575  0.22452751]\n",
      "  [ 0.83723867 -0.22938564 -0.06479691  0.1355586  -0.16271973 -0.00705934]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.4431974   0.02934236 -0.24489635  0.9248031   0.87693787 -2.029384  ]\n",
      "  [ 0.35905886  1.0504621   1.0189207  -0.27319857 -1.8937643  -0.2614786 ]\n",
      "  [-1.6220058  -0.71638083  0.76727796  0.23454471 -0.14377177  1.4803357 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 4, 6])\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.4431974   0.02934236 -0.24489635  0.9248031   0.87693787 -2.029384  ]\n",
      "  [ 0.35905886  1.0504621   1.0189207  -0.27319857 -1.8937643  -0.2614786 ]\n",
      "  [-1.6220058  -0.71638083  0.76727796  0.23454471 -0.14377177  1.4803357 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.08223304  0.0510872  -0.44664547  0.2856996  -0.06149156 -0.15877774]\n",
      "  [ 0.30049682 -0.03921787 -0.29893655  0.09110181 -0.01120077 -0.31495458]\n",
      "  [ 0.28378633 -0.25326186 -0.3567241   0.4403119  -0.269485   -0.38583827]\n",
      "  [ 0.19974194 -0.11045322 -0.26045525  0.36354902 -0.17380571 -0.42462105]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[ 0.6814094   0.32227704 -2.0220773   1.1483729  -0.15379624  0.02381406]\n",
      "  [ 0.678599    0.0305926  -0.42856577  0.9126773   0.7835456  -1.9768487 ]\n",
      "  [ 0.70078725  0.8483496   0.71928704  0.24599129 -1.9818175  -0.5325977 ]\n",
      "  [-1.5625759  -0.8757228   0.662706    0.76799095 -0.28827405  1.2958758 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass ---------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40 + \" Starting Forward Pass \" + \"=\"*40)\n",
    "outputs = model(src, tgt)  # Shape: (1,4, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Loss Calculation ========================================\n",
      "\n",
      "Final Loss: 1.8822\n"
     ]
    }
   ],
   "source": [
    "# Loss Calculation -----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40 + \" Loss Calculation \" + \"=\"*40)\n",
    "targets = tgt[:, 1:].contiguous().view(-1)\n",
    "loss = criterion(outputs.view(-1, vocab_size), targets)\n",
    "print(f\"\\nFinal Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Backward Pass ========================================\n"
     ]
    }
   ],
   "source": [
    "# Backward Pass --------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40 + \" Backward Pass \" + \"=\"*40)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Gradient Magnitudes ========================================\n",
      "encoder.embed.embed.weight                         0.0825\n",
      "encoder.layers.0.self_attn.WQ.weight               0.0559\n",
      "encoder.layers.0.self_attn.WQ.bias                 0.0065\n",
      "encoder.layers.0.self_attn.WK.weight               0.0486\n",
      "encoder.layers.0.self_attn.WK.bias                 0.0000\n",
      "encoder.layers.0.self_attn.WV.weight               0.0697\n",
      "encoder.layers.0.self_attn.WV.bias                 0.0254\n",
      "encoder.layers.0.self_attn.WO.weight               0.0926\n",
      "encoder.layers.0.self_attn.WO.bias                 0.0476\n",
      "encoder.layers.0.ffn.ffn.0.weight                  0.0451\n",
      "encoder.layers.0.ffn.ffn.0.bias                    0.0170\n",
      "encoder.layers.0.ffn.ffn.2.weight                  0.1116\n",
      "encoder.layers.0.ffn.ffn.2.bias                    0.0921\n",
      "encoder.layers.0.norm1.weight                      0.0424\n",
      "encoder.layers.0.norm1.bias                        0.0928\n",
      "encoder.layers.0.norm2.weight                      0.0345\n",
      "encoder.layers.0.norm2.bias                        0.0869\n",
      "encoder.layers.1.self_attn.WQ.weight               0.0188\n",
      "encoder.layers.1.self_attn.WQ.bias                 0.0109\n",
      "encoder.layers.1.self_attn.WK.weight               0.0133\n",
      "encoder.layers.1.self_attn.WK.bias                 0.0000\n",
      "encoder.layers.1.self_attn.WV.weight               0.0905\n",
      "encoder.layers.1.self_attn.WV.bias                 0.0758\n",
      "encoder.layers.1.self_attn.WO.weight               0.0810\n",
      "encoder.layers.1.self_attn.WO.bias                 0.1059\n",
      "encoder.layers.1.ffn.ffn.0.weight                  0.0826\n",
      "encoder.layers.1.ffn.ffn.0.bias                    0.0445\n",
      "encoder.layers.1.ffn.ffn.2.weight                  0.1092\n",
      "encoder.layers.1.ffn.ffn.2.bias                    0.0981\n",
      "encoder.layers.1.norm1.weight                      0.0401\n",
      "encoder.layers.1.norm1.bias                        0.0976\n",
      "encoder.layers.1.norm2.weight                      0.1376\n",
      "encoder.layers.1.norm2.bias                        0.1546\n",
      "decoder.embed.embed.weight                         0.4896\n",
      "decoder.layers.0.self_attn.WQ.weight               0.2864\n",
      "decoder.layers.0.self_attn.WQ.bias                 0.0470\n",
      "decoder.layers.0.self_attn.WK.weight               0.4518\n",
      "decoder.layers.0.self_attn.WK.bias                 0.0000\n",
      "decoder.layers.0.self_attn.WV.weight               0.8104\n",
      "decoder.layers.0.self_attn.WV.bias                 0.0934\n",
      "decoder.layers.0.self_attn.WO.weight               0.5257\n",
      "decoder.layers.0.self_attn.WO.bias                 0.1370\n",
      "decoder.layers.0.cross_attn.WQ.weight              0.0885\n",
      "decoder.layers.0.cross_attn.WQ.bias                0.0306\n",
      "decoder.layers.0.cross_attn.WK.weight              0.0745\n",
      "decoder.layers.0.cross_attn.WK.bias                0.0000\n",
      "decoder.layers.0.cross_attn.WV.weight              0.2353\n",
      "decoder.layers.0.cross_attn.WV.bias                0.2154\n",
      "decoder.layers.0.cross_attn.WO.weight              0.3342\n",
      "decoder.layers.0.cross_attn.WO.bias                0.4384\n",
      "decoder.layers.0.ffn.ffn.0.weight                  0.3886\n",
      "decoder.layers.0.ffn.ffn.0.bias                    0.1512\n",
      "decoder.layers.0.ffn.ffn.2.weight                  0.8126\n",
      "decoder.layers.0.ffn.ffn.2.bias                    0.4836\n",
      "decoder.layers.0.norm1.weight                      0.3416\n",
      "decoder.layers.0.norm1.bias                        0.4271\n",
      "decoder.layers.0.norm2.weight                      0.2621\n",
      "decoder.layers.0.norm2.bias                        0.4801\n",
      "decoder.layers.0.norm3.weight                      0.2198\n",
      "decoder.layers.0.norm3.bias                        0.4877\n",
      "decoder.layers.1.self_attn.WQ.weight               0.1245\n",
      "decoder.layers.1.self_attn.WQ.bias                 0.0245\n",
      "decoder.layers.1.self_attn.WK.weight               0.2463\n",
      "decoder.layers.1.self_attn.WK.bias                 0.0000\n",
      "decoder.layers.1.self_attn.WV.weight               0.3951\n",
      "decoder.layers.1.self_attn.WV.bias                 0.2981\n",
      "decoder.layers.1.self_attn.WO.weight               0.4101\n",
      "decoder.layers.1.self_attn.WO.bias                 0.4267\n",
      "decoder.layers.1.cross_attn.WQ.weight              0.1734\n",
      "decoder.layers.1.cross_attn.WQ.bias                0.0327\n",
      "decoder.layers.1.cross_attn.WK.weight              0.1501\n",
      "decoder.layers.1.cross_attn.WK.bias                0.0000\n",
      "decoder.layers.1.cross_attn.WV.weight              0.2532\n",
      "decoder.layers.1.cross_attn.WV.bias                0.2397\n",
      "decoder.layers.1.cross_attn.WO.weight              0.3266\n",
      "decoder.layers.1.cross_attn.WO.bias                0.3231\n",
      "decoder.layers.1.ffn.ffn.0.weight                  0.5985\n",
      "decoder.layers.1.ffn.ffn.0.bias                    0.0819\n",
      "decoder.layers.1.ffn.ffn.2.weight                  0.3952\n",
      "decoder.layers.1.ffn.ffn.2.bias                    0.2774\n",
      "decoder.layers.1.norm1.weight                      0.3303\n",
      "decoder.layers.1.norm1.bias                        0.3275\n",
      "decoder.layers.1.norm2.weight                      0.4246\n",
      "decoder.layers.1.norm2.bias                        0.2798\n",
      "decoder.layers.1.norm3.weight                      0.5263\n",
      "decoder.layers.1.norm3.bias                        0.4479\n",
      "final_layer.weight                                 2.2461\n",
      "final_layer.bias                                   0.7398\n"
     ]
    }
   ],
   "source": [
    "# Gradient Monitoring -------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40 + \" Gradient Magnitudes \" + \"=\"*40)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name:50} {param.grad.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction/Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction/Inference Example -----------------------------------------------------\n",
    "def predict(model, src_input, max_length=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode source sequence\n",
    "        enc_out = model.encoder(src_input)\n",
    "        \n",
    "        # Initialize target with <start> token\n",
    "        target = torch.LongTensor([[vocab[\"<start>\"]]]).to(src_input.device)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40 + \" Prediction Start \" + \"=\"*40)\n",
    "        for i in range(max_length-1):\n",
    "            # Generate prediction\n",
    "            output = model.decoder(target, enc_out)\n",
    "            logits = model.final_layer(output[:, -1, :])\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = logits.argmax(-1).item()\n",
    "            print(f\"Step {i+1}: Predicted token {next_token} ({list(vocab.keys())[list(vocab.values()).index(next_token)]})\")\n",
    "            \n",
    "            # Append to target sequence\n",
    "            target = torch.cat([target, torch.LongTensor([[next_token]]).to(src_input.device)], dim=1)\n",
    "            \n",
    "            if next_token == vocab[\"<pad>\"]:  # Early stopping\n",
    "                break\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================== Making Prediction ========================================\n",
      "\n",
      "======================================== Encoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[3 5 6 4 2]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 5, 6])\n",
      "[[[-3.5316937e+00  1.6512412e+00  2.7454066e+00 -2.3487000e+00 -8.1406766e-01  3.5352362e-03]\n",
      "  [ 2.9384294e+00  5.3358903e+00 -1.2465447e+00  2.3554676e+00 -2.9935477e+00 -1.7453690e+00]\n",
      "  [-1.4768381e+00  2.8245058e+00 -1.8362260e+00  3.9558768e+00  4.9465564e-01 -1.7560655e+00]\n",
      "  [-4.9802560e-01 -1.5842095e-01  1.4116590e+00 -2.3076582e+00 -2.0125613e+00 -5.9117329e-01]\n",
      "  [-1.2358226e+00 -8.2297730e-01 -4.2127466e-01 -7.5625145e-01 -1.6337032e+00  2.1894784e+00]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 5, 6])\n",
      "[[[-3.5316937e+00  1.6512412e+00  2.7454066e+00 -2.3487000e+00 -8.1406766e-01  3.5352362e-03]\n",
      "  [ 2.9384294e+00  5.3358903e+00 -1.2465447e+00  2.3554676e+00 -2.9935477e+00 -1.7453690e+00]\n",
      "  [-1.4768381e+00  2.8245058e+00 -1.8362260e+00  3.9558768e+00  4.9465564e-01 -1.7560655e+00]\n",
      "  [-4.9802560e-01 -1.5842095e-01  1.4116590e+00 -2.3076582e+00 -2.0125613e+00 -5.9117329e-01]\n",
      "  [-1.2358226e+00 -8.2297730e-01 -4.2127466e-01 -7.5625145e-01 -1.6337032e+00  2.1894784e+00]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[-3.5316937   2.6512413   2.7454066  -1.3487     -0.81406766  1.0035353 ]\n",
      "  [ 3.7799003   5.8761926  -1.2001455   3.3543906  -2.9913933  -0.7453712 ]\n",
      "  [-0.5675407   2.408359   -1.7435275   4.951571    0.4989645  -0.7560747 ]\n",
      "  [-0.35690558 -1.1484134   1.5504571  -1.3173375  -2.006098    0.4088058 ]\n",
      "  [-1.992625   -1.4766209  -0.23667593  0.2265625  -1.6250856   3.1894412 ]]]\n",
      "\n",
      "============================== Encoder Layer ==============================\n",
      "\n",
      "Encoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 5, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.1577217  0.1535524  0.3144288  0.15010248 0.2241946 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.05036052  0.41356683  0.14896062 -1.3904159   0.5025073  -0.48000228]\n",
      "  [-0.21323597  0.03128868  0.52338433 -1.546832    0.6308199  -0.7806337 ]\n",
      "  [-0.02009055  0.08141217  0.3470712  -1.2901261   0.5748379  -0.5601667 ]\n",
      "  [ 0.05929413  0.16330142  0.20698962 -1.3483413   0.58822    -0.459157  ]\n",
      "  [-0.09057808  0.34014374  0.31300536 -1.2460146   0.4770139  -0.42704076]]]\n",
      "\n",
      "Post Attention (Norm):\n",
      "[[[-1.3843883   1.2249113   1.1569734  -1.0885396  -0.12091307  0.21195611]\n",
      "  [ 0.83340555  1.6306677  -0.6118686   0.23426934 -1.1853606  -0.9011137 ]\n",
      "  [-0.6452778   0.9539053  -1.0655868   1.5627697   0.21809211 -1.0239028 ]\n",
      "  [ 0.23024943 -0.27670097  1.745612   -1.5159204  -0.59581476  0.41257465]\n",
      "  [-1.065255   -0.45715714  0.32184896 -0.38198993 -0.4646045   2.0471575 ]]]\n",
      "\n",
      "Encoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 5, 6])\n",
      "[[[-1.3843883   1.2249113   1.1569734  -1.0885396  -0.12091307  0.21195611]\n",
      "  [ 0.83340555  1.6306677  -0.6118686   0.23426934 -1.1853606  -0.9011137 ]\n",
      "  [-0.6452778   0.9539053  -1.0655868   1.5627697   0.21809211 -1.0239028 ]\n",
      "  [ 0.23024943 -0.27670097  1.745612   -1.5159204  -0.59581476  0.41257465]\n",
      "  [-1.065255   -0.45715714  0.32184896 -0.38198993 -0.4646045   2.0471575 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.21953236  0.05105793  0.4565798  -0.312665    0.48060864 -0.6216748 ]\n",
      "  [-0.08811759  0.09154925  0.66449654 -0.22153443  0.37664342 -0.30278203]\n",
      "  [ 0.00874774  0.06151623  0.70962846  0.09326078  0.1976096  -0.31924537]\n",
      "  [-0.24199294  0.249995    0.32791835 -0.23159163  0.5316242  -0.27279723]\n",
      "  [-0.08086288  0.58470494  0.34721735  0.03565602  0.2713505  -0.31516367]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-1.282765    1.0608141   1.3355311  -1.1178      0.31517452 -0.31095466]\n",
      "  [ 0.682746    1.6955241  -0.03533183 -0.07668875 -0.9282847  -1.3379647 ]\n",
      "  [-0.7501108   0.8765294  -0.47383803  1.5073229   0.28599858 -1.445902  ]\n",
      "  [-0.06526975 -0.07878304  1.818034   -1.6329195  -0.11263704  0.07157554]\n",
      "  [-1.436036   -0.01443835  0.5899757  -0.5433598  -0.37250015  1.7763586 ]]]\n",
      "\n",
      "============================== Encoder Layer ==============================\n",
      "\n",
      "Encoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 5, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.18154223 0.13954452 0.07820506 0.3520332  0.24867506]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.34058204 -0.23094237 -0.3476916  -0.00909077 -0.09537263  0.50830775]\n",
      "  [ 0.27050337 -0.39448062 -0.18182965 -0.2384155  -0.2506759   0.54774845]\n",
      "  [ 0.25551957 -0.3473255  -0.28692046 -0.03008293 -0.12157771  0.5116564 ]\n",
      "  [ 0.35188735 -0.18914673 -0.3746079  -0.00500718 -0.08078752  0.5022042 ]\n",
      "  [ 0.34224632 -0.17889315 -0.33142793 -0.17418596 -0.15626158  0.54402494]]]\n",
      "\n",
      "Post Attention (Norm):\n",
      "[[[-1.202161    0.99443835  1.1902516  -1.4311206   0.23820938  0.21038237]\n",
      "  [ 1.117406    1.5082058  -0.19772902 -0.30778265 -1.2784569  -0.8416434 ]\n",
      "  [-0.5884336   0.63735044 -0.90711427  1.7724288   0.20059785 -1.1148293 ]\n",
      "  [ 0.2691963  -0.32195637  1.5023651  -1.782386   -0.24253312  0.57531387]\n",
      "  [-0.98648334 -0.17995673  0.22478463 -0.6494874  -0.48039645  2.0715394 ]]]\n",
      "\n",
      "Encoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 5, 6])\n",
      "[[[-1.202161    0.99443835  1.1902516  -1.4311206   0.23820938  0.21038237]\n",
      "  [ 1.117406    1.5082058  -0.19772902 -0.30778265 -1.2784569  -0.8416434 ]\n",
      "  [-0.5884336   0.63735044 -0.90711427  1.7724288   0.20059785 -1.1148293 ]\n",
      "  [ 0.2691963  -0.32195637  1.5023651  -1.782386   -0.24253312  0.57531387]\n",
      "  [-0.98648334 -0.17995673  0.22478463 -0.6494874  -0.48039645  2.0715394 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.31386253  0.10006846 -0.24563113 -0.18273315  0.16127576 -0.07760863]\n",
      "  [ 0.16571489 -0.07943158 -0.03224346 -0.00315954  0.2955658   0.10683022]\n",
      "  [ 0.08923146  0.30980918  0.11766657 -0.30418217  0.04409704 -0.27077997]\n",
      "  [ 0.62145585 -0.13177806  0.09280512 -0.1153632   0.25087494  0.10268885]\n",
      "  [-0.15204182  0.16991948  0.07563189  0.04124837  0.02323717 -0.05237693]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.9264432   1.1149883   0.9606703  -1.6734514   0.3994167   0.12481932]\n",
      "  [ 1.283733    1.4385724  -0.32478666 -0.4108629  -1.1251897  -0.86146617]\n",
      "  [-0.5008187   0.95711833 -0.79338706  1.4823756   0.24903175 -1.3943197 ]\n",
      "  [ 0.67461073 -0.5284292   1.3050569  -1.8206224  -0.11493489  0.4843189 ]\n",
      "  [-1.1522213  -0.02754687  0.28185782 -0.62372714 -0.47315797  1.9947958 ]]]\n",
      "\n",
      "======================================== Prediction Start ========================================\n",
      "\n",
      "======================================== Decoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[1]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 1, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 1, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[ 0.67611086 -0.8870462  -5.0502243   3.6508052  -0.7652741   2.5243046 ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 1, 6]), K: torch.Size([1, 1, 6]), V: torch.Size([1, 1, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1.]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.7943455   0.6793328  -0.34162438  0.4902335   1.1164801  -0.54763985]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[-0.08414341 -0.11507366 -1.907081    1.3881657   0.07812881  0.64000344]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 1, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.23680963 0.15480058 0.140826   0.20976943 0.2577944 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.13987227  0.00973105 -0.31805226 -0.4587733  -0.26560822  0.03092791]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[-0.03334233  0.083928   -2.010806    1.1064321   0.00276208  0.8510262 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 1, 6])\n",
      "[[[-0.03334233  0.083928   -2.010806    1.1064321   0.00276208  0.8510262 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.00740567  0.0720197   0.29183882  0.06082219  0.28262392 -0.11354793]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.13946538  0.06164855 -2.0114903   1.1798747   0.20477162  0.704661  ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 1, 6]), K: torch.Size([1, 1, 6]), V: torch.Size([1, 1, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1.]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.5743567   0.8840643   0.23544756  0.15842701  0.1647327  -0.15561274]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[ 0.12581983  0.6414134  -2.1057682   1.0376695   0.05982205  0.24104327]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 1, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.26710007 0.173288   0.16750504 0.21446617 0.1776406 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.87049305 -0.26932955 -0.07900444  0.16967519 -0.19420072 -0.0319175 ]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[ 0.8279261   0.26537126 -2.0388665   1.018108   -0.19105247  0.11851353]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 1, 6])\n",
      "[[[ 0.8279261   0.26537126 -2.0388665   1.018108   -0.19105247  0.11851353]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.08223304  0.0510872  -0.44664544  0.2856996  -0.06149159 -0.15877776]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[ 0.68140936  0.32227707 -2.0220773   1.148373   -0.15379627  0.02381413]]]\n",
      "Step 1: Predicted token 2 (football)\n",
      "\n",
      "======================================== Decoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[1 2]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 2, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 2, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[ 0.67611086 -0.8870462  -5.0502243   3.6508052  -0.7652741   2.5243046 ]\n",
      "  [ 1.9751263   1.5624372   1.2523872   1.1027219   2.6508937   0.17112571]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 2, 6]), K: torch.Size([1, 2, 6]), V: torch.Size([1, 2, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.7943455   0.6793329  -0.3416245   0.49023354  1.1164801  -0.54763997]\n",
      "  [-0.5651562   0.17214173  0.39176157  0.1599339   1.1503499  -0.06207694]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[-0.08414341 -0.11507362 -1.907081    1.3881657   0.07812881  0.6400034 ]\n",
      "  [-0.2282318   0.06775223 -0.01470357 -0.36255547  1.9521735  -1.4144348 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 2, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.23680963 0.15480058 0.140826   0.20976943 0.2577944 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.13987225  0.00973105 -0.31805226 -0.4587733  -0.26560822  0.03092793]\n",
      "  [-0.19483826  0.10718629 -0.4037957  -0.393172   -0.24284904  0.08154393]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[-0.0333423   0.08392803 -2.010806    1.1064321   0.00276208  0.8510262 ]\n",
      "  [-0.26047453  0.36572227 -0.25568825 -0.60881233  1.9724343  -1.2131814 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 2, 6])\n",
      "[[[-0.0333423   0.08392803 -2.010806    1.1064321   0.00276208  0.8510262 ]\n",
      "  [-0.26047453  0.36572227 -0.25568825 -0.60881233  1.9724343  -1.2131814 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.00740568  0.0720197   0.29183882  0.06082216  0.28262395 -0.11354794]\n",
      "  [ 0.30430973  0.09494828 -0.7111801   0.667899    0.5789343  -0.6132789 ]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.13946538  0.06164856 -2.0114903   1.1798747   0.20477161  0.70466095]\n",
      "  [-0.00722209  0.30090132 -0.75433105  0.00405176  1.8463391  -1.3897388 ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 2, 6]), K: torch.Size([1, 2, 6]), V: torch.Size([1, 2, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.5743567   0.8840643   0.23544759  0.15842703  0.16473275 -0.15561277]\n",
      "  [ 0.25968087  0.8600394   0.23234692 -0.06395748  0.10467176 -0.16520198]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[ 0.12581986  0.6414134  -2.1057682   1.0376695   0.05982211  0.24104318]\n",
      "  [ 0.0423577   0.8463576  -0.64301854 -0.23408258  1.5455637  -1.5571778 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 2, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.26710007 0.173288   0.16750504 0.21446617 0.1776406 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.87049305 -0.26932955 -0.07900441  0.1696752  -0.19420072 -0.0319175 ]\n",
      "  [ 0.67456096 -0.10025306  0.01203014  0.14709716 -0.14998767  0.07784672]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[ 0.8279262   0.2653713  -2.0388665   1.0181081  -0.19105247  0.11851344]\n",
      "  [ 0.6314124   0.661787   -0.7713915  -0.20523265  1.3377099  -1.654285  ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 2, 6])\n",
      "[[[ 0.8279262   0.2653713  -2.0388665   1.0181081  -0.19105247  0.11851344]\n",
      "  [ 0.6314124   0.661787   -0.7713915  -0.20523265  1.3377099  -1.654285  ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.08223303  0.0510872  -0.44664544  0.28569967 -0.06149158 -0.15877774]\n",
      "  [ 0.1841785   0.23134395 -0.34468013 -0.23668471 -0.21082743 -0.31749916]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[ 0.6814094   0.3222771  -2.0220773   1.148373   -0.15379629  0.02381404]\n",
      "  [ 0.8074601   0.87469035 -0.8673649  -0.28284737  1.0773618  -1.6093001 ]]]\n",
      "Step 2: Predicted token 2 (football)\n",
      "\n",
      "======================================== Decoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[1 2 2]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 3, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 3, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[ 0.67611086 -0.8870462  -5.0502243   3.6508052  -0.7652741   2.5243046 ]\n",
      "  [ 1.9751263   1.5624372   1.2523872   1.1027219   2.6508937   0.17112571]\n",
      "  [ 2.0429528   0.605988    1.2986864   1.0994931   2.6530483   0.17111874]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 3, 6]), K: torch.Size([1, 3, 6]), V: torch.Size([1, 3, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.7943455   0.6793327  -0.34162438  0.4902337   1.1164801  -0.5476399 ]\n",
      "  [-0.5651562   0.17214179  0.39176145  0.15993395  1.15035    -0.06207705]\n",
      "  [-0.54370373  0.09576175  0.65453136  0.02835655  1.1327348   0.11253493]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[-0.08414338 -0.11507369 -1.9070811   1.3881658   0.07812884  0.6400035 ]\n",
      "  [-0.2282318   0.06775233 -0.01470368 -0.36255547  1.9521737  -1.4144349 ]\n",
      "  [-0.05249683 -0.75808823  0.34915432 -0.38109422  1.9705255  -1.128001  ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 3, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.23680963 0.15480056 0.14082599 0.20976941 0.2577944 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.13987225  0.00973104 -0.31805223 -0.45877331 -0.26560825  0.03092791]\n",
      "  [-0.19483823  0.10718627 -0.4037957  -0.39317203 -0.24284905  0.08154395]\n",
      "  [-0.17839092  0.09113851 -0.38856316 -0.4125562  -0.251533    0.0708494 ]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.26047444  0.3657223  -0.25568837 -0.60881233  1.9724342  -1.2131813 ]\n",
      "  [-0.0576757  -0.53480214  0.151835   -0.6734343   2.075827   -0.96174973]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 3, 6])\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.26047444  0.3657223  -0.25568837 -0.60881233  1.9724342  -1.2131813 ]\n",
      "  [-0.0576757  -0.53480214  0.151835   -0.6734343   2.075827   -0.96174973]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.00740566  0.07201973  0.29183885  0.06082216  0.28262395 -0.11354792]\n",
      "  [ 0.30430967  0.09494825 -0.7111801   0.667899    0.5789342  -0.61327887]\n",
      "  [ 0.21440068  0.11717276 -0.5933806   0.5363779   0.57869    -0.5165739 ]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.13946535  0.0616485  -2.0114903   1.1798747   0.20477162  0.70466095]\n",
      "  [-0.00722205  0.30090138 -0.75433123  0.00405178  1.8463389  -1.3897388 ]\n",
      "  [ 0.07943293 -0.3740253  -0.39290738 -0.15251027  2.0514636  -1.2114536 ]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 3, 6]), K: torch.Size([1, 3, 6]), V: torch.Size([1, 3, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.5743567   0.8840643   0.23544765  0.158427    0.16473272 -0.15561277]\n",
      "  [ 0.2596808   0.86003935  0.23234692 -0.0639575   0.10467169 -0.165202  ]\n",
      "  [ 0.0786581   0.8414962   0.2678793  -0.19526717  0.08046815 -0.19526175]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[ 0.12581992  0.64141333 -2.1057682   1.0376695   0.05982208  0.24104318]\n",
      "  [ 0.04235772  0.84635764 -0.6430187  -0.23408253  1.5455636  -1.5571777 ]\n",
      "  [ 0.01106851  0.30220157 -0.2553526  -0.46496448  1.8684946  -1.4614475 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 3, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.26710007 0.173288   0.16750504 0.21446617 0.1776406 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.87049305 -0.26932955 -0.07900444  0.1696752  -0.1942007  -0.0319175 ]\n",
      "  [ 0.674561   -0.10025308  0.01203015  0.14709719 -0.14998768  0.07784672]\n",
      "  [ 0.67799765 -0.09590378  0.01653191  0.130569   -0.14098266  0.07776298]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.63141245  0.6617869  -0.7713917  -0.20523259  1.3377094  -1.6542847 ]\n",
      "  [ 0.60315835  0.099438   -0.36499846 -0.46472117  1.6866724  -1.5595491 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 3, 6])\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.63141245  0.6617869  -0.7713917  -0.20523259  1.3377094  -1.6542847 ]\n",
      "  [ 0.60315835  0.099438   -0.36499846 -0.46472117  1.6866724  -1.5595491 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.08223304  0.0510872  -0.4466455   0.2856996  -0.06149156 -0.15877773]\n",
      "  [ 0.18417849  0.23134387 -0.34468013 -0.23668462 -0.21082735 -0.31749913]\n",
      "  [ 0.26005793  0.2162382  -0.2855111  -0.20297903 -0.18367718 -0.3693515 ]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[ 0.6814094   0.32227704 -2.0220773   1.1483729  -0.15379624  0.02381408]\n",
      "  [ 0.80746025  0.8746903  -0.86736494 -0.28284723  1.0773616  -1.6092999 ]\n",
      "  [ 0.84844655  0.36322746 -0.49298704 -0.508221    1.4154059  -1.6258715 ]]]\n",
      "Step 3: Predicted token 2 (football)\n",
      "\n",
      "======================================== Decoder Start ========================================\n",
      "\n",
      "Embedding Input (token IDs): [[1 2 2 2]]\n",
      "\n",
      "Embedding Output: torch.Size([1, 4, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Input: torch.Size([1, 4, 6])\n",
      "[[[ 0.67611086 -1.8870462  -5.0502243   2.6508052  -0.7652741   1.5243046 ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]\n",
      "  [ 1.1336553   1.0221349   1.2059879   0.10379888  2.6487393  -0.828872  ]]]\n",
      "\n",
      "Positional Encoding Output:\n",
      "[[[ 0.67611086 -0.8870462  -5.0502243   3.6508052  -0.7652741   2.5243046 ]\n",
      "  [ 1.9751263   1.5624372   1.2523872   1.1027219   2.6508937   0.17112571]\n",
      "  [ 2.0429528   0.605988    1.2986864   1.0994931   2.6530483   0.17111874]\n",
      "  [ 1.2747753   0.0321424   1.344786    1.0941195   2.6552026   0.17110705]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 4, 6]), V: torch.Size([1, 4, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.7943455   0.6793327  -0.34162438  0.4902337   1.1164801  -0.5476399 ]\n",
      "  [-0.5651562   0.17214179  0.39176145  0.15993395  1.15035    -0.06207705]\n",
      "  [-0.54370373  0.09576175  0.65453136  0.02835655  1.1327348   0.11253493]\n",
      "  [-0.5476867   0.00212398  0.7310231   0.02671173  1.1096355   0.12859222]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[-0.08414338 -0.11507369 -1.9070811   1.3881658   0.07812884  0.6400035 ]\n",
      "  [-0.2282318   0.06775233 -0.01470368 -0.36255547  1.9521737  -1.4144349 ]\n",
      "  [-0.05249683 -0.75808823  0.34915432 -0.38109422  1.9705255  -1.128001  ]\n",
      "  [-0.48144254 -1.0282524   0.58303493 -0.17068118  1.9161011  -0.8187595 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.23680963 0.15480056 0.14082599 0.20976941 0.2577944 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[-0.13987225  0.00973104 -0.31805223 -0.45877331 -0.26560825  0.03092791]\n",
      "  [-0.19483823  0.10718627 -0.4037957  -0.39317203 -0.24284905  0.08154395]\n",
      "  [-0.17839092  0.09113851 -0.38856316 -0.4125562  -0.251533    0.0708494 ]\n",
      "  [-0.17393962  0.0822273  -0.3811898  -0.418092   -0.25138047  0.06312336]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.26047444  0.3657223  -0.25568837 -0.60881233  1.9724342  -1.2131813 ]\n",
      "  [-0.0576757  -0.53480214  0.151835   -0.6734343   2.075827   -0.96174973]\n",
      "  [-0.5283359  -0.85126936  0.4241295  -0.45432672  2.0495307  -0.6397282 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 4, 6])\n",
      "[[[-0.03334221  0.083928   -2.010806    1.1064321   0.00276213  0.8510262 ]\n",
      "  [-0.26047444  0.3657223  -0.25568837 -0.60881233  1.9724342  -1.2131813 ]\n",
      "  [-0.0576757  -0.53480214  0.151835   -0.6734343   2.075827   -0.96174973]\n",
      "  [-0.5283359  -0.85126936  0.4241295  -0.45432672  2.0495307  -0.6397282 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[ 0.00740568  0.07201971  0.29183885  0.06082219  0.28262395 -0.11354794]\n",
      "  [ 0.30430967  0.09494823 -0.7111801   0.667899    0.5789342  -0.61327887]\n",
      "  [ 0.21440068  0.11717276 -0.5933806   0.5363779   0.57869    -0.5165739 ]\n",
      "  [ 0.1389437   0.09191491 -0.4153165   0.42583963  0.5012624  -0.44481504]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[-0.13946532  0.0616485  -2.0114903   1.1798747   0.20477162  0.70466095]\n",
      "  [-0.00722205  0.30090138 -0.75433123  0.00405178  1.8463389  -1.3897388 ]\n",
      "  [ 0.07943293 -0.3740253  -0.39290738 -0.15251027  2.0514636  -1.2114536 ]\n",
      "  [-0.37111047 -0.68383795 -0.03450936 -0.06603896  2.1142151  -0.95871854]]]\n",
      "\n",
      "============================== Decoder Layer ==============================\n",
      "\n",
      "Decoder Self-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=True)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 4, 6]), V: torch.Size([1, 4, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[1. 0. 0. 0.]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 5.7435668e-01  8.8406432e-01  2.3544767e-01  1.5842699e-01  1.6473269e-01 -1.5561277e-01]\n",
      "  [ 2.5968081e-01  8.6003935e-01  2.3234692e-01 -6.3957505e-02  1.0467169e-01 -1.6520201e-01]\n",
      "  [ 7.8658104e-02  8.4149623e-01  2.6787931e-01 -1.9526717e-01  8.0468133e-02 -1.9526175e-01]\n",
      "  [-7.3426962e-04  8.1365258e-01  2.6859042e-01 -2.5329274e-01  8.9551240e-02 -2.0966871e-01]]]\n",
      "\n",
      "Post Self-Attention (Norm):\n",
      "[[[ 0.12581992  0.64141333 -2.1057682   1.0376695   0.05982208  0.24104318]\n",
      "  [ 0.04235772  0.84635764 -0.6430187  -0.23408253  1.5455636  -1.5571777 ]\n",
      "  [ 0.01106851  0.30220157 -0.2553526  -0.46496448  1.8684946  -1.4614475 ]\n",
      "  [-0.47246057  0.01137914  0.11194188 -0.42181286  2.011661   -1.2407087 ]]]\n",
      "\n",
      "Decoder Cross-Attention:\n",
      "\n",
      "MultiHeadAttention (Decoder=False)\n",
      "Input Q: torch.Size([1, 4, 6]), K: torch.Size([1, 5, 6]), V: torch.Size([1, 5, 6])\n",
      "\n",
      "Attention Weights Sample:\n",
      "[0.26710007 0.173288   0.16750504 0.21446617 0.1776406 ]\n",
      "\n",
      "Attention Output:\n",
      "[[[ 0.87049305 -0.26932955 -0.07900444  0.1696752  -0.1942007  -0.0319175 ]\n",
      "  [ 0.674561   -0.10025308  0.01203015  0.14709719 -0.14998768  0.07784672]\n",
      "  [ 0.67799765 -0.09590378  0.01653191  0.130569   -0.14098266  0.07776298]\n",
      "  [ 0.699886   -0.1165746   0.00449823  0.11809283 -0.14323543  0.05665344]]]\n",
      "\n",
      "Post Cross-Attention (Norm):\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.63141245  0.6617869  -0.7713917  -0.20523259  1.3377094  -1.6542847 ]\n",
      "  [ 0.60315835  0.099438   -0.36499846 -0.46472117  1.6866724  -1.5595491 ]\n",
      "  [ 0.13609111 -0.2283597   0.01448512 -0.4458819   1.9341254  -1.4104602 ]]]\n",
      "\n",
      "Decoder Feed-Forward:\n",
      "\n",
      "FeedForward Input: torch.Size([1, 4, 6])\n",
      "[[[ 0.8279262   0.26537123 -2.0388665   1.018108   -0.19105242  0.11851346]\n",
      "  [ 0.63141245  0.6617869  -0.7713917  -0.20523259  1.3377094  -1.6542847 ]\n",
      "  [ 0.60315835  0.099438   -0.36499846 -0.46472117  1.6866724  -1.5595491 ]\n",
      "  [ 0.13609111 -0.2283597   0.01448512 -0.4458819   1.9341254  -1.4104602 ]]]\n",
      "\n",
      "FeedForward Output:\n",
      "[[[-0.08223304  0.0510872  -0.44664547  0.2856996  -0.06149156 -0.15877774]\n",
      "  [ 0.18417846  0.23134384 -0.34468013 -0.23668462 -0.21082735 -0.3174991 ]\n",
      "  [ 0.26005793  0.2162382  -0.28551108 -0.20297903 -0.18367721 -0.3693515 ]\n",
      "  [ 0.29298535  0.24289638 -0.25471267 -0.15232325 -0.18747012 -0.41048115]]]\n",
      "\n",
      "Post FFN (Norm):\n",
      "[[[ 0.6814094   0.32227704 -2.0220773   1.1483729  -0.15379624  0.02381406]\n",
      "  [ 0.80746025  0.8746903  -0.86736494 -0.28284723  1.0773616  -1.6092999 ]\n",
      "  [ 0.84844655  0.36322746 -0.49298704 -0.508221    1.4154059  -1.6258715 ]\n",
      "  [ 0.47200176  0.08627605 -0.15077987 -0.48387492  1.6979976  -1.6216205 ]]]\n",
      "Step 4: Predicted token 2 (football)\n",
      "\n",
      "Final Prediction IDs: [1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40 + \" Making Prediction \" + \"=\"*40)\n",
    "prediction = predict(model, src)\n",
    "prediction = prediction.squeeze().tolist()\n",
    "print(\"\\nFinal Prediction IDs:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', 'football', 'football', 'football', 'football']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "pred = [inverse_vocab[w] for w in prediction]\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_text(predicted_ids, vocab):\n",
    "    inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    tokens = []\n",
    "    for id in predicted_ids:\n",
    "        if id == vocab[\"<pad>\"]:\n",
    "            continue\n",
    "        token = inverse_vocab.get(id, \"<unk>\")\n",
    "        if token == \"<start>\":\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified prediction function\n",
    "def predict(model, src_input, vocab, max_length=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_out = model.encoder(src_input)\n",
    "        target = torch.LongTensor([[vocab[\"<start>\"]]]).to(src_input.device)\n",
    "        \n",
    "        for _ in range(max_length-1):\n",
    "            output = model.decoder(target, enc_out)\n",
    "            logits = model.final_layer(output[:, -1, :])\n",
    "            next_token = logits.argmax(-1)\n",
    "            target = torch.cat([target, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == vocab[\"<pad>\"]:\n",
    "                break\n",
    "                \n",
    "        return ids_to_text(target.squeeze().cpu().numpy(), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "print(\"\\nGenerating Prediction...\")\n",
    "generated_text = predict(model, src, vocab)\n",
    "print(f\"Generated Sequence: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
